03 サポートベクトルマシン
=====================

## 1. マージン最大化分類器

$`x`$ に対するマージンを $`c(x)\hat{s}(x)`$ と定義する

* $`c(x)`$：正例に対して`+1`、負例に対して`-1`

* $`\hat{s}(x)`$ を事例 $`x`$ に対するスコア

$`\hat{s}(\bf{x}) = \bf{w}\cdot\bf{x} - t`$ を考える

* 真陽性例 $`\bf{x}_i`$：マージンは $`\bf{w}\bf{x}_i - t > 0`$

* 真陰性例 $`\bf{x}_i`$：マージンは $`- (\bf{w}\bf{x}_i - t) > 0`$

与えられた`訓練データ集合`と`決定境界`に対して、以下のマージンの和を最小にすることを考える

* $`m^{\oplus}`$：全ての正例の中でも、最も小さいマージン

* $`m^{\ominus}`$：全ての負例の中でも、最も小さいマージン

> この和は、決定境界に最も近い正例・負例が正しく分類されるように保ち続ける限り、閾値 $`t`$ とは無関係

$`t`$ は $`m^{\oplus}`$ と $`m^{\ominus}`$ の値が等しくなるように調整可能

![SVMの概略図](./images/03_サポートベクトルマシン/SVMの概略図.png)

`サポートベクトル`：決定境界に最も近い訓練事例

* `SVM`の決定曲線は、`サポートベクトル`の線形結合

* `マージン`：$`m / \| \bf{w} \|`$

  > $`m`$：決定境界に最も近い訓練インスタンスの、決定境界から $`\bf{w}`$ に沿って測った距離

* `マージン`の最大化は、$`\| \bf{w} \|`$ の最小化と同等

  > $`t`$、$`\| \bf{w} \|`$、$`m`$：値を定数倍しても結論は変わらない($`m=1`$ と設定)
  >
  > ただし、訓練インスタンスの点がマージンの間に落ちないことを仮定する



### 最適化問題・制約付き最適化問題

**最適化問題**

`最適化問題`：可能な値の集合から、最適な値などを探索する問題の総称

* `残差`の平方和 $`f(a,b) = \sum_{i=1}^n(w_i - (a + bh_i))^2`$ を最小にする、$`a`$ と $`b`$ の値を探索する

$`\begin{eqnarray}a^*, b^* = \arg_{a,b} \min f(a,b)\end{eqnarray}`$

$`f`$：目的関数

* 最小値は、$`a`$ と $`b`$ についての偏導関数が`0`になるような $`a`$ と $`b`$ の値を求める

* $`\bigtriangledown f(a,b) = \bf{0}`$ となるような $`a`$ と $`b`$ の値を求める問題で定義できる

  > `勾配`($`\bigtriangledown f`$)：これらの偏導関数からなるベクトル

ただし、目的関数が凸でない場合には、解の一意性は必ずしも保証されない



**制約付き最適化問題**

`制約付き最適化問題`：制約を与えたもとでの最適化問題

$`\begin{eqnarray}a^*, b^* = \arg_{a,b} \min f(a,b)\end{eqnarray}`$

subject: $`g(a,b) = c`$

* 制約によって示される関係が線形の場合：一方の変数を多数の変数で表し、制約のない最適化問題を解く($`a - b = 0`$ など)

* 制約によって示される関係が非線形の場合：`ラグランジュ乗数`($`g`$)を用いる



**ラグランジュの未定乗数法**

$`\begin{eqnarray}\Lambda(a, b, \lambda) = f(a, b) - \lambda(g(a, b) - c)\end{eqnarray}`$

* $`\lambda`$：ラグランジュ乗数

この関数 $`\Lambda`$ について、制約のない最適化問題を解く

$`\begin{eqnarray}\bigtriangledown \Lambda(a, b, \lambda) = \bf{0} \begin{cases}\bigtriangledown_{a,b} \Lambda(a,b,\lambda) = \bigtriangledown f(a,b) - \lambda \bigtriangledown_g (a,b) \\\bigtriangledown_{\lambda} \Lambda(a,b,\lambda) = g(a, b) - c\end{cases}\end{eqnarray}`$

* $`f`$ と $`g`$ の勾配は、同じ方向を示すベクトル

* 制約を満たしている

> 複数の等式制約や不等式制約には、それぞれの制約に個別のラグランジュ乗数を与える



### マージンの最適化問題

$`\| \bf{w} \|`$ の最小化と等価である $`\frac{1}{2} \| \bf{w} \|`$ の最小化を考える

$`\begin{eqnarray}\bf{w}^*, t^* = \arg_{\bf{w}, t} \min \frac{1}{2} \| \bf{w} \|^2\end{eqnarray}`$

subject：$`y_i(\bf{w} \cdot \bf{x}_i - t) \geq 1`$, $`1 \leq i \leq n`$

訓練データの各事例に対する制約に、ラグランジュ乗数 $`\alpha_i`$ を掛けたものを目的関数に加える

$`\begin{eqnarray}\Lambda(\bf{w}, t, \alpha_1, \cdots, \alpha_n) = \frac{1}{2} \| \bf{w} \|^2 - \sum_{i=1}^n \alpha_i(y_i (\bf{w} \cdot \bf{x}_i - t) - 1) \\= \frac{1}{2} \| \bf{w} \|^2 - \sum_{i=1}^n \alpha_i y_i (\bf{w} \cdot \bf{x}_i) + \sum_{i=1}^n \alpha_i y_i t + \sum_{i=1}^n \alpha_i \\= \frac{1}{2} \bf{w}\cdot\bf{w} - \bf{w} \Bigr( \sum_{i=1}^n \alpha_i y_i \bf{x}_i \Bigl) + t \Bigr( \sum_{i=1}^n \alpha_i y_i \Bigl) + \sum_{i=1}^n \alpha_i\end{eqnarray}`$

このラグランジュ関数の偏導関数をとり、`0`になるようにする

$`\begin{eqnarray}\frac{\partial}{\partial t} \Lambda(\bf{w}, t, \alpha_1, \cdots, \alpha_n) = \sum_{i=1}^n \alpha_i y_i = 0\end{eqnarray}`$

$`\begin{eqnarray}\frac{\partial}{\partial \bf{w}} \Lambda(\bf{w}, t, \alpha_1, \cdots, \alpha_n) = \bf{w} - \sum_{i=1}^n \alpha_i y_i \bf{w}_i = 0\end{eqnarray}`$

> ここで、$`\bf{w} = \sum_{i=1}^n \alpha_i y_i \bf{x}_i`$ となるので、パーセプトロンと同じ重みベクトルの表現となる

これらの解をラグランジュ関数に代入すると、以下の式となる

$`\begin{eqnarray}\Lambda (\alpha_1, \cdots, \alpha_n) = - \frac{1}{2} \Bigr( \sum_{i=1}^n \alpha_i y_i \bf{x}_i \Bigl) \cdot \Bigr( \sum_{i=1}^n \alpha_i y_i \bf{x}_i \Bigl) + \sum_{i=1}^n \alpha_i \\= - \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j \bf{x}_i \cdot \bf{x}_j + \sum_{i=1}^n \alpha_i\end{eqnarray}`$

ここで、双対問題は、以下のものとなる

* 上の関数において、$`\alpha_i`$ が正であるという複数個の不等式制約

* 1個の等式制約のもとで最大化

$`\begin{eqnarray}\alpha_1^*, \cdots, \alpha_n^* = \arg_{\alpha_1, \cdots, \alpha_n} \max - \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j \bf{x}_i \cdot \bf{x}_j + \sum_{i=1}^n \alpha_i\end{eqnarray}`$

subject：$`\alpha_i \geq 0`$, $`1 \leq i \leq n`$, $`\sum_{i=1}^n \alpha_i y_i = 0`$

**SVMに対する最適化問題の双対形式**

1. マージンを最大化するような決定境界の探索は、サポートベクトルの探索と同等

1. 最適化問題が訓練インスタンスの組に対する内積(グラム行列の成分)で表される



### 2つのマージン最大化分類器とそのサポートベクトル

各データ点 $`\bf{X}`$、ラベル $`\bf{y}`$、$`\bf{X}'`$($`\bf{X}`$ の各行にクラスラベルを乗じたもの)

$`
\begin{eqnarray}
\bf{X} = \left(
  \begin{array}{ccc}
     1 &  2 \\
    -1 &  2 \\
    -1 & -2
  \end{array}
\right),
\bf{y} = \left(
  \begin{array}{ccc}
    -1 \\
    -1 \\
    +1
  \end{array}
\right),
\bf{X}' = \left(
  \begin{array}{ccc}
    -1 & -2 \\
     1 & -2 \\
    -1 & -2
  \end{array}
\right)
\end{eqnarray}
`$
`

グラム行列は、以下の通りになる

$`
\begin{eqnarray}
\bf{X'X'^T} = \left(
  \begin{array}{ccc}
    5 & 3 & 5 \\
    3 & 5 & 3l \\
    5 & 3 & 5
  \end{array}
\right)
\end{eqnarray}
`$
`

双対最適化問題は、以下の通りに表される

$`
\begin{eqnarray}
\alpha_1^*, \alpha_2^*, \alpha_3^* = \arg_{\alpha_1, \alpha_2, \alpha_3} \max - \frac{1}{2}(5 \alpha_1^2 + 3 \alpha_1 \alpha_2 + 5 \alpha_1 \alpha_3 + 3 \alpha_2 \alpha_1 + 5 \alpha_2^2 + 3 \alpha_2 \alpha_3 + 5 \alpha_3 \alpha_1 + 3 \alpha_3 \alpha_2 + 5 \alpha_3^2) + \alpha_1 + \alpha_2 + \alpha_3\\
= \arg_{\alpha_1, \alpha_2, \alpha_3} \max - \frac{1}{2}(5 \alpha_1^2 + 6 \alpha_1 \alpha_2 + 10 \alpha_1 \alpha_3 + 5 \alpha_2^2 + 6 \alpha_2 \alpha_3 +5 \alpha_3^2) + \alpha_1 + \alpha_2 + \alpha_3
\end{eqnarray}
`$
`

subject：$`\alpha_1 \geq 0`$, $`\alpha_2 \geq 0`$, $`\alpha_3 \geq 0`$, $`- \alpha_1 - \alpha_2 + \alpha_3 = 0`$

ここで、$`\alpha_3`$ を消去すると以下の式で表される

$`\begin{eqnarray}\alpha_1^*, \alpha_2^*, \alpha_3^* = \arg_{\alpha_1, \alpha_2, \alpha_3} \max - \frac{1}{2}(20 \alpha_1^2 + 32 \alpha_1 \alpha_2 + 16 \alpha_2^2) + 2 \alpha_1 + 2 \alpha_2\end{eqnarray}`$

この式に関して、偏導関数が`0`となる方程式を立てる

$`\begin{eqnarray}\frac{\partial}{\partial \alpha_1}\alpha_1^*, \alpha_2^*, \alpha_3^* = -20 \alpha_1 - 16 \alpha_2 + 2 = 0 \\\frac{\partial}{\partial \alpha_2}\alpha_1^*, \alpha_2^*, \alpha_3^* = -16 \alpha_1 - 16 \alpha_2 + 2 = 0\end{eqnarray}`$

この式より、以下の解が導出できる

$`\begin{eqnarray}\alpha_1 = 0, \alpha_2 = \alpha_3 = \frac{1}{8}\end{eqnarray}`$

ゆえに、$`\bf{w}`$、マージンは以下の通りに表される

$`\begin{eqnarray}\bf{w} = \alpha_1 y_1 \bf{x_1} + \alpha_2 y_2 \bf{x_2} + \alpha_3 y_3 \bf{x_3}\\= 0 \cdot(-1) \cdot \bf{x}_1 + \frac{1}{8} \cdot(-1) \cdot\bf{x}_2 + \frac{1}{8} \cdot 1 \cdot \bf{x}_3\\= \frac{1}{8}(\bf{x}_3 - \bf{x}_2) = \left(\begin{array}{ccc}0 \\-\frac{1}{2}\end{array}\right)\end{eqnarray}`$

$`\begin{eqnarray}\frac{1}{\| \bf{w} \|} = 2\end{eqnarray}`$

**正例を追加した場合**

$`\begin{eqnarray}\bf{X}' = \left(\begin{array}{ccc}-1 & -2 \\1 & -2 \\-1 & -2 \\3 &  1\end{array}\right), \bf{X}'\bf{X}'^T = \left(\begin{array}{ccc}5 & 3 &  5 & -5 \\3 & 5 &  3 &  1 \\5 & 3 &  5 & -5 \\-5 & 1 & -5 & 10\end{array}\right)\end{eqnarray}`$

ラグランジュ乗数は以下の通りに表される

$`\begin{eqnarray}\alpha_1 = \frac{1}{2}, \alpha_2 = 0, \alpha_3 = \frac{1}{10}, \alpha_4 = \frac{2}{5}\end{eqnarray}`$

また、マージンと決定境界は以下の通りに表される

$`\begin{eqnarray}\frac{1}{\| \bf{w} \|} = 1\\\bf{w} = \left(\begin{array}{ccc}\frac{3}{5} \\\frac{-4}{5}\end{array}\right)\end{eqnarray}`$

![SVMの例](./images/03_サポートベクトルマシン/SVMの例.png)



## 2.ソフトマージンSVM
