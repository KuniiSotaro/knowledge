05 カーネル法を用いた非線形分類器
============================

* `特徴空間`：変換された空間

* `入力空間`：もとの空間

データの非線形性を、線形分類器に変換する：特徴空間を陽に構成する必要はない

> 新しいデータを特徴空間に変換し、モデルに当てはめる必要があるが、必要な操作は全て入力空間で実行できる



### 例：パーセプトロンの双対形式

1. $`\sum_{i=1}^{\vert D \vert}\alpha_i y_i \bf{x}_i\cdot\bf{x}_j`$ を評価

1. 各 $`\bf{x}_i`$ が正しく分類されていることを確認

ここで鍵になるのは、内積 $`\bf{x}_i\cdot\bf{x}_j`$

**簡単な内積と特徴空間**

まず、内積を求める

$`\bf{x}_i = (x_i,y_i)`$

$`\bf{x}_j = (x_j,y_j)`$

```math
\bf{x}_i\cdot\bf{x}_j = x_ix_j + y_iy_j
```

次に、`平方変換`に対応する特徴空間を考える

$`\bf{x}_i' = (x_i^2, y_i^2)`$

$`\bf{x}_i' = (x_j^2, y_j^2)`$

```math
(x_i^2,y_i^2)\cdot(x_j^2,y_j^2) = x_i^2y_i^2 + x_j^2y_j^2
```

ただし、この方法では $`(\bf{x}_i\cdot\bf{x}_jj)^2`$ と比較して、第三項の交差積によって一致しなくなる

**拡張した内積と特徴空間**

特徴ベクトルに、$`\sqrt{2}xy`$ を加える

```math
\phi(\bf{x}_i) = (x_i^2,y_i^2,\sqrt{2}x_iy_i)
```

```math
\phi(\bf{x}_j) = (x_j^2,y_j^2,\sqrt{2}x_jy_j)
```

```math
\phi(\bf{x}_i) \cdot \phi(\bf{x}_j) = x_i^2x_j^2 + y_i^2y_j^2 + 2x_ix_jy_iy_j = (\bf{x}_i\cdot\bf{x}_j)^2
```

ここで、以下の式で定義する

```math
\kappa(\bf{x}_i, \bf{x}_j) = (\bf{x_i\cdot x_j})^2
```

双対パーセプトロンアルゴリズムにおいて、$`\bf{x}_i\cdot\bf{x}_j`$ を $`\kappa(\bf{x}_i\cdot\bf{x}_j)`$ に置き換えることで、以下の`カーネルパーセプトロン`が得られる

**$`KernelPerceptron(D,\kappa)`$：カーネル関数を用いたパーセプトロン学習アルゴリズム**

|                                                                                                              |
| ------------------------------------------------------------------------------------------------------------ |
| Input: 同次座標系においてラベル付けされた訓練集合 $`D`$                                                      |
| Output: 係数 $`\alpha_i`$ (非線形な決定境界を構成) |
| 1: $`\alpha \leftarrow 0 (1 \leq i \leq \vert D \vert)`$                                                     |
| 2: $`converged \leftarrow false`$                                                                            |
| 3: while $`converged = false`$ do                                                                            |
| 4:   $`converged \leftarrow true`$                                                                           |
| 5:   for $`i = 1`$ to $`\vert D \vert`$ do                                                                   |
| 6:     if $`\sum_{i=1}^{\vert D \vert} \alpha_i y_i \kappa(\bf{x}_i,\bf{x}_j) \leq 0`$ then                    |
| 7:       $`\alpha_i \leftarrow \alpha_i + 1`$                                                                |
| 8:       $`converged \leftarrow false`$                                                                      |
| 9:     end                                                                                                   |
| 10:   end                                                                                                    |
| 11:end                                                                                                       |



### カーネルの導入

$`\kappa(\bf{x}_i,\bf{x}_j) = (\bf{x}_i\cdot\bf{x}_j)^p`$：任意の次数 $`p`$ の多項式カーネルを定義する

* $`d`$ 次元の入力空間を、高次元の特徴空間に埋め込む

* 特徴空間における新しい特徴量は、$`p`$ 個の項の積からなる(重複を含む)

* 定数を含めることで、低次元の全ての項も得ることができる($`\kappa(\bf{x}_i,\bf{x}_j) = (\bf{x}_i\cdot\bf{x}_j + 1)^p`$)

例：2次元入力空間

$`p=2`$ の場合、特徴空間は以下の式で表される

```math
\phi(\bf{x}) = \bigr( x^2, y^2, \sqrt{2}xy, \sqrt{2}x, \sqrt{2}y, 1 \bigl)
```

> 線形の項が得られる



### ガウシアンカーネル

```math
\kappa(\bf{x}_i, \bf{x}_j) = \exp \Bigr( \frac{- \| \bf{x}_i - \bf{x}_j \|^2}{\sigma^2} \Bigl)
```

* $`\sigma`$：バンド幅(平滑化パラメータ)

* `半正定値性`：全ての点 $`\phi(\bf{x})`$ が、特徴空間の原点を中心とする超球上にある

  > $`\kappa(\bf{x},\bf{x}) = \phi(\bf{x})\cdot\phi(\bf{x}) = \| \phi(\bf{x}) \|^2`$

`ガウシアン・カーネル`は、インスタンス空間における各サポートベクトル上に、多変量正規分布(の定数倍)を当てはめていることを見なしている

> `決定境界`：それらの多変量正規分布の密度曲面に対して定義される



### カーネルトリック

`ソフトマージンSVM`最適化問題は、内積が定義されていたので、「カーネルトリック」が用いられる

```math
\begin{eqnarray}\alpha_1^*, \cdots, \alpha_n^* = \arg_{\alpha_1, \cdots, \alpha_n} \max - \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j \kappa(\bf{x}_i, \bf{x}_j) + \sum_{i=1}^n \alpha_i\end{eqnarray}
```

subject: $`0 \leq \alpha_i \leq C`$, $`\sum_{i=1}^n \alpha_i y_i = 0`$

ここで、非線形カーネルで学習された決定境界は、入力空間における重みベクトルとして単純に表せないことに注意する

* 新しい事例 $`\bf{x}`$ を分類するためには、$`y_i \sum_{i=1}^n \alpha_i y_i \kappa(\bf{x},\bf{x}_j)`$ を評価する

* $`\alpha_j = `$ となる事例が多くなり、計算量が軽減されることより、カーネル法はよく用いられる



| 版   | 年/月/日   |
| ---- | ---------- |
| 初版 | 2019/06/23 |
