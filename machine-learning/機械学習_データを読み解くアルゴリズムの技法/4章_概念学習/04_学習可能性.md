04 学習可能性
============

* 表現力が高い手法であればあるほど、学習が難しくなっていく

  > `学習可能性`を考える必要がある

* 学習可能性を考えるには、学習モデルを明確にしておく必要がある

  > 何をもって学習可能であるかを定義するのか?

  * よく用いられるのは、`PAC学習`

## PAC学習

* `PAC学習`：「ほとんどの場合」において、「ほとんど正しい」結果を与えてくれるときに、学習可能である

  * 「ほとんど正しい」：非典型な例についての間違いについては許容する

  * 「ほとんどの場合」：低い確率であるが、起こりうることが起こった場合に、全く適切に学習できない「失敗」の可能性も許容すること

* `PAC学習可能性`が成り立つ条件は、 $`err_{F} < \epsilon`$ であるような概念(仮説) $`h`$ を出力する確率が $`1-\delta`$ 以上であること

  > * $`err_{F}`$ ：確率分布 $`F`$ に基づいて計算した誤り率
  >
  > * $`0 < |epsilon`$ 、$`\delta < \frac{1}{2}`$ で固定した



### PAC学習の考え方

**前提**

* 目的としている概念が仮説空間 $`H`$ の中に含まれている

* 訓練データはその概念からノイズなしで発生しているとする

* 学習によって、訓練データに対して完全かつ整合的な概念が得られる

**1.訓練データから悪い概念を学習してしまう確率の評価**

* 分布 $`F`$ のもとで、 $`err_{F}`$ が $`\epsilon`$ 以上になってしまうような「悪い」概念が仮説空間の中に1つだけある場合を考える

* 各訓練データに対して悪い概念が完全かつ整合的でない確率は、$`\epsilon`$ 以上

  * $`m`$ 個の独立な訓練データが完全かつ整合的になる確率は $`(1 - \epsilon)^m`$

  * 公式を用いると、$`(1 - \epsilon)^m \leq e^{- m \epsilon}`$ となる

    > 公式：$`1 - \epsilon \leq e^{- m \epsilon}(0 \leq \epsilon \leq 1)`$

**2.悪い概念と訓練データと完全かつ整合的になってしまう確率の評価**

* 仮説空間 $`H`$ の中に、悪い概念が $`k`$ 個あったとする

  * $`k \leq \vert H \vert`$ より、$`k(1-\epsilon)^m \leq \vert H \vert (1-\epsilon)^m \leq \vert H \vert e^{-m \epsilon}`$

    > $`k`$ 個の悪い概念のどれかが訓練データと完全かつ整合的hになる確率

**3.PAC学習可能性の式の整理**

* $` \vert H \vert e^{-m \epsilon} \leq \delta`$ の対数を取って整理することで、以下の式で表される

$`\begin{eqnarray} m \geq \frac{1}{\epsilon}\Bigl( \ln \vert H \vert + \ln \frac{1}{\delta} \Bigr) \end{eqnarray}`$

> 訓練データの数 $`m`$ をこの式を満たすようにすれば、`PAC学習可能性`の要件を満たす

**4.PAC学習可能性の式の評価**

* `サンプル複雑度`：学習可能性の要件を満たすような訓練データの数のこと

  * 失敗割合の上限 $`\delta`$ を小さくするのにかかる手間は、真の誤差の上限 $`\epsilon`$ を小さくするのにかかる手間よりも指数的に楽



### 最悪な状況に基づいた値

* $`\vert H \vert `$ ：仮説空間 $`H`$ の全てが悪い概念であるといいう最悪の状況に基づいた値

  * 悲観的に(緩く)不等式を評価している

* 例) $`n`$　個のブール変数のリテラルからなる連言概念で作られる仮説空間を考える


### 訓練誤差が少なくなるような概念

## VC次元

### インスタンス集合のシャッタリング
