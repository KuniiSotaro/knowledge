04 学習可能性
============

* 表現力が高い手法であればあるほど、学習が難しくなっていく

  > `学習可能性`を考える必要がある

* 学習可能性を考えるには、学習モデルを明確にしておく必要がある

  > 何をもって学習可能であるかを定義するのか?

  * よく用いられるのは、`PAC学習`

## PAC学習

* `PAC学習`：「ほとんどの場合」において、「ほとんど正しい」結果を与えてくれるときに、学習可能である

  * 「ほとんど正しい」：非典型な例についての間違いについては許容する

  * 「ほとんどの場合」：低い確率であるが、起こりうることが起こった場合に、全く適切に学習できない「失敗」の可能性も許容すること

* `PAC学習可能性`が成り立つ条件は、 $`err_{F} < \epsilon`$ であるような概念(仮説) $`h`$ を出力する確率が $`1-\delta`$ 以上であること

  > * $`err_{F}`$ ：確率分布 $`F`$ に基づいて計算した誤り率
  >
  > * $`0 < |epsilon`$ 、$`\delta < \frac{1}{2}`$ で固定した



### PAC学習の考え方

**前提**

* 目的としている概念が仮説空間 $`H`$ の中に含まれている

* 訓練データはその概念からノイズなしで発生しているとする

* 学習によって、訓練データに対して完全かつ整合的な概念が得られる

**1.訓練データから悪い概念を学習してしまう確率の評価**

* 分布 $`F`$ のもとで、 $`err_{F}`$ が $`\epsilon`$ 以上になってしまうような「悪い」概念が仮説空間の中に1つだけある場合を考える

* 各訓練データに対して悪い概念が完全かつ整合的でない確率は、$`\epsilon`$ 以上

  * $`m`$ 個の独立な訓練データが完全かつ整合的になる確率は $`(1 - \epsilon)^m`$

  * 公式を用いると、$`(1 - \epsilon)^m \leq e^{- m \epsilon}`$ となる

    > 公式：$`1 - \epsilon \leq e^{- m \epsilon}(0 \leq \epsilon \leq 1)`$

**2.悪い概念と訓練データと完全かつ整合的になってしまう確率の評価**

* 仮説空間 $`H`$ の中に、悪い概念が $`k`$ 個あったとする

  * $`k \leq \vert H \vert`$ より、$`k(1-\epsilon)^m \leq \vert H \vert (1-\epsilon)^m \leq \vert H \vert e^{-m \epsilon}`$

    > $`k`$ 個の悪い概念のどれかが訓練データと完全かつ整合的hになる確率

**3.PAC学習可能性の式の整理**

* $` \vert H \vert e^{-m \epsilon} \leq \delta`$ の対数を取って整理することで、以下の式で表される

$`\begin{eqnarray} m \geq \frac{1}{\epsilon}\Bigl( \ln \vert H \vert + \ln \frac{1}{\delta} \Bigr) \end{eqnarray}`$

> 訓練データの数 $`m`$ をこの式を満たすようにすれば、`PAC学習可能性`の要件を満たす

**4.PAC学習可能性の式の評価**

* `サンプル複雑度`：学習可能性の要件を満たすような訓練データの数のこと

  * 失敗割合の上限 $`\delta`$ を小さくするのにかかる手間は、真の誤差の上限 $`\epsilon`$ を小さくするのにかかる手間よりも指数的に楽



### 最悪な状況に基づいた値

* $`\vert H \vert `$ ：仮説空間 $`H`$ の全てが悪い概念であるといいう最悪の状況に基づいた値

  * 悲観的に(緩く)不等式を評価している

* 例) $`n`$　個のブール変数のリテラルからなる連言概念で作られる仮説空間を考える

  * 各ブール変数：`true`、`false`、連言に現れない(3通り)

    * 概念の数： $`\vert H \vert = 3^n`$ 通り

    * `サンプル複雑度`： $`\frac{1}{\epsilon}\Bigl( n \ln 3 + \ln \frac{1}{\delta}\Bigr)`$

      > $`\delta = 0.05`$ 、 $`\epsilon = 0.1`$ ：$`10(n \cdot 1.1 + 3) = 11n + 30`$
      >
      > イルカの例のように $`n=4`$ を考えると、サンプル複雑度は`74`となる
      >
      > ただし、連言概念が異なる例は $`2^4 = 16`$ 通りしかないので、明らかに悲観的となる



### 訓練誤差が少なくなるような概念

* これまでは、訓練データに対して完全かつ整合的な概念が得られることを仮定した

> 計算量、概念が仮説空間に入っていない場合、訓練データにノイズが入っているなどの問題を考える
>
> **より訓練誤差が少なくなるような概念を学習することが求められる**

* 「悪い概念」：その概念の真の誤差が、訓練誤差より $`\epsilon`$ 以上大きくなる場合

  * 最大でも $`e^{-2m \epsilon^2}`$ で抑えられる(確率論)

  * つまり、$`\frac{1}{\epsilon} \rightarrow \frac{1}{2\epsilon^2}`$ に置き換わるので、訓練データの数 $`m`$ が大きくなる必要がある

    > 例) $`\epsilon = 0.1`$ ： $`\frac{1}{0.2} = 5`$ 倍の量の訓練データが必要



## VC次元

### インスタンス集合のシャッタリング

**4つのインスタンス**

$`m = ManyTeeth \land \lnot{Gills} \land \lnot{Short} \land \lnot{Beak}`$

$`g = \lnot{ManyTeeth} \land Gills \land \lnot{Short} \land \lnot{Beak}`$

$`s = \lnot{ManyTeeth} \land \lnot{Gills} \land Short \land \lnot{Beak}`$

$`b = \lnot{ManyTeeth} \land \lnot{Gills} \land \lnot{Short} \land Beak`$

**連言概念での分離**

* 集合 $`\{m,g,s,b\}`$ の部分集合($`2^4=16`$ 通り)を、連言概念で分類する

  * 部分集合に含まれないインスタンスに対応するリテラルに、否定 $`\lnot`$ を付けたものを、分類するための連言概念に付け加える

  > 例) $`\{m,s\}=\lnot{Gills}\land\lnot{Beak}`$、$`\{g,s,b\}=\lnot{ManyTeeth}`$、$`\{s\}=\lnot{ManyTeeth}\land\lnot{Gills}\lnot{Beak}`$

* `シャッターされている`：連言概念で全ての分類が可能になっていること



### VC次元とは

`VC次元`：仮説空間によってシャッターされている集合のうち、最も多くの要素をもつ集合の要素数

* 用いている仮説空間の表現力の高さを表す量

* $`d`$ 個のブール値のリテラルの連言概念からなる仮説空間の`VC次元`は、$`d`$ 個となる

  > ただし、$`d+1`$ 個の要素をもつどの集合もシャッターされないことを示す必要がある

* 概念が無限個あるような仮説空間でも、VC次元が有限であれば`サンプル複雑度`が導ける

訓練データに対して、完全かつ整合的な概念が学習できる場合のサンプル複雑度の評価

* 2次元のVC次元：1直線上の閾値

* 3次元のVC次元：2次元平面上の直線

> VC次元を、$`D`$ とする

$`m \geq \frac{1}{\epsilon} \max \Bigr( 8D\log_2\frac{13}{\epsilon}, 4\log_2\frac{2}{\delta} \Bigl)`$

* $`D`$ 個の点をシャッターするには、仮説空間のなかに少なくとも $`2^D`$ 個の概念が必要

  > $`\log_2 \vert H \vert \geq D`$

* $`\frac{1}{\delta}`$ は線形のまま、$`\frac{1}{\epsilon}`$ は線形項と対数の項の積

  > $`\delta = 0.05`$ 、 $`\epsilon = 0.1`$、サンプル複雑度は $`\max(562 \cdot D, 213)`$



| 版   | 年/月/日   |
| ---- | ---------- |
| 初版 | 2019/06/16 |
