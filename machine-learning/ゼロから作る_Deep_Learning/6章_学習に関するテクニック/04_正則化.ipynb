{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "04 正則化\n",
    "========\n",
    "\n",
    "* 機械学習の問題では、`過学習`が問題になることが多い\n",
    "\n",
    "* これは、訓練データにだけ適応しすぎてしまい、訓練データに含まれない他のデータにはうまく対応できない状態のことを言う\n",
    "\n",
    "* 機械学習で目指すことは、汎化性能なので、正しく識別できるモデルが望まれる\n",
    "\n",
    "* 複雑で表現力の高いモデルを作ることは可能だが、その分、過学習を抑制するテクニックが重要となる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.過学習\n",
    "\n",
    "* 過学習が起きる原因として、主に次の2つがあげられる\n",
    "\n",
    "> * パラメータを大量に持ち、表現力の高いモデルであること\n",
    ">\n",
    "> * 訓練データが少ないこと\n",
    "\n",
    "* ここでは、この2つの要件をわざと満たして、過学習を発生させる\n",
    "\n",
    "    * そのため、MNISTデータセットの訓練データを本来の60,000個から3000個だけに限定する\n",
    "    \n",
    "    * また、ネットワークの複雑性を高めるために、7層のネットワークを使う(各層：100個のニューロン、活性化関数：ReLU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, train acc:0.10666666666666667, test acc:0.1092\n",
      "epoch:1, train acc:0.12333333333333334, test acc:0.117\n",
      "epoch:2, train acc:0.13333333333333333, test acc:0.1263\n",
      "epoch:3, train acc:0.13666666666666666, test acc:0.1241\n",
      "epoch:4, train acc:0.15, test acc:0.1275\n",
      "epoch:5, train acc:0.16, test acc:0.1339\n",
      "epoch:6, train acc:0.17, test acc:0.1449\n",
      "epoch:7, train acc:0.17666666666666667, test acc:0.1474\n",
      "epoch:8, train acc:0.17333333333333334, test acc:0.1397\n",
      "epoch:9, train acc:0.19333333333333333, test acc:0.1523\n",
      "epoch:10, train acc:0.22333333333333333, test acc:0.1654\n",
      "epoch:11, train acc:0.25, test acc:0.1768\n",
      "epoch:12, train acc:0.27666666666666667, test acc:0.1907\n",
      "epoch:13, train acc:0.30666666666666664, test acc:0.2039\n",
      "epoch:14, train acc:0.3433333333333333, test acc:0.218\n",
      "epoch:15, train acc:0.31333333333333335, test acc:0.2019\n",
      "epoch:16, train acc:0.32666666666666666, test acc:0.2197\n",
      "epoch:17, train acc:0.37, test acc:0.2417\n",
      "epoch:18, train acc:0.39666666666666667, test acc:0.2605\n",
      "epoch:19, train acc:0.45, test acc:0.2832\n",
      "epoch:20, train acc:0.45666666666666667, test acc:0.2945\n",
      "epoch:21, train acc:0.47333333333333333, test acc:0.3184\n",
      "epoch:22, train acc:0.48333333333333334, test acc:0.3339\n",
      "epoch:23, train acc:0.5133333333333333, test acc:0.3324\n",
      "epoch:24, train acc:0.5233333333333333, test acc:0.3401\n",
      "epoch:25, train acc:0.53, test acc:0.35\n",
      "epoch:26, train acc:0.5, test acc:0.3303\n",
      "epoch:27, train acc:0.49333333333333335, test acc:0.3291\n",
      "epoch:28, train acc:0.53, test acc:0.3445\n",
      "epoch:29, train acc:0.5433333333333333, test acc:0.3764\n",
      "epoch:30, train acc:0.5333333333333333, test acc:0.3655\n",
      "epoch:31, train acc:0.56, test acc:0.386\n",
      "epoch:32, train acc:0.56, test acc:0.3932\n",
      "epoch:33, train acc:0.5533333333333333, test acc:0.3898\n",
      "epoch:34, train acc:0.5833333333333334, test acc:0.4143\n",
      "epoch:35, train acc:0.5966666666666667, test acc:0.4321\n",
      "epoch:36, train acc:0.5933333333333334, test acc:0.427\n",
      "epoch:37, train acc:0.5966666666666667, test acc:0.4311\n",
      "epoch:38, train acc:0.6033333333333334, test acc:0.4352\n",
      "epoch:39, train acc:0.61, test acc:0.4484\n",
      "epoch:40, train acc:0.6066666666666667, test acc:0.4536\n",
      "epoch:41, train acc:0.6166666666666667, test acc:0.4661\n",
      "epoch:42, train acc:0.6233333333333333, test acc:0.4705\n",
      "epoch:43, train acc:0.62, test acc:0.4715\n",
      "epoch:44, train acc:0.6433333333333333, test acc:0.4994\n",
      "epoch:45, train acc:0.65, test acc:0.4977\n",
      "epoch:46, train acc:0.65, test acc:0.4971\n",
      "epoch:47, train acc:0.67, test acc:0.5094\n",
      "epoch:48, train acc:0.66, test acc:0.4939\n",
      "epoch:49, train acc:0.6766666666666666, test acc:0.5092\n",
      "epoch:50, train acc:0.69, test acc:0.5268\n",
      "epoch:51, train acc:0.6966666666666667, test acc:0.5375\n",
      "epoch:52, train acc:0.6933333333333334, test acc:0.5442\n",
      "epoch:53, train acc:0.7066666666666667, test acc:0.5526\n",
      "epoch:54, train acc:0.69, test acc:0.544\n",
      "epoch:55, train acc:0.68, test acc:0.5458\n",
      "epoch:56, train acc:0.71, test acc:0.5631\n",
      "epoch:57, train acc:0.69, test acc:0.5527\n",
      "epoch:58, train acc:0.7133333333333334, test acc:0.5752\n",
      "epoch:59, train acc:0.7066666666666667, test acc:0.5751\n",
      "epoch:60, train acc:0.7133333333333334, test acc:0.5775\n",
      "epoch:61, train acc:0.7033333333333334, test acc:0.5702\n",
      "epoch:62, train acc:0.7066666666666667, test acc:0.5731\n",
      "epoch:63, train acc:0.7366666666666667, test acc:0.5884\n",
      "epoch:64, train acc:0.7566666666666667, test acc:0.5945\n",
      "epoch:65, train acc:0.7666666666666667, test acc:0.6001\n",
      "epoch:66, train acc:0.75, test acc:0.6061\n",
      "epoch:67, train acc:0.7666666666666667, test acc:0.6052\n",
      "epoch:68, train acc:0.7866666666666666, test acc:0.613\n",
      "epoch:69, train acc:0.8, test acc:0.6173\n",
      "epoch:70, train acc:0.79, test acc:0.6177\n",
      "epoch:71, train acc:0.7966666666666666, test acc:0.6151\n",
      "epoch:72, train acc:0.8033333333333333, test acc:0.6228\n",
      "epoch:73, train acc:0.8, test acc:0.626\n",
      "epoch:74, train acc:0.81, test acc:0.6359\n",
      "epoch:75, train acc:0.8433333333333334, test acc:0.6474\n",
      "epoch:76, train acc:0.8233333333333334, test acc:0.6554\n",
      "epoch:77, train acc:0.82, test acc:0.6431\n",
      "epoch:78, train acc:0.83, test acc:0.6538\n",
      "epoch:79, train acc:0.8366666666666667, test acc:0.6525\n",
      "epoch:80, train acc:0.8433333333333334, test acc:0.663\n",
      "epoch:81, train acc:0.8433333333333334, test acc:0.6618\n",
      "epoch:82, train acc:0.8533333333333334, test acc:0.6673\n",
      "epoch:83, train acc:0.8533333333333334, test acc:0.6608\n",
      "epoch:84, train acc:0.84, test acc:0.6457\n",
      "epoch:85, train acc:0.8433333333333334, test acc:0.6511\n",
      "epoch:86, train acc:0.84, test acc:0.6622\n",
      "epoch:87, train acc:0.8433333333333334, test acc:0.6618\n",
      "epoch:88, train acc:0.8433333333333334, test acc:0.66\n",
      "epoch:89, train acc:0.85, test acc:0.6613\n",
      "epoch:90, train acc:0.8566666666666667, test acc:0.6641\n",
      "epoch:91, train acc:0.8433333333333334, test acc:0.6608\n",
      "epoch:92, train acc:0.8466666666666667, test acc:0.6696\n",
      "epoch:93, train acc:0.85, test acc:0.6772\n",
      "epoch:94, train acc:0.8466666666666667, test acc:0.6747\n",
      "epoch:95, train acc:0.85, test acc:0.6753\n",
      "epoch:96, train acc:0.8566666666666667, test acc:0.6674\n",
      "epoch:97, train acc:0.8566666666666667, test acc:0.6709\n",
      "epoch:98, train acc:0.86, test acc:0.6699\n",
      "epoch:99, train acc:0.86, test acc:0.6772\n",
      "epoch:100, train acc:0.8466666666666667, test acc:0.6709\n",
      "epoch:101, train acc:0.8633333333333333, test acc:0.6731\n",
      "epoch:102, train acc:0.8466666666666667, test acc:0.6813\n",
      "epoch:103, train acc:0.8533333333333334, test acc:0.6749\n",
      "epoch:104, train acc:0.87, test acc:0.6764\n",
      "epoch:105, train acc:0.8566666666666667, test acc:0.6893\n",
      "epoch:106, train acc:0.86, test acc:0.6886\n",
      "epoch:107, train acc:0.8666666666666667, test acc:0.6859\n",
      "epoch:108, train acc:0.87, test acc:0.6865\n",
      "epoch:109, train acc:0.87, test acc:0.6994\n",
      "epoch:110, train acc:0.8633333333333333, test acc:0.6846\n",
      "epoch:111, train acc:0.8566666666666667, test acc:0.6837\n",
      "epoch:112, train acc:0.8533333333333334, test acc:0.6805\n",
      "epoch:113, train acc:0.8566666666666667, test acc:0.6816\n",
      "epoch:114, train acc:0.86, test acc:0.6858\n",
      "epoch:115, train acc:0.8666666666666667, test acc:0.6955\n",
      "epoch:116, train acc:0.8633333333333333, test acc:0.6898\n",
      "epoch:117, train acc:0.8733333333333333, test acc:0.6969\n",
      "epoch:118, train acc:0.8633333333333333, test acc:0.6969\n",
      "epoch:119, train acc:0.87, test acc:0.6987\n",
      "epoch:120, train acc:0.87, test acc:0.6957\n",
      "epoch:121, train acc:0.8666666666666667, test acc:0.6986\n",
      "epoch:122, train acc:0.86, test acc:0.6962\n",
      "epoch:123, train acc:0.8666666666666667, test acc:0.6978\n",
      "epoch:124, train acc:0.8566666666666667, test acc:0.6994\n",
      "epoch:125, train acc:0.8466666666666667, test acc:0.6949\n",
      "epoch:126, train acc:0.86, test acc:0.6875\n",
      "epoch:127, train acc:0.8733333333333333, test acc:0.6867\n",
      "epoch:128, train acc:0.8733333333333333, test acc:0.7011\n",
      "epoch:129, train acc:0.8666666666666667, test acc:0.7034\n",
      "epoch:130, train acc:0.8766666666666667, test acc:0.7046\n",
      "epoch:131, train acc:0.8766666666666667, test acc:0.7011\n",
      "epoch:132, train acc:0.8733333333333333, test acc:0.701\n",
      "epoch:133, train acc:0.8666666666666667, test acc:0.7086\n",
      "epoch:134, train acc:0.8633333333333333, test acc:0.7024\n",
      "epoch:135, train acc:0.8633333333333333, test acc:0.6922\n",
      "epoch:136, train acc:0.8666666666666667, test acc:0.6868\n",
      "epoch:137, train acc:0.8733333333333333, test acc:0.6959\n",
      "epoch:138, train acc:0.85, test acc:0.6862\n",
      "epoch:139, train acc:0.8666666666666667, test acc:0.6949\n",
      "epoch:140, train acc:0.8666666666666667, test acc:0.702\n",
      "epoch:141, train acc:0.86, test acc:0.6906\n",
      "epoch:142, train acc:0.8833333333333333, test acc:0.6964\n",
      "epoch:143, train acc:0.8766666666666667, test acc:0.7095\n",
      "epoch:144, train acc:0.8733333333333333, test acc:0.7006\n",
      "epoch:145, train acc:0.8833333333333333, test acc:0.7084\n",
      "epoch:146, train acc:0.89, test acc:0.7069\n",
      "epoch:147, train acc:0.8733333333333333, test acc:0.7102\n",
      "epoch:148, train acc:0.8733333333333333, test acc:0.7115\n",
      "epoch:149, train acc:0.8866666666666667, test acc:0.7116\n",
      "epoch:150, train acc:0.8733333333333333, test acc:0.7108\n",
      "epoch:151, train acc:0.8866666666666667, test acc:0.7073\n",
      "epoch:152, train acc:0.8766666666666667, test acc:0.7074\n",
      "epoch:153, train acc:0.8766666666666667, test acc:0.7052\n",
      "epoch:154, train acc:0.88, test acc:0.7068\n",
      "epoch:155, train acc:0.88, test acc:0.7136\n",
      "epoch:156, train acc:0.88, test acc:0.7099\n",
      "epoch:157, train acc:0.88, test acc:0.7146\n",
      "epoch:158, train acc:0.8766666666666667, test acc:0.7077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:159, train acc:0.8833333333333333, test acc:0.7086\n",
      "epoch:160, train acc:0.8833333333333333, test acc:0.7081\n",
      "epoch:161, train acc:0.88, test acc:0.7106\n",
      "epoch:162, train acc:0.8733333333333333, test acc:0.7006\n",
      "epoch:163, train acc:0.8833333333333333, test acc:0.7074\n",
      "epoch:164, train acc:0.88, test acc:0.6982\n",
      "epoch:165, train acc:0.8766666666666667, test acc:0.7031\n",
      "epoch:166, train acc:0.8766666666666667, test acc:0.7076\n",
      "epoch:167, train acc:0.8733333333333333, test acc:0.7106\n",
      "epoch:168, train acc:0.88, test acc:0.7075\n",
      "epoch:169, train acc:0.8866666666666667, test acc:0.7022\n",
      "epoch:170, train acc:0.8933333333333333, test acc:0.7027\n",
      "epoch:171, train acc:0.8966666666666666, test acc:0.7117\n",
      "epoch:172, train acc:0.89, test acc:0.7075\n",
      "epoch:173, train acc:0.89, test acc:0.7081\n",
      "epoch:174, train acc:0.8766666666666667, test acc:0.7051\n",
      "epoch:175, train acc:0.8866666666666667, test acc:0.7081\n",
      "epoch:176, train acc:0.9, test acc:0.7133\n",
      "epoch:177, train acc:0.8766666666666667, test acc:0.7048\n",
      "epoch:178, train acc:0.89, test acc:0.7127\n",
      "epoch:179, train acc:0.8866666666666667, test acc:0.7087\n",
      "epoch:180, train acc:0.8866666666666667, test acc:0.7032\n",
      "epoch:181, train acc:0.9066666666666666, test acc:0.7114\n",
      "epoch:182, train acc:0.9033333333333333, test acc:0.7182\n",
      "epoch:183, train acc:0.8933333333333333, test acc:0.7091\n",
      "epoch:184, train acc:0.89, test acc:0.7129\n",
      "epoch:185, train acc:0.8766666666666667, test acc:0.7107\n",
      "epoch:186, train acc:0.8833333333333333, test acc:0.7173\n",
      "epoch:187, train acc:0.8666666666666667, test acc:0.7094\n",
      "epoch:188, train acc:0.89, test acc:0.708\n",
      "epoch:189, train acc:0.8966666666666666, test acc:0.7145\n",
      "epoch:190, train acc:0.89, test acc:0.7083\n",
      "epoch:191, train acc:0.8766666666666667, test acc:0.71\n",
      "epoch:192, train acc:0.8833333333333333, test acc:0.7131\n",
      "epoch:193, train acc:0.8833333333333333, test acc:0.7064\n",
      "epoch:194, train acc:0.8933333333333333, test acc:0.7125\n",
      "epoch:195, train acc:0.9, test acc:0.7108\n",
      "epoch:196, train acc:0.8933333333333333, test acc:0.7132\n",
      "epoch:197, train acc:0.8833333333333333, test acc:0.7091\n",
      "epoch:198, train acc:0.9, test acc:0.7139\n",
      "epoch:199, train acc:0.8966666666666666, test acc:0.7096\n",
      "epoch:200, train acc:0.8966666666666666, test acc:0.7118\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x119eb9128>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.multi_layer_net import MultiLayerNet\n",
    "from common.optimizer import SGD\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "\n",
    "# 過学習を再現するために、学習データを削減\n",
    "x_train = x_train[:300]\n",
    "t_train = t_train[:300]\n",
    "\n",
    "# weight decay（荷重減衰）の設定 =======================\n",
    "#weight_decay_lambda = 0 # weight decayを使用しない場合\n",
    "weight_decay_lambda = 0.1\n",
    "# ====================================================\n",
    "\n",
    "network = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100], output_size=10,\n",
    "                        weight_decay_lambda=weight_decay_lambda)\n",
    "optimizer = SGD(lr=0.01)\n",
    "\n",
    "max_epochs = 201\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "epoch_cnt = 0\n",
    "\n",
    "for i in range(1000000000):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    grads = network.gradient(x_batch, t_batch)\n",
    "    optimizer.update(network.params, grads)\n",
    "\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "\n",
    "        print(\"epoch:\" + str(epoch_cnt) + \", train acc:\" + str(train_acc) + \", test acc:\" + str(test_acc))\n",
    "\n",
    "        epoch_cnt += 1\n",
    "        if epoch_cnt >= max_epochs:\n",
    "            break\n",
    "\n",
    "\n",
    "# 3.グラフの描画==========\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
    "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* この結果より、訓練データとテストデータとの間で認識精度に大きな隔たりができてしまう"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Weight decay\n",
    "\n",
    "* 過学習抑制のために、`Weight decay`(荷重減衰)を用いる\n",
    "\n",
    "    * これは、学習の過程において、大きな重みを持つことに対してペナルティを課すことで、過学習を抑制しようというもの\n",
    "    \n",
    "    * そもそも過学習は、重みパラメータが大きな値を取ることによって発生することが多くある"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ニューラルネットワークの学習は、損失関数の値を小さくすることを目的として行われる\n",
    "\n",
    "    * この時、重みが大きくなることが抑制できる可能性がある\n",
    "    \n",
    "    * 重みを$\\vec{W}$とすれば、L2ノルムのWeight decayは、$\\frac{1}{2}\\lambda \\vec{W^2}$となり、これを損失関数に加算する\n",
    "    \n",
    "    * ここで、$\\lambda$は正則化の強さをコントロールするハイパーパラメータ\n",
    "    \n",
    "        * これを大きくすると、大きな重みに対して強いペナルティを課すことになる\n",
    "        \n",
    "    * また、$\\frac{1}{2}$は、微分の結果を$\\lambda \\vec{W}$にするための調整用の定数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Weight decayは、全ての重みに対して、損失関数に$\\frac{1}{2}\\lambda\\vec{W^2}$を加算する\n",
    "\n",
    "    * そのため、重みの勾配を求める計算では、これまでの誤差逆伝播法による結果に、正則化項の微分$\\lambda \\vec{W}$を加算する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ここで、先ほど行なった実験に対して$\\lambda = 0.1$として、Weight decayを適用する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, train acc:0.07333333333333333, test acc:0.093\n",
      "epoch:1, train acc:0.07666666666666666, test acc:0.1024\n",
      "epoch:2, train acc:0.12, test acc:0.1216\n",
      "epoch:3, train acc:0.17, test acc:0.1465\n",
      "epoch:4, train acc:0.21333333333333335, test acc:0.164\n",
      "epoch:5, train acc:0.25, test acc:0.1896\n",
      "epoch:6, train acc:0.2966666666666667, test acc:0.2196\n",
      "epoch:7, train acc:0.31666666666666665, test acc:0.2389\n",
      "epoch:8, train acc:0.36, test acc:0.2645\n",
      "epoch:9, train acc:0.36333333333333334, test acc:0.2941\n",
      "epoch:10, train acc:0.37, test acc:0.3179\n",
      "epoch:11, train acc:0.39, test acc:0.3347\n",
      "epoch:12, train acc:0.4033333333333333, test acc:0.3493\n",
      "epoch:13, train acc:0.41333333333333333, test acc:0.3571\n",
      "epoch:14, train acc:0.4066666666666667, test acc:0.3656\n",
      "epoch:15, train acc:0.4266666666666667, test acc:0.3807\n",
      "epoch:16, train acc:0.43666666666666665, test acc:0.3924\n",
      "epoch:17, train acc:0.45666666666666667, test acc:0.4017\n",
      "epoch:18, train acc:0.4666666666666667, test acc:0.4156\n",
      "epoch:19, train acc:0.4866666666666667, test acc:0.4367\n",
      "epoch:20, train acc:0.49666666666666665, test acc:0.4369\n",
      "epoch:21, train acc:0.5133333333333333, test acc:0.4409\n",
      "epoch:22, train acc:0.5333333333333333, test acc:0.4503\n",
      "epoch:23, train acc:0.5533333333333333, test acc:0.4626\n",
      "epoch:24, train acc:0.5666666666666667, test acc:0.4709\n",
      "epoch:25, train acc:0.5933333333333334, test acc:0.4879\n",
      "epoch:26, train acc:0.5766666666666667, test acc:0.4786\n",
      "epoch:27, train acc:0.6133333333333333, test acc:0.5019\n",
      "epoch:28, train acc:0.6133333333333333, test acc:0.5043\n",
      "epoch:29, train acc:0.6133333333333333, test acc:0.5108\n",
      "epoch:30, train acc:0.6333333333333333, test acc:0.5206\n",
      "epoch:31, train acc:0.65, test acc:0.5259\n",
      "epoch:32, train acc:0.65, test acc:0.5262\n",
      "epoch:33, train acc:0.6533333333333333, test acc:0.5216\n",
      "epoch:34, train acc:0.66, test acc:0.52\n",
      "epoch:35, train acc:0.6733333333333333, test acc:0.5377\n",
      "epoch:36, train acc:0.6866666666666666, test acc:0.5505\n",
      "epoch:37, train acc:0.6766666666666666, test acc:0.5496\n",
      "epoch:38, train acc:0.6766666666666666, test acc:0.548\n",
      "epoch:39, train acc:0.7, test acc:0.5602\n",
      "epoch:40, train acc:0.7066666666666667, test acc:0.5692\n",
      "epoch:41, train acc:0.6933333333333334, test acc:0.5659\n",
      "epoch:42, train acc:0.6966666666666667, test acc:0.5691\n",
      "epoch:43, train acc:0.6733333333333333, test acc:0.5596\n",
      "epoch:44, train acc:0.71, test acc:0.5741\n",
      "epoch:45, train acc:0.71, test acc:0.5795\n",
      "epoch:46, train acc:0.7466666666666667, test acc:0.5878\n",
      "epoch:47, train acc:0.75, test acc:0.5934\n",
      "epoch:48, train acc:0.76, test acc:0.5963\n",
      "epoch:49, train acc:0.7466666666666667, test acc:0.5923\n",
      "epoch:50, train acc:0.7533333333333333, test acc:0.6097\n",
      "epoch:51, train acc:0.76, test acc:0.5974\n",
      "epoch:52, train acc:0.7633333333333333, test acc:0.6042\n",
      "epoch:53, train acc:0.77, test acc:0.6081\n",
      "epoch:54, train acc:0.77, test acc:0.6152\n",
      "epoch:55, train acc:0.7666666666666667, test acc:0.6172\n",
      "epoch:56, train acc:0.78, test acc:0.6182\n",
      "epoch:57, train acc:0.7566666666666667, test acc:0.6134\n",
      "epoch:58, train acc:0.7666666666666667, test acc:0.6212\n",
      "epoch:59, train acc:0.77, test acc:0.622\n",
      "epoch:60, train acc:0.7733333333333333, test acc:0.6344\n",
      "epoch:61, train acc:0.7666666666666667, test acc:0.6364\n",
      "epoch:62, train acc:0.7733333333333333, test acc:0.6294\n",
      "epoch:63, train acc:0.78, test acc:0.6318\n",
      "epoch:64, train acc:0.7766666666666666, test acc:0.6322\n",
      "epoch:65, train acc:0.7733333333333333, test acc:0.6319\n",
      "epoch:66, train acc:0.7766666666666666, test acc:0.6431\n",
      "epoch:67, train acc:0.7833333333333333, test acc:0.6366\n",
      "epoch:68, train acc:0.79, test acc:0.6455\n",
      "epoch:69, train acc:0.7833333333333333, test acc:0.6469\n",
      "epoch:70, train acc:0.7833333333333333, test acc:0.6298\n",
      "epoch:71, train acc:0.7866666666666666, test acc:0.6404\n",
      "epoch:72, train acc:0.8033333333333333, test acc:0.6447\n",
      "epoch:73, train acc:0.8066666666666666, test acc:0.647\n",
      "epoch:74, train acc:0.7966666666666666, test acc:0.6492\n",
      "epoch:75, train acc:0.8066666666666666, test acc:0.6467\n",
      "epoch:76, train acc:0.8033333333333333, test acc:0.6563\n",
      "epoch:77, train acc:0.79, test acc:0.6542\n",
      "epoch:78, train acc:0.8066666666666666, test acc:0.6495\n",
      "epoch:79, train acc:0.8166666666666667, test acc:0.6585\n",
      "epoch:80, train acc:0.8033333333333333, test acc:0.6469\n",
      "epoch:81, train acc:0.82, test acc:0.651\n",
      "epoch:82, train acc:0.8233333333333334, test acc:0.659\n",
      "epoch:83, train acc:0.8133333333333334, test acc:0.6598\n",
      "epoch:84, train acc:0.8233333333333334, test acc:0.6555\n",
      "epoch:85, train acc:0.8333333333333334, test acc:0.6643\n",
      "epoch:86, train acc:0.8133333333333334, test acc:0.6685\n",
      "epoch:87, train acc:0.8133333333333334, test acc:0.6687\n",
      "epoch:88, train acc:0.84, test acc:0.6799\n",
      "epoch:89, train acc:0.8366666666666667, test acc:0.679\n",
      "epoch:90, train acc:0.8333333333333334, test acc:0.6782\n",
      "epoch:91, train acc:0.82, test acc:0.6717\n",
      "epoch:92, train acc:0.8266666666666667, test acc:0.666\n",
      "epoch:93, train acc:0.8266666666666667, test acc:0.6651\n",
      "epoch:94, train acc:0.8266666666666667, test acc:0.6744\n",
      "epoch:95, train acc:0.83, test acc:0.664\n",
      "epoch:96, train acc:0.8366666666666667, test acc:0.6862\n",
      "epoch:97, train acc:0.8466666666666667, test acc:0.6756\n",
      "epoch:98, train acc:0.8333333333333334, test acc:0.675\n",
      "epoch:99, train acc:0.8366666666666667, test acc:0.6895\n",
      "epoch:100, train acc:0.8533333333333334, test acc:0.6898\n",
      "epoch:101, train acc:0.84, test acc:0.6844\n",
      "epoch:102, train acc:0.8466666666666667, test acc:0.6754\n",
      "epoch:103, train acc:0.84, test acc:0.6878\n",
      "epoch:104, train acc:0.85, test acc:0.6803\n",
      "epoch:105, train acc:0.85, test acc:0.6798\n",
      "epoch:106, train acc:0.85, test acc:0.6922\n",
      "epoch:107, train acc:0.8566666666666667, test acc:0.6912\n",
      "epoch:108, train acc:0.8633333333333333, test acc:0.6912\n",
      "epoch:109, train acc:0.8633333333333333, test acc:0.6869\n",
      "epoch:110, train acc:0.8666666666666667, test acc:0.6858\n",
      "epoch:111, train acc:0.8633333333333333, test acc:0.6872\n",
      "epoch:112, train acc:0.8566666666666667, test acc:0.6827\n",
      "epoch:113, train acc:0.87, test acc:0.6869\n",
      "epoch:114, train acc:0.86, test acc:0.6895\n",
      "epoch:115, train acc:0.8666666666666667, test acc:0.6895\n",
      "epoch:116, train acc:0.8666666666666667, test acc:0.6901\n",
      "epoch:117, train acc:0.8733333333333333, test acc:0.6971\n",
      "epoch:118, train acc:0.87, test acc:0.6869\n",
      "epoch:119, train acc:0.8666666666666667, test acc:0.6888\n",
      "epoch:120, train acc:0.8566666666666667, test acc:0.677\n",
      "epoch:121, train acc:0.87, test acc:0.6893\n",
      "epoch:122, train acc:0.86, test acc:0.6841\n",
      "epoch:123, train acc:0.8733333333333333, test acc:0.697\n",
      "epoch:124, train acc:0.86, test acc:0.6878\n",
      "epoch:125, train acc:0.87, test acc:0.6978\n",
      "epoch:126, train acc:0.8633333333333333, test acc:0.6964\n",
      "epoch:127, train acc:0.8666666666666667, test acc:0.6945\n",
      "epoch:128, train acc:0.8633333333333333, test acc:0.697\n",
      "epoch:129, train acc:0.8866666666666667, test acc:0.6926\n",
      "epoch:130, train acc:0.8833333333333333, test acc:0.7018\n",
      "epoch:131, train acc:0.8766666666666667, test acc:0.6975\n",
      "epoch:132, train acc:0.8766666666666667, test acc:0.6906\n",
      "epoch:133, train acc:0.88, test acc:0.6847\n",
      "epoch:134, train acc:0.8766666666666667, test acc:0.6981\n",
      "epoch:135, train acc:0.89, test acc:0.7064\n",
      "epoch:136, train acc:0.8833333333333333, test acc:0.6962\n",
      "epoch:137, train acc:0.8866666666666667, test acc:0.7009\n",
      "epoch:138, train acc:0.89, test acc:0.6986\n",
      "epoch:139, train acc:0.89, test acc:0.7046\n",
      "epoch:140, train acc:0.8866666666666667, test acc:0.7044\n",
      "epoch:141, train acc:0.8833333333333333, test acc:0.6989\n",
      "epoch:142, train acc:0.88, test acc:0.6969\n",
      "epoch:143, train acc:0.8833333333333333, test acc:0.703\n",
      "epoch:144, train acc:0.89, test acc:0.6946\n",
      "epoch:145, train acc:0.8733333333333333, test acc:0.6824\n",
      "epoch:146, train acc:0.9, test acc:0.6996\n",
      "epoch:147, train acc:0.8833333333333333, test acc:0.7002\n",
      "epoch:148, train acc:0.8833333333333333, test acc:0.6944\n",
      "epoch:149, train acc:0.88, test acc:0.7031\n",
      "epoch:150, train acc:0.8833333333333333, test acc:0.699\n",
      "epoch:151, train acc:0.9066666666666666, test acc:0.7091\n",
      "epoch:152, train acc:0.89, test acc:0.7042\n",
      "epoch:153, train acc:0.8933333333333333, test acc:0.7096\n",
      "epoch:154, train acc:0.8866666666666667, test acc:0.7036\n",
      "epoch:155, train acc:0.8866666666666667, test acc:0.7023\n",
      "epoch:156, train acc:0.9, test acc:0.7124\n",
      "epoch:157, train acc:0.9, test acc:0.7103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:158, train acc:0.89, test acc:0.6982\n",
      "epoch:159, train acc:0.89, test acc:0.7015\n",
      "epoch:160, train acc:0.8966666666666666, test acc:0.7025\n",
      "epoch:161, train acc:0.89, test acc:0.7046\n",
      "epoch:162, train acc:0.9033333333333333, test acc:0.7148\n",
      "epoch:163, train acc:0.8966666666666666, test acc:0.7044\n",
      "epoch:164, train acc:0.8833333333333333, test acc:0.7155\n",
      "epoch:165, train acc:0.9133333333333333, test acc:0.7188\n",
      "epoch:166, train acc:0.9033333333333333, test acc:0.7169\n",
      "epoch:167, train acc:0.9, test acc:0.7109\n",
      "epoch:168, train acc:0.91, test acc:0.7165\n",
      "epoch:169, train acc:0.91, test acc:0.7134\n",
      "epoch:170, train acc:0.8933333333333333, test acc:0.708\n",
      "epoch:171, train acc:0.9033333333333333, test acc:0.7054\n",
      "epoch:172, train acc:0.9066666666666666, test acc:0.7096\n",
      "epoch:173, train acc:0.9033333333333333, test acc:0.7092\n",
      "epoch:174, train acc:0.9066666666666666, test acc:0.7179\n",
      "epoch:175, train acc:0.8966666666666666, test acc:0.7138\n",
      "epoch:176, train acc:0.9066666666666666, test acc:0.7194\n",
      "epoch:177, train acc:0.8933333333333333, test acc:0.7058\n",
      "epoch:178, train acc:0.8833333333333333, test acc:0.6931\n",
      "epoch:179, train acc:0.89, test acc:0.6852\n",
      "epoch:180, train acc:0.8966666666666666, test acc:0.7084\n",
      "epoch:181, train acc:0.8933333333333333, test acc:0.7149\n",
      "epoch:182, train acc:0.8933333333333333, test acc:0.7162\n",
      "epoch:183, train acc:0.88, test acc:0.7131\n",
      "epoch:184, train acc:0.9066666666666666, test acc:0.7227\n",
      "epoch:185, train acc:0.9033333333333333, test acc:0.7172\n",
      "epoch:186, train acc:0.8966666666666666, test acc:0.7206\n",
      "epoch:187, train acc:0.9033333333333333, test acc:0.7137\n",
      "epoch:188, train acc:0.9033333333333333, test acc:0.7149\n",
      "epoch:189, train acc:0.91, test acc:0.7155\n",
      "epoch:190, train acc:0.9066666666666666, test acc:0.7123\n",
      "epoch:191, train acc:0.9033333333333333, test acc:0.7197\n",
      "epoch:192, train acc:0.9033333333333333, test acc:0.7172\n",
      "epoch:193, train acc:0.9033333333333333, test acc:0.7213\n",
      "epoch:194, train acc:0.91, test acc:0.7209\n",
      "epoch:195, train acc:0.9033333333333333, test acc:0.7137\n",
      "epoch:196, train acc:0.9066666666666666, test acc:0.7084\n",
      "epoch:197, train acc:0.8933333333333333, test acc:0.7166\n",
      "epoch:198, train acc:0.9066666666666666, test acc:0.7137\n",
      "epoch:199, train acc:0.9066666666666666, test acc:0.711\n",
      "epoch:200, train acc:0.9033333333333333, test acc:0.7176\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x11b10fe48>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.multi_layer_net import MultiLayerNet\n",
    "from common.optimizer import SGD\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "\n",
    "# 過学習を再現するために、学習データを削減\n",
    "x_train = x_train[:300]\n",
    "t_train = t_train[:300]\n",
    "\n",
    "# weight decay（荷重減衰）の設定 =======================\n",
    "#weight_decay_lambda = 0 # weight decayを使用しない場合\n",
    "weight_decay_lambda = 0.1\n",
    "# ====================================================\n",
    "\n",
    "network = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100], output_size=10,\n",
    "                        weight_decay_lambda=weight_decay_lambda)\n",
    "optimizer = SGD(lr=0.01)\n",
    "\n",
    "max_epochs = 201\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "epoch_cnt = 0\n",
    "\n",
    "for i in range(1000000000):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    grads = network.gradient(x_batch, t_batch)\n",
    "    optimizer.update(network.params, grads)\n",
    "\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "\n",
    "        print(\"epoch:\" + str(epoch_cnt) + \", train acc:\" + str(train_acc) + \", test acc:\" + str(test_acc))\n",
    "\n",
    "        epoch_cnt += 1\n",
    "        if epoch_cnt >= max_epochs:\n",
    "            break\n",
    "\n",
    "\n",
    "# 3.グラフの描画==========\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
    "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 上の図の通り、訓練データの認識精度とテストデータの認識精度には隔たりはあるが、小さくなった\n",
    "\n",
    "    * これは過学習が抑制されたことになる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Dropout\n",
    "\n",
    "* 過学習を抑制する手法として、損失関数に対して重みのL2ノルムを加算するWeight decayという手法を用いた\n",
    "\n",
    "    * Weight decayは簡単に実装でき、ある程度過学習を抑制することができる\n",
    "    \n",
    "    * しかし、ニューラルネットワークのモデルが複雑になってくると、Weight decayだけでは対応が困難になってくる\n",
    "    \n",
    "    * そのため、`Dropout`という手法がよく用いられる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `Dropout`は、ニューロンをランダムに消去しながら学習する方法\n",
    "\n",
    "    * 訓練時に隠れ層のニューロンをランダムに選び出し、その選び出したニューロンを消去する\n",
    "    \n",
    "    * 消去されたニューロンは、以下の図に示すように、信号の伝達が行われなくなる\n",
    "    \n",
    "* なお、訓練時にはデータが流れるたびに、消去するニューロンをランダムに選択する\n",
    "\n",
    "    * そして、テスト時には、全てのニューロンの信号を伝達するが、各ニューロンの出力に対して、訓練時に消去した割合を乗算して出力する\n",
    "    \n",
    "![Dropoutの概念図](./images/Dropoutの概念図.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 続いて、Dropoutを実装する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    def __init__(self, dropout_ratio=0.5):\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.mask = None\n",
    "        \n",
    "    def forward(self, x, train_flg=True):\n",
    "        if train_flg:\n",
    "            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n",
    "        else:\n",
    "            return x * (1.0 - self.dropout_ratio)\n",
    "        \n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ここでのポイントは、順伝播のたびに`self.mask`に消去するニューロンをFalseとして格納するという点\n",
    "\n",
    "    * `self.mask`は`x`と同じ形状の配列をランダムに生成し、その値が`dropout_ratio`よりも大きい要素だけをTrueとする\n",
    "    \n",
    "    * 逆伝播の際の挙動は、ReLUと同じになる\n",
    "    \n",
    "    * つまり、順伝播で信号を通したニューロンは、逆伝播の際に伝わる信号をそのまま通す\n",
    "    \n",
    "    * 順伝播で信号を通さな買ったニューロンは、逆伝播では信号がそこでストップする"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ここで、Dropoutの効果を確かめるために、MNISTデータセットで検証してみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.3087786536888815\n",
      "=== epoch:1, train acc:0.11333333333333333, test acc:0.0856 ===\n",
      "train loss:2.315510523106321\n",
      "train loss:2.3042128994634594\n",
      "train loss:2.3181204769313535\n",
      "=== epoch:2, train acc:0.12333333333333334, test acc:0.086 ===\n",
      "train loss:2.3091020230606323\n",
      "train loss:2.2950699297585273\n",
      "train loss:2.2982836803983857\n",
      "=== epoch:3, train acc:0.11666666666666667, test acc:0.087 ===\n",
      "train loss:2.2964459410747544\n",
      "train loss:2.2924658708459242\n",
      "train loss:2.2947519175577935\n",
      "=== epoch:4, train acc:0.12666666666666668, test acc:0.0889 ===\n",
      "train loss:2.317960757719537\n",
      "train loss:2.2922436827025283\n",
      "train loss:2.2996893215534757\n",
      "=== epoch:5, train acc:0.12333333333333334, test acc:0.0886 ===\n",
      "train loss:2.307299397284476\n",
      "train loss:2.2916123685541634\n",
      "train loss:2.291768365901828\n",
      "=== epoch:6, train acc:0.12666666666666668, test acc:0.091 ===\n",
      "train loss:2.301367766673905\n",
      "train loss:2.30053047925489\n",
      "train loss:2.301289266402916\n",
      "=== epoch:7, train acc:0.12666666666666668, test acc:0.0921 ===\n",
      "train loss:2.2942922769689345\n",
      "train loss:2.2842726663669843\n",
      "train loss:2.3019840129599354\n",
      "=== epoch:8, train acc:0.12666666666666668, test acc:0.0938 ===\n",
      "train loss:2.301595045336883\n",
      "train loss:2.295897092773602\n",
      "train loss:2.30983049690358\n",
      "=== epoch:9, train acc:0.13, test acc:0.0947 ===\n",
      "train loss:2.300252052732502\n",
      "train loss:2.2975670059559277\n",
      "train loss:2.292366213682736\n",
      "=== epoch:10, train acc:0.12666666666666668, test acc:0.0956 ===\n",
      "train loss:2.295891346296314\n",
      "train loss:2.2900763636577866\n",
      "train loss:2.2861232382994454\n",
      "=== epoch:11, train acc:0.12666666666666668, test acc:0.0972 ===\n",
      "train loss:2.3008160595176186\n",
      "train loss:2.2848911753907504\n",
      "train loss:2.2911231461313646\n",
      "=== epoch:12, train acc:0.13333333333333333, test acc:0.0982 ===\n",
      "train loss:2.285220865392003\n",
      "train loss:2.2938632500149017\n",
      "train loss:2.2745284526632945\n",
      "=== epoch:13, train acc:0.13333333333333333, test acc:0.1 ===\n",
      "train loss:2.2928690757474217\n",
      "train loss:2.274338056167491\n",
      "train loss:2.287382708248272\n",
      "=== epoch:14, train acc:0.13333333333333333, test acc:0.1007 ===\n",
      "train loss:2.303812381054129\n",
      "train loss:2.2871504856630214\n",
      "train loss:2.288015771813148\n",
      "=== epoch:15, train acc:0.13333333333333333, test acc:0.1005 ===\n",
      "train loss:2.2836102147945674\n",
      "train loss:2.3044540600381933\n",
      "train loss:2.2945223371549592\n",
      "=== epoch:16, train acc:0.13333333333333333, test acc:0.1005 ===\n",
      "train loss:2.2782824171331875\n",
      "train loss:2.2818172548492353\n",
      "train loss:2.290503063529526\n",
      "=== epoch:17, train acc:0.13, test acc:0.1011 ===\n",
      "train loss:2.281211468312014\n",
      "train loss:2.268654822637256\n",
      "train loss:2.27922016452524\n",
      "=== epoch:18, train acc:0.13, test acc:0.1011 ===\n",
      "train loss:2.2819787396200586\n",
      "train loss:2.306998957695987\n",
      "train loss:2.267069874829623\n",
      "=== epoch:19, train acc:0.12666666666666668, test acc:0.1022 ===\n",
      "train loss:2.2884659583605313\n",
      "train loss:2.2870257352434114\n",
      "train loss:2.2916581459644\n",
      "=== epoch:20, train acc:0.13333333333333333, test acc:0.1022 ===\n",
      "train loss:2.2933450769317836\n",
      "train loss:2.280254845201592\n",
      "train loss:2.284943302816011\n",
      "=== epoch:21, train acc:0.13333333333333333, test acc:0.1029 ===\n",
      "train loss:2.2862141774466935\n",
      "train loss:2.277636007777025\n",
      "train loss:2.276994149309825\n",
      "=== epoch:22, train acc:0.13666666666666666, test acc:0.1037 ===\n",
      "train loss:2.280013916096605\n",
      "train loss:2.276143886218379\n",
      "train loss:2.2946984077519677\n",
      "=== epoch:23, train acc:0.13666666666666666, test acc:0.1049 ===\n",
      "train loss:2.276369332530651\n",
      "train loss:2.2725718732049818\n",
      "train loss:2.2870575159449613\n",
      "=== epoch:24, train acc:0.13666666666666666, test acc:0.1059 ===\n",
      "train loss:2.270094455619029\n",
      "train loss:2.2638671475808216\n",
      "train loss:2.2831175340336323\n",
      "=== epoch:25, train acc:0.13333333333333333, test acc:0.1054 ===\n",
      "train loss:2.2740794572715397\n",
      "train loss:2.2502346113535583\n",
      "train loss:2.273691169894286\n",
      "=== epoch:26, train acc:0.13333333333333333, test acc:0.106 ===\n",
      "train loss:2.27059499336614\n",
      "train loss:2.2776370042590792\n",
      "train loss:2.270609397372449\n",
      "=== epoch:27, train acc:0.13666666666666666, test acc:0.1053 ===\n",
      "train loss:2.2872895932843007\n",
      "train loss:2.2628997972572407\n",
      "train loss:2.270640392221271\n",
      "=== epoch:28, train acc:0.13666666666666666, test acc:0.1058 ===\n",
      "train loss:2.2775842444646797\n",
      "train loss:2.2659450627164675\n",
      "train loss:2.259910502671656\n",
      "=== epoch:29, train acc:0.13666666666666666, test acc:0.1065 ===\n",
      "train loss:2.283426940627236\n",
      "train loss:2.26614734169235\n",
      "train loss:2.2697651324737014\n",
      "=== epoch:30, train acc:0.14, test acc:0.1059 ===\n",
      "train loss:2.2778565865976557\n",
      "train loss:2.2619314781198954\n",
      "train loss:2.287668139797705\n",
      "=== epoch:31, train acc:0.14, test acc:0.1065 ===\n",
      "train loss:2.281457888088981\n",
      "train loss:2.2624308549658947\n",
      "train loss:2.27060310735051\n",
      "=== epoch:32, train acc:0.14, test acc:0.1084 ===\n",
      "train loss:2.273300103166583\n",
      "train loss:2.255776696206635\n",
      "train loss:2.2831708247696874\n",
      "=== epoch:33, train acc:0.14333333333333334, test acc:0.11 ===\n",
      "train loss:2.2633773297800834\n",
      "train loss:2.253294232509998\n",
      "train loss:2.257649671695159\n",
      "=== epoch:34, train acc:0.14, test acc:0.1076 ===\n",
      "train loss:2.26325847219547\n",
      "train loss:2.264798435120947\n",
      "train loss:2.272190221954526\n",
      "=== epoch:35, train acc:0.14, test acc:0.1073 ===\n",
      "train loss:2.267817419532795\n",
      "train loss:2.2830509704633966\n",
      "train loss:2.2726957377520534\n",
      "=== epoch:36, train acc:0.14333333333333334, test acc:0.1107 ===\n",
      "train loss:2.27361623462767\n",
      "train loss:2.262779273550822\n",
      "train loss:2.270917092030734\n",
      "=== epoch:37, train acc:0.14666666666666667, test acc:0.1109 ===\n",
      "train loss:2.273296378980192\n",
      "train loss:2.2664718043292895\n",
      "train loss:2.252827681006915\n",
      "=== epoch:38, train acc:0.15333333333333332, test acc:0.1127 ===\n",
      "train loss:2.256561490502311\n",
      "train loss:2.2708361363854355\n",
      "train loss:2.273506032713691\n",
      "=== epoch:39, train acc:0.16, test acc:0.1128 ===\n",
      "train loss:2.2743927317067443\n",
      "train loss:2.259460991346707\n",
      "train loss:2.257352057428112\n",
      "=== epoch:40, train acc:0.15666666666666668, test acc:0.1128 ===\n",
      "train loss:2.2793802764321383\n",
      "train loss:2.2621735505303766\n",
      "train loss:2.2492555353556773\n",
      "=== epoch:41, train acc:0.16333333333333333, test acc:0.1146 ===\n",
      "train loss:2.2846655834490686\n",
      "train loss:2.261024181911003\n",
      "train loss:2.257155825207823\n",
      "=== epoch:42, train acc:0.16666666666666666, test acc:0.1162 ===\n",
      "train loss:2.272018982352057\n",
      "train loss:2.2540548499680857\n",
      "train loss:2.2700530616458434\n",
      "=== epoch:43, train acc:0.17, test acc:0.1187 ===\n",
      "train loss:2.256987156729989\n",
      "train loss:2.255885185844752\n",
      "train loss:2.257336214299067\n",
      "=== epoch:44, train acc:0.17333333333333334, test acc:0.1202 ===\n",
      "train loss:2.24905374511754\n",
      "train loss:2.2553069969863206\n",
      "train loss:2.24249336997952\n",
      "=== epoch:45, train acc:0.16, test acc:0.1178 ===\n",
      "train loss:2.2342326934669496\n",
      "train loss:2.2448447917130117\n",
      "train loss:2.2459180119876985\n",
      "=== epoch:46, train acc:0.15333333333333332, test acc:0.1158 ===\n",
      "train loss:2.2693096423033197\n",
      "train loss:2.2330390747641915\n",
      "train loss:2.254348474795524\n",
      "=== epoch:47, train acc:0.14666666666666667, test acc:0.1149 ===\n",
      "train loss:2.2375293982531597\n",
      "train loss:2.252029264234703\n",
      "train loss:2.249939998444573\n",
      "=== epoch:48, train acc:0.14333333333333334, test acc:0.114 ===\n",
      "train loss:2.2537300775204447\n",
      "train loss:2.254526016367137\n",
      "train loss:2.2513145851063725\n",
      "=== epoch:49, train acc:0.15333333333333332, test acc:0.1182 ===\n",
      "train loss:2.251560919295726\n",
      "train loss:2.2426508598632995\n",
      "train loss:2.2476768723157545\n",
      "=== epoch:50, train acc:0.15666666666666668, test acc:0.1161 ===\n",
      "train loss:2.2375803022366707\n",
      "train loss:2.22770866895975\n",
      "train loss:2.2447697150171986\n",
      "=== epoch:51, train acc:0.15333333333333332, test acc:0.1161 ===\n",
      "train loss:2.2528702513091163\n",
      "train loss:2.2662283268053987\n",
      "train loss:2.2577562131802345\n",
      "=== epoch:52, train acc:0.16666666666666666, test acc:0.1227 ===\n",
      "train loss:2.2555250199076204\n",
      "train loss:2.2606550253527793\n",
      "train loss:2.2347571759348783\n",
      "=== epoch:53, train acc:0.17, test acc:0.1266 ===\n",
      "train loss:2.24194266522984\n",
      "train loss:2.261519466432554\n",
      "train loss:2.241423990755253\n",
      "=== epoch:54, train acc:0.16333333333333333, test acc:0.1264 ===\n",
      "train loss:2.238129048188959\n",
      "train loss:2.223583334929801\n",
      "train loss:2.2401464502322916\n",
      "=== epoch:55, train acc:0.17333333333333334, test acc:0.1266 ===\n",
      "train loss:2.2279948996806613\n",
      "train loss:2.246979721361864\n",
      "train loss:2.2423704252086742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:56, train acc:0.17333333333333334, test acc:0.1284 ===\n",
      "train loss:2.2532122300327484\n",
      "train loss:2.2345598443490915\n",
      "train loss:2.237450592075069\n",
      "=== epoch:57, train acc:0.18666666666666668, test acc:0.1309 ===\n",
      "train loss:2.245796806208952\n",
      "train loss:2.2499666726836813\n",
      "train loss:2.246897630936449\n",
      "=== epoch:58, train acc:0.19666666666666666, test acc:0.1434 ===\n",
      "train loss:2.2528819683206214\n",
      "train loss:2.236590616394523\n",
      "train loss:2.241771148203245\n",
      "=== epoch:59, train acc:0.19666666666666666, test acc:0.1524 ===\n",
      "train loss:2.243001510701838\n",
      "train loss:2.255462783484567\n",
      "train loss:2.2339195868814583\n",
      "=== epoch:60, train acc:0.20333333333333334, test acc:0.16 ===\n",
      "train loss:2.23224492589819\n",
      "train loss:2.2229694883081517\n",
      "train loss:2.2294301454795953\n",
      "=== epoch:61, train acc:0.20666666666666667, test acc:0.1613 ===\n",
      "train loss:2.240948107995683\n",
      "train loss:2.2526108044995596\n",
      "train loss:2.2320539604127076\n",
      "=== epoch:62, train acc:0.21666666666666667, test acc:0.1664 ===\n",
      "train loss:2.244778518084197\n",
      "train loss:2.263860920335668\n",
      "train loss:2.2496985674481826\n",
      "=== epoch:63, train acc:0.22333333333333333, test acc:0.1744 ===\n",
      "train loss:2.2379783605291586\n",
      "train loss:2.221611929096206\n",
      "train loss:2.2313779227708017\n",
      "=== epoch:64, train acc:0.22, test acc:0.1783 ===\n",
      "train loss:2.24679398854459\n",
      "train loss:2.244072812606622\n",
      "train loss:2.231457336878319\n",
      "=== epoch:65, train acc:0.23, test acc:0.1857 ===\n",
      "train loss:2.228791074058676\n",
      "train loss:2.2288358506781494\n",
      "train loss:2.227053058883958\n",
      "=== epoch:66, train acc:0.22666666666666666, test acc:0.1852 ===\n",
      "train loss:2.2231199879605428\n",
      "train loss:2.2336173094192553\n",
      "train loss:2.1926478605899713\n",
      "=== epoch:67, train acc:0.22333333333333333, test acc:0.1875 ===\n",
      "train loss:2.20350795711821\n",
      "train loss:2.2253465873361766\n",
      "train loss:2.2275782378421733\n",
      "=== epoch:68, train acc:0.22333333333333333, test acc:0.1794 ===\n",
      "train loss:2.205759531952873\n",
      "train loss:2.218938862146997\n",
      "train loss:2.240833481975424\n",
      "=== epoch:69, train acc:0.22333333333333333, test acc:0.1889 ===\n",
      "train loss:2.2132915124188104\n",
      "train loss:2.204976146518754\n",
      "train loss:2.229326990111686\n",
      "=== epoch:70, train acc:0.22666666666666666, test acc:0.1884 ===\n",
      "train loss:2.2142187891318827\n",
      "train loss:2.224364403321863\n",
      "train loss:2.21268125106613\n",
      "=== epoch:71, train acc:0.22333333333333333, test acc:0.1806 ===\n",
      "train loss:2.2191724337403627\n",
      "train loss:2.2126975857634155\n",
      "train loss:2.203116333465936\n",
      "=== epoch:72, train acc:0.22, test acc:0.1745 ===\n",
      "train loss:2.2123116974914363\n",
      "train loss:2.2205433824468797\n",
      "train loss:2.224812339088295\n",
      "=== epoch:73, train acc:0.22, test acc:0.1733 ===\n",
      "train loss:2.236424384062444\n",
      "train loss:2.222467971728229\n",
      "train loss:2.216663644572433\n",
      "=== epoch:74, train acc:0.22, test acc:0.1774 ===\n",
      "train loss:2.2356919275356963\n",
      "train loss:2.2024042847210548\n",
      "train loss:2.2284156655529284\n",
      "=== epoch:75, train acc:0.22, test acc:0.1755 ===\n",
      "train loss:2.2027349038550716\n",
      "train loss:2.2353942173802843\n",
      "train loss:2.2334171087016728\n",
      "=== epoch:76, train acc:0.22333333333333333, test acc:0.175 ===\n",
      "train loss:2.194512277130417\n",
      "train loss:2.222225884345083\n",
      "train loss:2.198552277306271\n",
      "=== epoch:77, train acc:0.22333333333333333, test acc:0.1779 ===\n",
      "train loss:2.201412209925493\n",
      "train loss:2.1995694023425565\n",
      "train loss:2.217050296326474\n",
      "=== epoch:78, train acc:0.22333333333333333, test acc:0.1802 ===\n",
      "train loss:2.217751152086846\n",
      "train loss:2.2265426979661345\n",
      "train loss:2.2071170378557823\n",
      "=== epoch:79, train acc:0.22666666666666666, test acc:0.1794 ===\n",
      "train loss:2.2037820347031585\n",
      "train loss:2.2141254903650616\n",
      "train loss:2.19375359026363\n",
      "=== epoch:80, train acc:0.23, test acc:0.1859 ===\n",
      "train loss:2.200192940284803\n",
      "train loss:2.1652653788985465\n",
      "train loss:2.192375651634474\n",
      "=== epoch:81, train acc:0.22666666666666666, test acc:0.1802 ===\n",
      "train loss:2.229945325010449\n",
      "train loss:2.1829431728338538\n",
      "train loss:2.204810229506187\n",
      "=== epoch:82, train acc:0.22666666666666666, test acc:0.1809 ===\n",
      "train loss:2.2145905422697494\n",
      "train loss:2.2065305407096587\n",
      "train loss:2.202724366125681\n",
      "=== epoch:83, train acc:0.22666666666666666, test acc:0.1865 ===\n",
      "train loss:2.2142935638765264\n",
      "train loss:2.2070981146122226\n",
      "train loss:2.1555668994387345\n",
      "=== epoch:84, train acc:0.22333333333333333, test acc:0.1816 ===\n",
      "train loss:2.19875529065937\n",
      "train loss:2.2087242223863046\n",
      "train loss:2.2074613643367984\n",
      "=== epoch:85, train acc:0.22333333333333333, test acc:0.1805 ===\n",
      "train loss:2.204031954750165\n",
      "train loss:2.1581563327067674\n",
      "train loss:2.2070842512421787\n",
      "=== epoch:86, train acc:0.23666666666666666, test acc:0.184 ===\n",
      "train loss:2.198959992534621\n",
      "train loss:2.1968079583192255\n",
      "train loss:2.165718334156807\n",
      "=== epoch:87, train acc:0.23666666666666666, test acc:0.1872 ===\n",
      "train loss:2.200732910638862\n",
      "train loss:2.1644760694562692\n",
      "train loss:2.191239821641155\n",
      "=== epoch:88, train acc:0.23666666666666666, test acc:0.1861 ===\n",
      "train loss:2.158816274973532\n",
      "train loss:2.180166072642536\n",
      "train loss:2.2071099993992704\n",
      "=== epoch:89, train acc:0.23666666666666666, test acc:0.1864 ===\n",
      "train loss:2.1897176498617617\n",
      "train loss:2.2126140220142467\n",
      "train loss:2.166945889016192\n",
      "=== epoch:90, train acc:0.23, test acc:0.1816 ===\n",
      "train loss:2.2033077240115717\n",
      "train loss:2.1957095419132413\n",
      "train loss:2.1808625554191714\n",
      "=== epoch:91, train acc:0.23666666666666666, test acc:0.1846 ===\n",
      "train loss:2.2064537818071885\n",
      "train loss:2.1800814067830574\n",
      "train loss:2.181776135028157\n",
      "=== epoch:92, train acc:0.24, test acc:0.1878 ===\n",
      "train loss:2.16608474724626\n",
      "train loss:2.19200748348963\n",
      "train loss:2.1680134760914176\n",
      "=== epoch:93, train acc:0.24, test acc:0.1864 ===\n",
      "train loss:2.177843103607907\n",
      "train loss:2.184021151101204\n",
      "train loss:2.177806204217835\n",
      "=== epoch:94, train acc:0.24333333333333335, test acc:0.1928 ===\n",
      "train loss:2.144102721058745\n",
      "train loss:2.1753079116423946\n",
      "train loss:2.2050180794254546\n",
      "=== epoch:95, train acc:0.24333333333333335, test acc:0.1873 ===\n",
      "train loss:2.1811167876530315\n",
      "train loss:2.169044162081278\n",
      "train loss:2.180020057849763\n",
      "=== epoch:96, train acc:0.24666666666666667, test acc:0.1962 ===\n",
      "train loss:2.1712542767333267\n",
      "train loss:2.1755199311275932\n",
      "train loss:2.1704568268767357\n",
      "=== epoch:97, train acc:0.24666666666666667, test acc:0.1948 ===\n",
      "train loss:2.1879370475592705\n",
      "train loss:2.176987791081528\n",
      "train loss:2.1596465549278756\n",
      "=== epoch:98, train acc:0.24666666666666667, test acc:0.2001 ===\n",
      "train loss:2.1641193982334523\n",
      "train loss:2.1448225613647875\n",
      "train loss:2.1647386352920126\n",
      "=== epoch:99, train acc:0.24333333333333335, test acc:0.1984 ===\n",
      "train loss:2.1333162014030904\n",
      "train loss:2.1473182881903226\n",
      "train loss:2.133178179861175\n",
      "=== epoch:100, train acc:0.24666666666666667, test acc:0.2002 ===\n",
      "train loss:2.164727152139843\n",
      "train loss:2.175273697869148\n",
      "train loss:2.200867956672193\n",
      "=== epoch:101, train acc:0.26666666666666666, test acc:0.2078 ===\n",
      "train loss:2.154569654564934\n",
      "train loss:2.1786178445981865\n",
      "train loss:2.15234658337362\n",
      "=== epoch:102, train acc:0.26666666666666666, test acc:0.2095 ===\n",
      "train loss:2.171169849132387\n",
      "train loss:2.1926060819794615\n",
      "train loss:2.1616256853050113\n",
      "=== epoch:103, train acc:0.2733333333333333, test acc:0.2129 ===\n",
      "train loss:2.1695654641212183\n",
      "train loss:2.1700288591537884\n",
      "train loss:2.1310473477604024\n",
      "=== epoch:104, train acc:0.27666666666666667, test acc:0.215 ===\n",
      "train loss:2.1591485313353753\n",
      "train loss:2.170529869203131\n",
      "train loss:2.1373869707494912\n",
      "=== epoch:105, train acc:0.27666666666666667, test acc:0.218 ===\n",
      "train loss:2.1185894392070654\n",
      "train loss:2.138418976180708\n",
      "train loss:2.1419151676454145\n",
      "=== epoch:106, train acc:0.27666666666666667, test acc:0.2143 ===\n",
      "train loss:2.177283381187726\n",
      "train loss:2.1055803570106693\n",
      "train loss:2.1256911348790095\n",
      "=== epoch:107, train acc:0.27666666666666667, test acc:0.2137 ===\n",
      "train loss:2.1707272484043263\n",
      "train loss:2.1285101892631584\n",
      "train loss:2.1220509422043206\n",
      "=== epoch:108, train acc:0.27666666666666667, test acc:0.2133 ===\n",
      "train loss:2.148343811650619\n",
      "train loss:2.1811119717403176\n",
      "train loss:2.1143581276226815\n",
      "=== epoch:109, train acc:0.27666666666666667, test acc:0.2152 ===\n",
      "train loss:2.1496927754791924\n",
      "train loss:2.116198559091573\n",
      "train loss:2.152998195516568\n",
      "=== epoch:110, train acc:0.2833333333333333, test acc:0.2211 ===\n",
      "train loss:2.112843978726445\n",
      "train loss:2.1020311672475858\n",
      "train loss:2.131837303031061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:111, train acc:0.2866666666666667, test acc:0.2197 ===\n",
      "train loss:2.1170553508128522\n",
      "train loss:2.1391418496300703\n",
      "train loss:2.1487839585829556\n",
      "=== epoch:112, train acc:0.28, test acc:0.2226 ===\n",
      "train loss:2.165458822989123\n",
      "train loss:2.12806757321322\n",
      "train loss:2.1428419910657173\n",
      "=== epoch:113, train acc:0.2833333333333333, test acc:0.2243 ===\n",
      "train loss:2.143507132114284\n",
      "train loss:2.098994043726341\n",
      "train loss:2.122753658332104\n",
      "=== epoch:114, train acc:0.2833333333333333, test acc:0.2257 ===\n",
      "train loss:2.1493402579359198\n",
      "train loss:2.136189318461146\n",
      "train loss:2.132375728359377\n",
      "=== epoch:115, train acc:0.29333333333333333, test acc:0.2295 ===\n",
      "train loss:2.130342299551215\n",
      "train loss:2.1006442077742027\n",
      "train loss:2.1291672405346262\n",
      "=== epoch:116, train acc:0.2966666666666667, test acc:0.2336 ===\n",
      "train loss:2.1742782631487505\n",
      "train loss:2.127245890407715\n",
      "train loss:2.090043700869557\n",
      "=== epoch:117, train acc:0.3, test acc:0.238 ===\n",
      "train loss:2.1387532693814384\n",
      "train loss:2.108063375348438\n",
      "train loss:2.1216747767536885\n",
      "=== epoch:118, train acc:0.3, test acc:0.2374 ===\n",
      "train loss:2.125199129687585\n",
      "train loss:2.111356873318386\n",
      "train loss:2.1340455657878334\n",
      "=== epoch:119, train acc:0.30666666666666664, test acc:0.2432 ===\n",
      "train loss:2.0922557211577377\n",
      "train loss:2.1355539652747937\n",
      "train loss:2.1246026497528763\n",
      "=== epoch:120, train acc:0.30666666666666664, test acc:0.2423 ===\n",
      "train loss:2.086012058718261\n",
      "train loss:2.1164028477053902\n",
      "train loss:2.1980754929822344\n",
      "=== epoch:121, train acc:0.30666666666666664, test acc:0.2443 ===\n",
      "train loss:2.122790298170329\n",
      "train loss:2.064450875487\n",
      "train loss:2.118420200957408\n",
      "=== epoch:122, train acc:0.30333333333333334, test acc:0.2444 ===\n",
      "train loss:2.128765866959048\n",
      "train loss:2.079964893390246\n",
      "train loss:2.1255396760455425\n",
      "=== epoch:123, train acc:0.31, test acc:0.2463 ===\n",
      "train loss:2.0838409478891746\n",
      "train loss:2.1020647644594117\n",
      "train loss:2.1120245947689202\n",
      "=== epoch:124, train acc:0.31, test acc:0.2466 ===\n",
      "train loss:2.096502523409269\n",
      "train loss:2.1519478077280914\n",
      "train loss:2.101187875522329\n",
      "=== epoch:125, train acc:0.31, test acc:0.2511 ===\n",
      "train loss:2.1021018120977497\n",
      "train loss:2.0950367038088897\n",
      "train loss:2.1140459976286294\n",
      "=== epoch:126, train acc:0.31, test acc:0.2564 ===\n",
      "train loss:2.124531057876938\n",
      "train loss:2.1344078726281284\n",
      "train loss:2.0998034229168856\n",
      "=== epoch:127, train acc:0.31666666666666665, test acc:0.2632 ===\n",
      "train loss:2.0870966343066484\n",
      "train loss:2.1265710426475977\n",
      "train loss:2.0792536916237263\n",
      "=== epoch:128, train acc:0.3233333333333333, test acc:0.2663 ===\n",
      "train loss:2.080004649764565\n",
      "train loss:2.068419841463966\n",
      "train loss:2.0130431522913494\n",
      "=== epoch:129, train acc:0.31666666666666665, test acc:0.2621 ===\n",
      "train loss:2.088251834090669\n",
      "train loss:2.0319329480026798\n",
      "train loss:2.093645548067992\n",
      "=== epoch:130, train acc:0.31333333333333335, test acc:0.2641 ===\n",
      "train loss:2.026446942452469\n",
      "train loss:2.0965093776196237\n",
      "train loss:2.091146888470368\n",
      "=== epoch:131, train acc:0.31666666666666665, test acc:0.2649 ===\n",
      "train loss:2.0787110342731143\n",
      "train loss:2.0766715320117246\n",
      "train loss:2.017563059911838\n",
      "=== epoch:132, train acc:0.31666666666666665, test acc:0.2646 ===\n",
      "train loss:2.052350709677194\n",
      "train loss:2.1165824499258097\n",
      "train loss:2.0922600286331403\n",
      "=== epoch:133, train acc:0.31333333333333335, test acc:0.2667 ===\n",
      "train loss:2.0492077112409306\n",
      "train loss:2.103378411241441\n",
      "train loss:2.055264890212701\n",
      "=== epoch:134, train acc:0.31666666666666665, test acc:0.2694 ===\n",
      "train loss:2.0419926548106115\n",
      "train loss:2.0991373458880798\n",
      "train loss:2.121051837873485\n",
      "=== epoch:135, train acc:0.33666666666666667, test acc:0.2719 ===\n",
      "train loss:2.1244638967542775\n",
      "train loss:2.139284662057962\n",
      "train loss:2.0085469453624456\n",
      "=== epoch:136, train acc:0.34, test acc:0.276 ===\n",
      "train loss:2.1307938433323583\n",
      "train loss:2.060686867583708\n",
      "train loss:2.1305513340941817\n",
      "=== epoch:137, train acc:0.3433333333333333, test acc:0.2804 ===\n",
      "train loss:2.111525236500525\n",
      "train loss:2.020682444086876\n",
      "train loss:2.0960083826118425\n",
      "=== epoch:138, train acc:0.35, test acc:0.2823 ===\n",
      "train loss:2.063973857283772\n",
      "train loss:2.045049238679216\n",
      "train loss:2.058385699501089\n",
      "=== epoch:139, train acc:0.35, test acc:0.285 ===\n",
      "train loss:2.0554243651929283\n",
      "train loss:2.059303082347485\n",
      "train loss:2.1067677547065693\n",
      "=== epoch:140, train acc:0.35333333333333333, test acc:0.2858 ===\n",
      "train loss:2.0754841507636015\n",
      "train loss:2.0478914318106187\n",
      "train loss:2.0386837570380534\n",
      "=== epoch:141, train acc:0.35333333333333333, test acc:0.2864 ===\n",
      "train loss:2.019899463669918\n",
      "train loss:2.0694181378323786\n",
      "train loss:1.970732449419384\n",
      "=== epoch:142, train acc:0.35333333333333333, test acc:0.2885 ===\n",
      "train loss:2.083912144878181\n",
      "train loss:2.043704302042574\n",
      "train loss:2.069538756627132\n",
      "=== epoch:143, train acc:0.3566666666666667, test acc:0.2912 ===\n",
      "train loss:2.0049562606014644\n",
      "train loss:2.0168167584248984\n",
      "train loss:2.0184194809399587\n",
      "=== epoch:144, train acc:0.3566666666666667, test acc:0.2915 ===\n",
      "train loss:2.0344952381493444\n",
      "train loss:2.068072151745553\n",
      "train loss:2.0340081764567564\n",
      "=== epoch:145, train acc:0.3566666666666667, test acc:0.2948 ===\n",
      "train loss:1.9853540800314136\n",
      "train loss:2.0326905207731243\n",
      "train loss:2.0939933974250513\n",
      "=== epoch:146, train acc:0.36, test acc:0.298 ===\n",
      "train loss:2.048264526504702\n",
      "train loss:1.986387224225933\n",
      "train loss:2.032971218238337\n",
      "=== epoch:147, train acc:0.36, test acc:0.3004 ===\n",
      "train loss:2.0645301677342927\n",
      "train loss:1.9879053774651883\n",
      "train loss:1.9951407464650655\n",
      "=== epoch:148, train acc:0.3566666666666667, test acc:0.2983 ===\n",
      "train loss:2.014017475653483\n",
      "train loss:1.9641152927826895\n",
      "train loss:2.0302527396784606\n",
      "=== epoch:149, train acc:0.36333333333333334, test acc:0.2992 ===\n",
      "train loss:1.969439278925218\n",
      "train loss:1.9653723225697943\n",
      "train loss:1.9737906896568438\n",
      "=== epoch:150, train acc:0.36333333333333334, test acc:0.3024 ===\n",
      "train loss:1.9397292780626199\n",
      "train loss:2.0206581026317894\n",
      "train loss:2.0217920389783144\n",
      "=== epoch:151, train acc:0.36666666666666664, test acc:0.305 ===\n",
      "train loss:1.9709603971434775\n",
      "train loss:2.050498628373368\n",
      "train loss:2.0564722840410443\n",
      "=== epoch:152, train acc:0.37333333333333335, test acc:0.3096 ===\n",
      "train loss:2.001766856468781\n",
      "train loss:2.0246820487511754\n",
      "train loss:1.9950617863792564\n",
      "=== epoch:153, train acc:0.38333333333333336, test acc:0.3142 ===\n",
      "train loss:1.98365383189884\n",
      "train loss:2.0954660749655556\n",
      "train loss:1.9622125854995103\n",
      "=== epoch:154, train acc:0.38333333333333336, test acc:0.3178 ===\n",
      "train loss:1.9644903579243789\n",
      "train loss:1.9933269142515155\n",
      "train loss:1.984216083009541\n",
      "=== epoch:155, train acc:0.38333333333333336, test acc:0.3197 ===\n",
      "train loss:1.9489278370570549\n",
      "train loss:1.910292989837499\n",
      "train loss:1.9905518631814403\n",
      "=== epoch:156, train acc:0.38666666666666666, test acc:0.3184 ===\n",
      "train loss:1.980116246488076\n",
      "train loss:1.9378205530896673\n",
      "train loss:2.0363005570015544\n",
      "=== epoch:157, train acc:0.38333333333333336, test acc:0.3217 ===\n",
      "train loss:1.9479843893299458\n",
      "train loss:1.9734135505395562\n",
      "train loss:1.9321958635673966\n",
      "=== epoch:158, train acc:0.3933333333333333, test acc:0.3257 ===\n",
      "train loss:2.022719544549884\n",
      "train loss:2.007096629849923\n",
      "train loss:2.041457766209947\n",
      "=== epoch:159, train acc:0.3933333333333333, test acc:0.3342 ===\n",
      "train loss:1.9417238698683863\n",
      "train loss:1.93092033269599\n",
      "train loss:1.9856979710891054\n",
      "=== epoch:160, train acc:0.4033333333333333, test acc:0.334 ===\n",
      "train loss:1.9623227936129262\n",
      "train loss:1.9391134746575667\n",
      "train loss:1.9373362928673712\n",
      "=== epoch:161, train acc:0.4, test acc:0.3348 ===\n",
      "train loss:1.9502195407137295\n",
      "train loss:1.955746379893799\n",
      "train loss:1.9510395678347263\n",
      "=== epoch:162, train acc:0.39666666666666667, test acc:0.3361 ===\n",
      "train loss:2.04084190178954\n",
      "train loss:1.9394396396680267\n",
      "train loss:1.8393488939529075\n",
      "=== epoch:163, train acc:0.4033333333333333, test acc:0.3358 ===\n",
      "train loss:1.9325035555324677\n",
      "train loss:2.017955907508266\n",
      "train loss:1.882396864548327\n",
      "=== epoch:164, train acc:0.4066666666666667, test acc:0.3366 ===\n",
      "train loss:1.92795754523293\n",
      "train loss:1.895199752843952\n",
      "train loss:1.95011456814704\n",
      "=== epoch:165, train acc:0.41333333333333333, test acc:0.3378 ===\n",
      "train loss:1.933764962474928\n",
      "train loss:1.9092378719735132\n",
      "train loss:1.940107086820336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:166, train acc:0.41333333333333333, test acc:0.3427 ===\n",
      "train loss:1.9358498062773344\n",
      "train loss:1.9937229207092886\n",
      "train loss:2.019662958916123\n",
      "=== epoch:167, train acc:0.42, test acc:0.3483 ===\n",
      "train loss:1.925592393522414\n",
      "train loss:1.9494921715592672\n",
      "train loss:1.997776035603155\n",
      "=== epoch:168, train acc:0.42, test acc:0.3496 ===\n",
      "train loss:1.925464842985955\n",
      "train loss:1.867987832469881\n",
      "train loss:1.9214415308523005\n",
      "=== epoch:169, train acc:0.43333333333333335, test acc:0.3537 ===\n",
      "train loss:1.9494152845007813\n",
      "train loss:1.939153171226915\n",
      "train loss:1.881411691119614\n",
      "=== epoch:170, train acc:0.43, test acc:0.3535 ===\n",
      "train loss:1.8875126520322787\n",
      "train loss:1.8928753023767926\n",
      "train loss:1.9014869894668056\n",
      "=== epoch:171, train acc:0.43, test acc:0.3544 ===\n",
      "train loss:1.8555357819393135\n",
      "train loss:1.9025571445343052\n",
      "train loss:1.9241555270342965\n",
      "=== epoch:172, train acc:0.42333333333333334, test acc:0.3542 ===\n",
      "train loss:1.9028973247443686\n",
      "train loss:1.9114181612311159\n",
      "train loss:1.9924924308462815\n",
      "=== epoch:173, train acc:0.43666666666666665, test acc:0.3581 ===\n",
      "train loss:1.9013864029337688\n",
      "train loss:1.831942319321998\n",
      "train loss:1.976012181865754\n",
      "=== epoch:174, train acc:0.44666666666666666, test acc:0.3613 ===\n",
      "train loss:1.9446453506279884\n",
      "train loss:1.945556004164959\n",
      "train loss:1.8435172260258192\n",
      "=== epoch:175, train acc:0.45, test acc:0.3666 ===\n",
      "train loss:1.8660934613645062\n",
      "train loss:1.779190593821234\n",
      "train loss:1.917618372720205\n",
      "=== epoch:176, train acc:0.44666666666666666, test acc:0.3645 ===\n",
      "train loss:1.821402817516164\n",
      "train loss:1.9436776342904154\n",
      "train loss:1.8518104147367027\n",
      "=== epoch:177, train acc:0.45, test acc:0.3636 ===\n",
      "train loss:1.9957065604768838\n",
      "train loss:1.8063942200292948\n",
      "train loss:1.86221215505975\n",
      "=== epoch:178, train acc:0.4533333333333333, test acc:0.3664 ===\n",
      "train loss:1.9576206778251022\n",
      "train loss:1.8497404570308036\n",
      "train loss:1.7967373960908248\n",
      "=== epoch:179, train acc:0.4533333333333333, test acc:0.37 ===\n",
      "train loss:1.9465773629472392\n",
      "train loss:1.9249400054506571\n",
      "train loss:1.8635242435553705\n",
      "=== epoch:180, train acc:0.45666666666666667, test acc:0.3756 ===\n",
      "train loss:1.839450735813147\n",
      "train loss:1.8827328493347135\n",
      "train loss:1.9039388614249944\n",
      "=== epoch:181, train acc:0.45666666666666667, test acc:0.376 ===\n",
      "train loss:1.778365906567578\n",
      "train loss:1.7580099832212173\n",
      "train loss:1.9183649827949438\n",
      "=== epoch:182, train acc:0.4633333333333333, test acc:0.3757 ===\n",
      "train loss:1.8605661958193702\n",
      "train loss:1.8272379677326844\n",
      "train loss:1.8540609301373694\n",
      "=== epoch:183, train acc:0.4666666666666667, test acc:0.3775 ===\n",
      "train loss:1.8509840034090064\n",
      "train loss:1.8133314482920893\n",
      "train loss:1.7548003217985666\n",
      "=== epoch:184, train acc:0.4633333333333333, test acc:0.3816 ===\n",
      "train loss:1.8812371512952248\n",
      "train loss:1.8504734132356193\n",
      "train loss:1.8346628454025447\n",
      "=== epoch:185, train acc:0.4666666666666667, test acc:0.3827 ===\n",
      "train loss:1.8060115474688851\n",
      "train loss:1.8332096358598609\n",
      "train loss:1.8077229199179652\n",
      "=== epoch:186, train acc:0.4666666666666667, test acc:0.3838 ===\n",
      "train loss:1.825881464227432\n",
      "train loss:1.88585332708224\n",
      "train loss:1.7082627615320103\n",
      "=== epoch:187, train acc:0.4666666666666667, test acc:0.3873 ===\n",
      "train loss:1.911672524547466\n",
      "train loss:1.8531297498649026\n",
      "train loss:1.804741393090997\n",
      "=== epoch:188, train acc:0.48, test acc:0.3916 ===\n",
      "train loss:1.8471589618463562\n",
      "train loss:1.8182200740622114\n",
      "train loss:1.7658196410684868\n",
      "=== epoch:189, train acc:0.48, test acc:0.3931 ===\n",
      "train loss:1.7749313247479912\n",
      "train loss:1.8004670042990996\n",
      "train loss:1.8290522073713316\n",
      "=== epoch:190, train acc:0.48, test acc:0.396 ===\n",
      "train loss:1.7997425978426385\n",
      "train loss:1.7803677860949079\n",
      "train loss:1.8415612134613566\n",
      "=== epoch:191, train acc:0.49, test acc:0.4024 ===\n",
      "train loss:1.7601295097890344\n",
      "train loss:1.7654409808950882\n",
      "train loss:1.818178352302489\n",
      "=== epoch:192, train acc:0.4866666666666667, test acc:0.4058 ===\n",
      "train loss:1.7926989852456643\n",
      "train loss:1.724904626487512\n",
      "train loss:1.8085285147467436\n",
      "=== epoch:193, train acc:0.49333333333333335, test acc:0.4108 ===\n",
      "train loss:1.8198593345620688\n",
      "train loss:1.7836948868095568\n",
      "train loss:1.8113775369277514\n",
      "=== epoch:194, train acc:0.5033333333333333, test acc:0.4162 ===\n",
      "train loss:1.8631312965553513\n",
      "train loss:1.76868869195272\n",
      "train loss:1.700480893947363\n",
      "=== epoch:195, train acc:0.5033333333333333, test acc:0.4193 ===\n",
      "train loss:1.767880347727698\n",
      "train loss:1.767776431352325\n",
      "train loss:1.7394312369986076\n",
      "=== epoch:196, train acc:0.5033333333333333, test acc:0.4206 ===\n",
      "train loss:1.6941628498891552\n",
      "train loss:1.7870202408447338\n",
      "train loss:1.7876608589134575\n",
      "=== epoch:197, train acc:0.5033333333333333, test acc:0.4223 ===\n",
      "train loss:1.8104971158395804\n",
      "train loss:1.793716639362103\n",
      "train loss:1.7996452262439313\n",
      "=== epoch:198, train acc:0.5166666666666667, test acc:0.4274 ===\n",
      "train loss:1.9590638171618893\n",
      "train loss:1.7205211762983583\n",
      "train loss:1.8186984787177638\n",
      "=== epoch:199, train acc:0.5233333333333333, test acc:0.4333 ===\n",
      "train loss:1.6573165667427063\n",
      "train loss:1.7209192071890351\n",
      "train loss:1.740164382738924\n",
      "=== epoch:200, train acc:0.5266666666666666, test acc:0.4354 ===\n",
      "train loss:1.6872721480395927\n",
      "train loss:1.803865715084364\n",
      "train loss:1.6878079970718998\n",
      "=== epoch:201, train acc:0.5266666666666666, test acc:0.4346 ===\n",
      "train loss:1.6967541333555454\n",
      "train loss:1.7610062352605684\n",
      "train loss:1.651581258311812\n",
      "=== epoch:202, train acc:0.52, test acc:0.4337 ===\n",
      "train loss:1.7141337102725176\n",
      "train loss:1.6769254892693806\n",
      "train loss:1.7944049286667334\n",
      "=== epoch:203, train acc:0.5233333333333333, test acc:0.4348 ===\n",
      "train loss:1.7541847354519569\n",
      "train loss:1.70289272057832\n",
      "train loss:1.6802085141316931\n",
      "=== epoch:204, train acc:0.52, test acc:0.4351 ===\n",
      "train loss:1.686623570827322\n",
      "train loss:1.717724428737013\n",
      "train loss:1.5718486303882424\n",
      "=== epoch:205, train acc:0.5166666666666667, test acc:0.435 ===\n",
      "train loss:1.6437205115582258\n",
      "train loss:1.6965635268115438\n",
      "train loss:1.8381493946556007\n",
      "=== epoch:206, train acc:0.52, test acc:0.4377 ===\n",
      "train loss:1.6747246086217045\n",
      "train loss:1.6531626882726738\n",
      "train loss:1.7800316444782474\n",
      "=== epoch:207, train acc:0.52, test acc:0.4398 ===\n",
      "train loss:1.69169723450116\n",
      "train loss:1.6933041837878542\n",
      "train loss:1.6099240025173418\n",
      "=== epoch:208, train acc:0.5233333333333333, test acc:0.441 ===\n",
      "train loss:1.6220215363276165\n",
      "train loss:1.6032195544479537\n",
      "train loss:1.7164445162742445\n",
      "=== epoch:209, train acc:0.5266666666666666, test acc:0.4414 ===\n",
      "train loss:1.601885459708624\n",
      "train loss:1.5415817978859316\n",
      "train loss:1.725901050339406\n",
      "=== epoch:210, train acc:0.53, test acc:0.4446 ===\n",
      "train loss:1.649612489189109\n",
      "train loss:1.6855312269899017\n",
      "train loss:1.6254753509065423\n",
      "=== epoch:211, train acc:0.54, test acc:0.4531 ===\n",
      "train loss:1.5770140761019145\n",
      "train loss:1.6371315747699713\n",
      "train loss:1.5973760878290957\n",
      "=== epoch:212, train acc:0.5433333333333333, test acc:0.4577 ===\n",
      "train loss:1.7253696948115538\n",
      "train loss:1.6958417917719373\n",
      "train loss:1.606121218054982\n",
      "=== epoch:213, train acc:0.5466666666666666, test acc:0.4611 ===\n",
      "train loss:1.619752690856688\n",
      "train loss:1.6923712935471118\n",
      "train loss:1.5934263141091438\n",
      "=== epoch:214, train acc:0.5566666666666666, test acc:0.4643 ===\n",
      "train loss:1.5536145129690002\n",
      "train loss:1.655563918038468\n",
      "train loss:1.7334133413265425\n",
      "=== epoch:215, train acc:0.5566666666666666, test acc:0.4648 ===\n",
      "train loss:1.752587805350229\n",
      "train loss:1.6489984033107157\n",
      "train loss:1.5694110326638955\n",
      "=== epoch:216, train acc:0.5566666666666666, test acc:0.4683 ===\n",
      "train loss:1.6987696164882515\n",
      "train loss:1.622264854033645\n",
      "train loss:1.6779059567132548\n",
      "=== epoch:217, train acc:0.57, test acc:0.4753 ===\n",
      "train loss:1.6802297179606969\n",
      "train loss:1.703469948785438\n",
      "train loss:1.5976867808901125\n",
      "=== epoch:218, train acc:0.5833333333333334, test acc:0.4841 ===\n",
      "train loss:1.6247761556944114\n",
      "train loss:1.5414211837328224\n",
      "train loss:1.5517870517549701\n",
      "=== epoch:219, train acc:0.5733333333333334, test acc:0.4808 ===\n",
      "train loss:1.5723695051940887\n",
      "train loss:1.5300621981684737\n",
      "train loss:1.4857682802710144\n",
      "=== epoch:220, train acc:0.5666666666666667, test acc:0.4772 ===\n",
      "train loss:1.6708039243401038\n",
      "train loss:1.7003808992781253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:1.7227794545904689\n",
      "=== epoch:221, train acc:0.58, test acc:0.4831 ===\n",
      "train loss:1.7220207348323775\n",
      "train loss:1.7285023694558799\n",
      "train loss:1.718451191455105\n",
      "=== epoch:222, train acc:0.6033333333333334, test acc:0.4912 ===\n",
      "train loss:1.6648256226834863\n",
      "train loss:1.6185955323395393\n",
      "train loss:1.5573406420188474\n",
      "=== epoch:223, train acc:0.6033333333333334, test acc:0.494 ===\n",
      "train loss:1.5385716315007463\n",
      "train loss:1.6599273796555034\n",
      "train loss:1.6083228012905117\n",
      "=== epoch:224, train acc:0.6066666666666667, test acc:0.496 ===\n",
      "train loss:1.6964839442699093\n",
      "train loss:1.5682481425787842\n",
      "train loss:1.613566234699986\n",
      "=== epoch:225, train acc:0.6166666666666667, test acc:0.5023 ===\n",
      "train loss:1.5941349984329707\n",
      "train loss:1.7062687613525025\n",
      "train loss:1.6215155275669577\n",
      "=== epoch:226, train acc:0.6133333333333333, test acc:0.5044 ===\n",
      "train loss:1.4708447408828886\n",
      "train loss:1.5369734862518067\n",
      "train loss:1.5635847233022813\n",
      "=== epoch:227, train acc:0.6166666666666667, test acc:0.5062 ===\n",
      "train loss:1.5284929326036916\n",
      "train loss:1.6121972496500083\n",
      "train loss:1.6143026463538166\n",
      "=== epoch:228, train acc:0.6133333333333333, test acc:0.5053 ===\n",
      "train loss:1.5829174402632538\n",
      "train loss:1.6564376495887652\n",
      "train loss:1.5240906561725216\n",
      "=== epoch:229, train acc:0.6166666666666667, test acc:0.5079 ===\n",
      "train loss:1.5785648117259743\n",
      "train loss:1.4082969644847392\n",
      "train loss:1.4674637688276553\n",
      "=== epoch:230, train acc:0.6166666666666667, test acc:0.507 ===\n",
      "train loss:1.4787829438673987\n",
      "train loss:1.5189476190844675\n",
      "train loss:1.5463349839356477\n",
      "=== epoch:231, train acc:0.6166666666666667, test acc:0.507 ===\n",
      "train loss:1.4929812758763747\n",
      "train loss:1.5428908700912083\n",
      "train loss:1.5521031618970067\n",
      "=== epoch:232, train acc:0.6133333333333333, test acc:0.5099 ===\n",
      "train loss:1.5665888596634483\n",
      "train loss:1.533574314705052\n",
      "train loss:1.526265428801179\n",
      "=== epoch:233, train acc:0.6166666666666667, test acc:0.5105 ===\n",
      "train loss:1.4599385890855086\n",
      "train loss:1.4741909780672302\n",
      "train loss:1.5784010739674748\n",
      "=== epoch:234, train acc:0.62, test acc:0.5129 ===\n",
      "train loss:1.4544738698497282\n",
      "train loss:1.5513265101337388\n",
      "train loss:1.396485152394408\n",
      "=== epoch:235, train acc:0.62, test acc:0.5139 ===\n",
      "train loss:1.6336694085755556\n",
      "train loss:1.5001548240718139\n",
      "train loss:1.490338203071598\n",
      "=== epoch:236, train acc:0.63, test acc:0.5223 ===\n",
      "train loss:1.4046558865510428\n",
      "train loss:1.5296412938610777\n",
      "train loss:1.4979674743892883\n",
      "=== epoch:237, train acc:0.6333333333333333, test acc:0.5251 ===\n",
      "train loss:1.5209895601089625\n",
      "train loss:1.4819806446725965\n",
      "train loss:1.5498579285254044\n",
      "=== epoch:238, train acc:0.64, test acc:0.5307 ===\n",
      "train loss:1.408883666112282\n",
      "train loss:1.488436739257371\n",
      "train loss:1.502074619029458\n",
      "=== epoch:239, train acc:0.6333333333333333, test acc:0.5299 ===\n",
      "train loss:1.5477052882843996\n",
      "train loss:1.5032921085754456\n",
      "train loss:1.418311636537795\n",
      "=== epoch:240, train acc:0.63, test acc:0.5282 ===\n",
      "train loss:1.4601052180272631\n",
      "train loss:1.4137143866416115\n",
      "train loss:1.5228902805012081\n",
      "=== epoch:241, train acc:0.6233333333333333, test acc:0.527 ===\n",
      "train loss:1.411564721965832\n",
      "train loss:1.3855318588333898\n",
      "train loss:1.320918304319362\n",
      "=== epoch:242, train acc:0.6233333333333333, test acc:0.5257 ===\n",
      "train loss:1.398584998360165\n",
      "train loss:1.3600587417758228\n",
      "train loss:1.4699237944074388\n",
      "=== epoch:243, train acc:0.6266666666666667, test acc:0.5262 ===\n",
      "train loss:1.4741990925589112\n",
      "train loss:1.4491556254395968\n",
      "train loss:1.5193282189270434\n",
      "=== epoch:244, train acc:0.63, test acc:0.5292 ===\n",
      "train loss:1.4286229302119606\n",
      "train loss:1.4940572649266344\n",
      "train loss:1.5074999299117249\n",
      "=== epoch:245, train acc:0.63, test acc:0.5342 ===\n",
      "train loss:1.453535756424412\n",
      "train loss:1.3942603411991248\n",
      "train loss:1.441701778583927\n",
      "=== epoch:246, train acc:0.64, test acc:0.5363 ===\n",
      "train loss:1.462869650265689\n",
      "train loss:1.4667731374271\n",
      "train loss:1.3658686380193257\n",
      "=== epoch:247, train acc:0.65, test acc:0.5439 ===\n",
      "train loss:1.3524383321924833\n",
      "train loss:1.3704296268356264\n",
      "train loss:1.4157909433193554\n",
      "=== epoch:248, train acc:0.6433333333333333, test acc:0.5427 ===\n",
      "train loss:1.4211435077643761\n",
      "train loss:1.3486476570688561\n",
      "train loss:1.3787514420417566\n",
      "=== epoch:249, train acc:0.6466666666666666, test acc:0.5438 ===\n",
      "train loss:1.4571230725639277\n",
      "train loss:1.361458447936493\n",
      "train loss:1.4147566725174188\n",
      "=== epoch:250, train acc:0.65, test acc:0.5473 ===\n",
      "train loss:1.3427347401251821\n",
      "train loss:1.3179453208229837\n",
      "train loss:1.403726215015173\n",
      "=== epoch:251, train acc:0.6533333333333333, test acc:0.5481 ===\n",
      "train loss:1.2828428273701917\n",
      "train loss:1.4052493731496154\n",
      "train loss:1.5432713136241898\n",
      "=== epoch:252, train acc:0.6566666666666666, test acc:0.5497 ===\n",
      "train loss:1.1835354652572678\n",
      "train loss:1.455630174806399\n",
      "train loss:1.4270879906798581\n",
      "=== epoch:253, train acc:0.65, test acc:0.5518 ===\n",
      "train loss:1.2343577249827842\n",
      "train loss:1.4522032970230387\n",
      "train loss:1.5446526225166175\n",
      "=== epoch:254, train acc:0.6566666666666666, test acc:0.5553 ===\n",
      "train loss:1.2825318512485664\n",
      "train loss:1.4513334425605158\n",
      "train loss:1.401443392888236\n",
      "=== epoch:255, train acc:0.6566666666666666, test acc:0.5583 ===\n",
      "train loss:1.4381263722477737\n",
      "train loss:1.3976097956510234\n",
      "train loss:1.454668159047291\n",
      "=== epoch:256, train acc:0.66, test acc:0.5617 ===\n",
      "train loss:1.2566160287055523\n",
      "train loss:1.3040259169352435\n",
      "train loss:1.3512739858587657\n",
      "=== epoch:257, train acc:0.66, test acc:0.5601 ===\n",
      "train loss:1.3510220041486602\n",
      "train loss:1.398933397271768\n",
      "train loss:1.3560242932449693\n",
      "=== epoch:258, train acc:0.66, test acc:0.5642 ===\n",
      "train loss:1.3725520565219553\n",
      "train loss:1.3552635457735187\n",
      "train loss:1.2756995693696975\n",
      "=== epoch:259, train acc:0.66, test acc:0.5612 ===\n",
      "train loss:1.322461061544003\n",
      "train loss:1.2673856875513023\n",
      "train loss:1.285076693850014\n",
      "=== epoch:260, train acc:0.66, test acc:0.5598 ===\n",
      "train loss:1.3164378731737025\n",
      "train loss:1.3552415698238314\n",
      "train loss:1.3045101748477002\n",
      "=== epoch:261, train acc:0.66, test acc:0.5621 ===\n",
      "train loss:1.3689714969397693\n",
      "train loss:1.212588699718813\n",
      "train loss:1.301527525085076\n",
      "=== epoch:262, train acc:0.6566666666666666, test acc:0.5605 ===\n",
      "train loss:1.2665391316015275\n",
      "train loss:1.395522001766894\n",
      "train loss:1.371158361721342\n",
      "=== epoch:263, train acc:0.67, test acc:0.5677 ===\n",
      "train loss:1.3799911085358927\n",
      "train loss:1.2951363691635875\n",
      "train loss:1.3182652945017188\n",
      "=== epoch:264, train acc:0.6733333333333333, test acc:0.5703 ===\n",
      "train loss:1.3876327139726667\n",
      "train loss:1.2708125511475665\n",
      "train loss:1.2707624750663027\n",
      "=== epoch:265, train acc:0.6833333333333333, test acc:0.5722 ===\n",
      "train loss:1.2725528760743479\n",
      "train loss:1.3102477886431\n",
      "train loss:1.2194021928758023\n",
      "=== epoch:266, train acc:0.6766666666666666, test acc:0.573 ===\n",
      "train loss:1.2621904310373555\n",
      "train loss:1.254290804802673\n",
      "train loss:1.3423164481427086\n",
      "=== epoch:267, train acc:0.68, test acc:0.5749 ===\n",
      "train loss:1.2107717446072335\n",
      "train loss:1.2906231785197815\n",
      "train loss:1.3217014430968377\n",
      "=== epoch:268, train acc:0.6866666666666666, test acc:0.578 ===\n",
      "train loss:1.3166461488584253\n",
      "train loss:1.3079768858022731\n",
      "train loss:1.1666271218291147\n",
      "=== epoch:269, train acc:0.69, test acc:0.5786 ===\n",
      "train loss:1.3166550432963888\n",
      "train loss:1.1353030173803456\n",
      "train loss:1.2471083607745035\n",
      "=== epoch:270, train acc:0.6833333333333333, test acc:0.5792 ===\n",
      "train loss:1.3148219189318002\n",
      "train loss:1.1826602017937178\n",
      "train loss:1.1047359877608673\n",
      "=== epoch:271, train acc:0.6866666666666666, test acc:0.5822 ===\n",
      "train loss:1.2729758288147186\n",
      "train loss:1.2365629678876777\n",
      "train loss:1.1471852943476215\n",
      "=== epoch:272, train acc:0.69, test acc:0.5851 ===\n",
      "train loss:1.107168186715492\n",
      "train loss:1.1851172097644453\n",
      "train loss:1.3757964919382013\n",
      "=== epoch:273, train acc:0.69, test acc:0.5863 ===\n",
      "train loss:1.177833817057312\n",
      "train loss:1.1529357916202658\n",
      "train loss:1.187487872655559\n",
      "=== epoch:274, train acc:0.69, test acc:0.587 ===\n",
      "train loss:1.1298615363352056\n",
      "train loss:1.2306406657941071\n",
      "train loss:1.201944690358513\n",
      "=== epoch:275, train acc:0.6966666666666667, test acc:0.5866 ===\n",
      "train loss:1.297737104458394\n",
      "train loss:1.1739586882047521\n",
      "train loss:1.191109074907087\n",
      "=== epoch:276, train acc:0.6933333333333334, test acc:0.5862 ===\n",
      "train loss:1.1750689565555401\n",
      "train loss:1.1423312407441888\n",
      "train loss:1.1148858321568291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:277, train acc:0.6933333333333334, test acc:0.5886 ===\n",
      "train loss:1.2172418596086119\n",
      "train loss:1.2566740725674672\n",
      "train loss:1.1917811043272992\n",
      "=== epoch:278, train acc:0.6966666666666667, test acc:0.5888 ===\n",
      "train loss:1.2238618989897103\n",
      "train loss:1.2665784614555582\n",
      "train loss:1.134152977261786\n",
      "=== epoch:279, train acc:0.7033333333333334, test acc:0.592 ===\n",
      "train loss:1.119147155391352\n",
      "train loss:1.0825094836677407\n",
      "train loss:1.2257903012679179\n",
      "=== epoch:280, train acc:0.7, test acc:0.5923 ===\n",
      "train loss:1.2035306932449104\n",
      "train loss:1.2458859069017008\n",
      "train loss:1.143409662027954\n",
      "=== epoch:281, train acc:0.7, test acc:0.5939 ===\n",
      "train loss:1.2233981984637052\n",
      "train loss:1.1502182133487107\n",
      "train loss:1.119108609604547\n",
      "=== epoch:282, train acc:0.7, test acc:0.5952 ===\n",
      "train loss:1.065704215898557\n",
      "train loss:1.2225365887026864\n",
      "train loss:1.2183345318897758\n",
      "=== epoch:283, train acc:0.71, test acc:0.6021 ===\n",
      "train loss:1.1966792654931053\n",
      "train loss:1.1635141953098531\n",
      "train loss:1.1401396507281938\n",
      "=== epoch:284, train acc:0.72, test acc:0.6037 ===\n",
      "train loss:1.0224550485451138\n",
      "train loss:0.9856006066476374\n",
      "train loss:1.1615634110019275\n",
      "=== epoch:285, train acc:0.71, test acc:0.6001 ===\n",
      "train loss:1.165914443896747\n",
      "train loss:1.1177251219115716\n",
      "train loss:1.153298358967789\n",
      "=== epoch:286, train acc:0.71, test acc:0.6011 ===\n",
      "train loss:1.1645350597331112\n",
      "train loss:1.1761838026438807\n",
      "train loss:1.1555257043448892\n",
      "=== epoch:287, train acc:0.7133333333333334, test acc:0.6026 ===\n",
      "train loss:1.0629283705232335\n",
      "train loss:1.0454688931289642\n",
      "train loss:1.0649025238856786\n",
      "=== epoch:288, train acc:0.7033333333333334, test acc:0.5981 ===\n",
      "train loss:0.9460255038001435\n",
      "train loss:1.0455053059979007\n",
      "train loss:1.0734236303911695\n",
      "=== epoch:289, train acc:0.71, test acc:0.5949 ===\n",
      "train loss:1.266454287204148\n",
      "train loss:1.104404792062807\n",
      "train loss:0.9678507493325622\n",
      "=== epoch:290, train acc:0.71, test acc:0.5948 ===\n",
      "train loss:1.0617027348510553\n",
      "train loss:1.1370475369441335\n",
      "train loss:1.1023190779312841\n",
      "=== epoch:291, train acc:0.7033333333333334, test acc:0.6003 ===\n",
      "train loss:1.0901128920660077\n",
      "train loss:1.0635411432652913\n",
      "train loss:1.157863145706975\n",
      "=== epoch:292, train acc:0.7066666666666667, test acc:0.6008 ===\n",
      "train loss:1.1426352147872938\n",
      "train loss:1.0043782179654301\n",
      "train loss:1.1220804727565679\n",
      "=== epoch:293, train acc:0.71, test acc:0.6007 ===\n",
      "train loss:1.1537923235609504\n",
      "train loss:1.1472137384752434\n",
      "train loss:1.0784427796578862\n",
      "=== epoch:294, train acc:0.7133333333333334, test acc:0.6029 ===\n",
      "train loss:0.9966208435709105\n",
      "train loss:1.227781552996594\n",
      "train loss:0.9886915725381624\n",
      "=== epoch:295, train acc:0.7166666666666667, test acc:0.6049 ===\n",
      "train loss:1.084186660023184\n",
      "train loss:1.113913284846197\n",
      "train loss:1.0280546380866387\n",
      "=== epoch:296, train acc:0.7133333333333334, test acc:0.6037 ===\n",
      "train loss:1.1337972604847104\n",
      "train loss:1.1800556977195846\n",
      "train loss:1.0398012787786712\n",
      "=== epoch:297, train acc:0.72, test acc:0.6071 ===\n",
      "train loss:1.1352938109760635\n",
      "train loss:0.9776995540077188\n",
      "train loss:1.1977859857441129\n",
      "=== epoch:298, train acc:0.7166666666666667, test acc:0.6057 ===\n",
      "train loss:1.1370196066110219\n",
      "train loss:1.082138479086248\n",
      "train loss:1.0501373546755628\n",
      "=== epoch:299, train acc:0.7233333333333334, test acc:0.6083 ===\n",
      "train loss:0.9823859231020784\n",
      "train loss:1.1462753708108346\n",
      "train loss:1.1317018557269625\n",
      "=== epoch:300, train acc:0.7266666666666667, test acc:0.6107 ===\n",
      "train loss:0.999154232630064\n",
      "train loss:1.015487514907281\n",
      "train loss:1.0189480731188925\n",
      "=== epoch:301, train acc:0.7233333333333334, test acc:0.6127 ===\n",
      "train loss:1.04253009972982\n",
      "train loss:1.0626797281173512\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.6123\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1222a51d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.multi_layer_net_extend import MultiLayerNetExtend\n",
    "from common.trainer import Trainer\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "\n",
    "# 過学習を再現するために、学習データを削減\n",
    "x_train = x_train[:300]\n",
    "t_train = t_train[:300]\n",
    "\n",
    "# Dropuoutの有無、割り合いの設定 ========================\n",
    "use_dropout = True  # Dropoutなしのときの場合はFalseに\n",
    "dropout_ratio = 0.2\n",
    "# ====================================================\n",
    "\n",
    "network = MultiLayerNetExtend(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100],\n",
    "                              output_size=10, use_dropout=use_dropout, dropout_ration=dropout_ratio)\n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=301, mini_batch_size=100,\n",
    "                  optimizer='sgd', optimizer_param={'lr': 0.01}, verbose=True)\n",
    "trainer.train()\n",
    "\n",
    "train_acc_list, test_acc_list = trainer.train_acc_list, trainer.test_acc_list\n",
    "\n",
    "# グラフの描画==========\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
    "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* この実験は、7層のネットワーク(各層のニューロンの個数は100個、活性化関数はReLU)を使い、片方にはDropoutを適用し、もう片方には適用しない\n",
    "\n",
    "* 上の結果より、Dropoutを用いることで、訓練データとテストデータの認識精度の隔たりが小さくなった\n",
    "\n",
    "* Dropoutを用いれば、表現力の高いネットワークであっても、過学習を抑制することができる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 版   | 年/月/日   |\n",
    "| ---- | ---------- |\n",
    "| 初版 | 2019/05/12 |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
