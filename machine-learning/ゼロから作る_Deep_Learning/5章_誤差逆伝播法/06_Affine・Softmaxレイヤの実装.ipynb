{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "06　Affine・Softmaxレイヤの実装\n",
    "===========================\n",
    "\n",
    "## 1. Affineレイヤ\n",
    "\n",
    "* ニューラルネットワークの順伝播では、重み付き信号の総和を計算するため、行列の積を用いた(`np.dot()`)\n",
    "\n",
    "* 前の章では、以下のような実装を行なった"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 入力\n",
    "X = np.random.rand(2)\n",
    "\n",
    "# 重み\n",
    "W = np.random.rand(2, 3)\n",
    "\n",
    "# バイアス\n",
    "B = np.random.rand(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.dot(X, W) + B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ここで、`X`、`W`、`B`はそれぞれ形状が多次元配列となっている\n",
    "\n",
    "    * そうすることで、ニューロンの重み付き和は、`Y = np.dot(X, W) + B`のように計算できる\n",
    "    \n",
    "    * そして、この`Y`が活性化関数によって、変換され、次の層へと伝播される(NNにおける順伝播の流れ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 次に、ここで行なった計算(行列の積とバイアスの和)を計算グラフで表す\n",
    "\n",
    "    * 行列の積を計算するノードを「`dot`」として表すことにすると、`np.dot(X, W) + B`の計算は、以下の式で表すことができる\n",
    "    \n",
    "![Affileレイヤの計算グラフ](./images/Affileレイヤの計算グラフ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ここでは、「行列」がノード間を伝播する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 計算グラフの逆伝播\n",
    "\n",
    "* 行列を対象とした逆伝播を求める場合は、計算グラフと同じ手順で考えることができる\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial L}{\\partial \\vec{X}} = \\frac{\\partial L}{\\partial \\vec{Y}} \\cdot \\vec{W^T} \\\\\n",
    "\\frac{\\partial L}{\\partial \\vec{W}} =  \\vec {X^T} \\cdot\\frac{\\partial L}{\\partial \\vec{Y}}\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $\\vec{W^T}$の$T$：転置行列\n",
    "\n",
    "    * $\\vec{W}$の$(i, j)$の要素を、$(j, i)$の要素に入れ替えることを言う\n",
    "    \n",
    "\\begin{eqnarray}\n",
    "\\vec{W} = \n",
    "\\begin{pmatrix}\n",
    "w_{11} & w_{12} & w_{13} \\\\\n",
    "w_{21} & w_{22} & w_{23}\n",
    "\\end{pmatrix} \\\\\n",
    "\\vec{W^T} = \n",
    "\\begin{pmatrix}\n",
    "w_{11} & w_{21} \\\\\n",
    "w_{12} & w_{22} \\\\\n",
    "w_{13} & w_{23}\n",
    "\\end{pmatrix}\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 上の式を用いて、計算グラフの逆伝播を記述する\n",
    "\n",
    "    * ここで、 $\\vec{W}$と$\\frac{\\partial L}{\\partial \\vec{W}}$は同じ形状\n",
    "    \n",
    "\\begin{eqnarray}\n",
    "\\vec{W} = (x_0, x_1, \\cdots, x_n) \\\\\n",
    "\\frac{\\partial L}{\\partial \\vec{X}} =  \\Bigl(\n",
    "\\frac{\\partial L}{\\partial x_0}, \\frac{\\partial L}{\\partial x_1}, \\cdots, \\frac{\\partial L}{\\partial x_n}\n",
    "\\Bigr)\n",
    "\\end{eqnarray}\n",
    "\n",
    "![Affineレイヤの逆伝播](./images/Affineレイヤの逆伝播.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. バッチ版Affineレイヤ\n",
    "\n",
    "* ここで、N個のデータをまとめて順伝播する場合(バッチ)を考える\n",
    "\n",
    "    * 入力である$\\vec{X}$の形状が$(N, 2)$となる\n",
    "    \n",
    "![バッチ版Affineレイヤの計算グラフ](./images/バッチ版Affineレイヤの計算グラフ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ただし、順伝播の際のバイアスの加算は$\\vec{X} \\cdot \\vec{W}$に対して、バイアスがそれぞれのデータに加算される\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dot_W = np.array([[0, 0, 0], [10, 10, 10]])\n",
    "B = np.array([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0],\n",
       "       [10, 10, 10]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dot_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  3],\n",
       "       [11, 12, 13]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dot_W + B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 順伝播でのバイアスの加算は、それぞれのデータ(1個目のデータ、2個目のデータ、...)に対して加算が行われる\n",
    "\n",
    "* そのため、逆伝播の際には、それぞれのデータの逆伝播の値がバイアスの要素に集約される必要がある"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dY = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "dY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 7, 9])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dB = np.sum(dY, axis=0)\n",
    "dB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 上の例では、データが2個(N=2)あるものと仮定する\n",
    "\n",
    "* バイアスの逆伝播は、その2個のデータに対しての微分を、データごとに合算して求める\n",
    "\n",
    "    * そのため、`np.sum()`で、0番目の軸(データを単位とした軸)に対して(`axis=0`)の総和を求める"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 実装は、以下の通りになる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W =W\n",
    "        self.b = b\n",
    "        \n",
    "        self.x = None\n",
    "        self.original_x_shape = None\n",
    "        # 重み・バイアスパラメータの微分\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # テンソル対応\n",
    "        self.original_x_shape = x.shape\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        self.x = x\n",
    "\n",
    "        out = np.dot(self.x, self.W) + self.b\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        \n",
    "        dx = dx.reshape(*self.original_x_shape)  # 入力データの形状に戻す（テンソル対応）\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Softmax-with-Lossレイヤ\n",
    "\n",
    "* 出力層である`ソフトマックス関数`について扱う\n",
    "\n",
    "    * この関数は、入力された値を正規化して出力する\n",
    "    \n",
    "    * 例)手書き数字認識の場合\n",
    "    \n",
    "![入力画像とAffine・ReLUレイヤ](./images/入力画像とAffine・ReLUレイヤ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 上の図のように、Softmaxレイヤの出力は、入力された値を正規化(出力の和が`1`になるように変形)して出力する\n",
    "\n",
    "* ただし、手書き数字認識は、10クラス分類を行うため、Softmaxレイヤの入力は10個あることになる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ニューラルネットワークで行う処理には、`推論`と`学習`の2つのフェーズがある\n",
    ">\n",
    "> `推論`：通常、Softmaxレイヤは使用しない(Affineレイヤの出力結果を認識結果として用いる)\n",
    "> \n",
    "> `学習`：Softmaxレイヤが必要になる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Softmaxレイヤを実装していくが、損失関数である`交差エントロピー誤差`も含めて、「Softmax-with-Lossレイヤ」という名前のレイヤで実装する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Soft-with-Lossレイヤの計算グラフ](./images/Soft-with-Lossレイヤの計算グラフ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 上の図の通り、Softmax-with-Lossレイヤはやや複雑\n",
    "\n",
    "* この計算グラフを簡略化して書くと、以下の通りになる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![簡易版Softmax-with-Lossレイヤ](./images/簡易版Softmax-with-Lossレイヤ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 上の計算グラフでは、ソフトマックス関数はSoftmaxレイヤとして、`交差エントロピー誤差`はCross Entropy Errorレイヤとして表記する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ここでは、3クラス分類を行う場合を想定し、前レイヤから3つの入力(スコア)を受け取るものとする\n",
    "\n",
    "* Softmaxレイヤは、入力である$(a_1, a_2, a_3)$を正規化して、$(y_1, y_2, y_3)$を出力する\n",
    "\n",
    "* Cross Entropy Errorレイヤは、Softmaxの出力$(y_1, y_2, y_3)$と、教師ラベルの$(t_1, t_2, t_3)$を受け取り、それらのデータから損失$L$を出力する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 上の出力で注目すべきは、逆伝播の結果\n",
    "\n",
    "* Softmaxレイヤからの逆伝播は、$(y_1 - t_1, y_2 - t_2, y_3 - t_3)$という結果となる\n",
    "\n",
    "    * $(y_1, y_2, y_3)$：Softmaxレイヤの出力\n",
    "    \n",
    "    * $(t_1, t_2, t_3)$：教師データ\n",
    "    \n",
    "    * そのため、これはSoftmaxレイヤの出力と教師ラベルの差分となる\n",
    "    \n",
    "* ニューラルネットワークの逆伝播では、この差分である誤差が前レイヤへ伝わっていく\n",
    "\n",
    "    * これは、ニューラルネットワークの学習において重要な性質となる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ここで、ニューラルネットワークの学習の目的は、ニューラルネットワークの出力(Softmaxの出力)を教師ラベルに近づけるように、重みパラメータを調整すること\n",
    "\n",
    "    * そのため、ニューラルネットワークの出力と教師ラベルとの誤差を効率よく、前レイヤに伝える必要がある\n",
    "    \n",
    "    * そのため、この差分は現在のニューラルネットワークの出力と教師ラベルの誤差を素直に表している"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> `交差エントロピー誤差`は、このように綺麗な結果になるように作られた"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ここで具体例を考える\n",
    "\n",
    "* 例)教師ラベルが$(0,1, 0)$であるデータに対して、Softmaxレイヤの出力が$(0.3, 0.2, 0.5)$であった場合\n",
    "\n",
    "    * 正解ラベルに対する確率は$0.2$なので、この時点のニューラルネットワークは正しい認識ができていない\n",
    "    \n",
    "    * この場合、Softmaxレイヤからの逆伝播は、$(0.3, -0.8, 0.5)$という大きな誤差を伝播することになる\n",
    "    \n",
    "    * この大きな誤差が前レイヤに伝播していくので、Softmaxレイヤよりも前のレイヤは、その大きな誤差から大きな内容を学習することになる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 例)教師ラベルが$(0, 1, 0)$であるデータに対して、Softmaxレイヤの出力が$(0.01, 0.99, 0)$の場合\n",
    "\n",
    "    * この場合、Softmaxレイヤからの逆伝播は、$(0.01, -0.01, 0)$ という小さなgosaninaru\n",
    "    \n",
    "    * この小さな誤差が前レイヤに伝播していくが、その誤差は小さいため、Softmaxレイヤよりも前にあるレイヤが学習する内容も小さくなる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 実装は、以下の通りになる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None # softmaxの出力\n",
    "        self.t = None # 教師データ\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        \n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        if self.t.size == self.y.size: # 教師データがone-hot-vectorの場合\n",
    "            dx = (self.y - self.t) / batch_size\n",
    "        else:\n",
    "            dx = self.y.copy()\n",
    "            dx[np.arange(batch_size), self.t] -= 1\n",
    "            dx = dx / batch_size\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 版   | 年/月/日   |\n",
    "| ---- | ---------- |\n",
    "| 初版 | 2019/05/11 |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
