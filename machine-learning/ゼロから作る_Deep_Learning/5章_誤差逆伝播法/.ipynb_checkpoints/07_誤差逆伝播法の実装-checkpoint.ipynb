{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "07 誤差逆伝播法の実装\n",
    "==================\n",
    "\n",
    "* 実装したレイヤを組み合わせることで、レゴブロックを組み合わせて作るように、ニューラルネットワークを構築することができる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ニューラルネットワークの学習の全体図\n",
    "\n",
    "### 前提\n",
    "\n",
    "* ニューラルネットワークは、適応可能な重みとバイアスがあり、この重みとバイアスを訓練データに適用することに調整することを、`学習`と呼ぶ\n",
    "\n",
    "* ニューラルネットワークの学習は、次の4つの手順で行う\n",
    "\n",
    "### ステップ1(ミニバッチ)\n",
    "\n",
    "* 訓練データの中からランダムに一部のデータを選び出す\n",
    "\n",
    "### ステップ2(勾配の算出)\n",
    "\n",
    "* 各重みパラメータに関する損失関数の勾配を求める\n",
    "\n",
    "### ステップ3(パラメータの更新)\n",
    "\n",
    "* 重みパラメータを勾配方向に微小量だけ更新する\n",
    "\n",
    "### ステップ4(繰り返す)\n",
    "\n",
    "* ステップ1、ステップ2、ステップ3を繰り返す"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 誤差逆伝播法が登場するのは、ステップ2の「勾配の算出」\n",
    "\n",
    "* 前章では、この勾配を求めるために`数値微分`を用いたが、`誤差逆伝播法`を用いることで、高速に効率良く勾配を求めることができる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 誤差逆伝播法に対応したニューラルネットワークの実装\n",
    "\n",
    "* ここでは、2層のニューラルネットワークを`TwoLayerNet`として実装する\n",
    "\n",
    "* このインスタンス変数は、以下の表にまとめた\n",
    "\n",
    "    * 変更箇所は、レイヤを使用していること\n",
    "    \n",
    "    * 認識結果を得る処理(`predict()`)や、勾配を求める処理(`gradient()`)がレイヤの伝播だけで達成できる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 変数        | 説明                                                                                                                                                                                                             |\n",
    "| ----------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| `params`    | ニューラルネットワークのパラメータを保持するディクショナリ変数(インスタンス変数)<br>`params['W1']`は1層目の重み、`params['b1']`は1層目のバイアス<br>`params['W2']`は2層目の重み、`params['b2']`は2層目のバイアス |\n",
    "| `layers`    | ニューラルネットワークのレイヤを保持する**順番付きディクショナリ変数**<br>`layers['Affine1']`、`layers['Relu1']`、`layers['Affine2']`<br>と言ったように、**順番付きディクショナリ**でレイヤを保持する            |\n",
    "| `lastLayer` | ニューラルネットワークの最後のレイヤ<br>この例では、`SoftmaxWithLoss`レイヤ                                                                                                                                      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| メソッド                                                                | 説明                                                                                                                                                                   |\n",
    "| ----------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| `__init__(self, input_size, hidden_size, output_size, weight_init_std)` | 初期化を行う<br>引数は頭から順に、<br>「入力層のニューロンの数」<br>「隠れ層のニューロンの数」<br>「出力層のニューロンの数」<br>「重み初期化時のガウス分布のスケール」 |\n",
    "| `predict(self, x)`                                                      | 認識(推論)を行う<br>引数の`x`は画像データ                                                                                                                              |\n",
    "| `loss(self, x, t)`                                                      | 損失関数の値を求める<br>引数の`x`は画像データ、`t`は正解ラベル                                                                                                         |\n",
    "| `accuracy(self, x, t)`                                                  | 認識精度を求める                                                                                                                                                       |\n",
    "| `numerical_gradient(self, x, t)`                                        | 重みパラメータに対する勾配を求める                                                                                                                                     |\n",
    "| `gradient(self, x, t)`                                                  | 重みパラメータに対する勾配を求める<br>`numerical_gradient()`の高速版                                                                                                   |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import numpy as np\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
    "        # 重みの初期化\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size) \n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "        # レイヤの生成\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "\n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    # x:入力データ, t:教師データ\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    # x:入力データ, t:教師データ\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "        \n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 設定\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* この実装では、レイヤを`OrderDict`として保存する\n",
    "\n",
    "    * 順番付きのディクショナリであり、追加した順にレイヤの`forward()`メソッドを呼び出すだけで処理が完了する\n",
    "    \n",
    "    * また、逆伝播では逆の順番でレイヤを呼び出す\n",
    "    \n",
    "        * AffineレイヤとReLUレイヤが、それぞれの内部で順伝播と逆伝播を正しく処理してくれるので、レイヤを正しい順番で連結し、順番にレイヤを呼び出す"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* このようにニューラルネットワークの構成要素を「レイヤ」として実装したので、ニューラルネットワークを簡単に構築できた"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 誤差逆伝播法の勾配確認\n",
    "\n",
    "* 誤差逆伝播法を用いることで、大量のパラメータが存在しても効率的に計算できた\n",
    "\n",
    "    * そのため、計算に時間のかかる数値微分ではなく、誤差逆伝播法によって勾配を求める\n",
    "    \n",
    "* 数値微分は、誤差逆伝播法の正しさを確認する上では必要なツールとなる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 数値微分の利点は、実装が簡単であること\n",
    "\n",
    "    * 実装にはミスが起きにくく、誤差逆伝播法の結果を比較して、実装の正しさを確認することができる\n",
    "    \n",
    "    　* この数値微分で勾配を求めた結果と、誤差逆伝播法で求めた勾配の結果が一致することを確認する作業を、`勾配確認`と呼ぶ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1:3.58656116260018e-10\n",
      "b1:2.089933073566369e-09\n",
      "W2:4.880730265797046e-09\n",
      "b2:1.397661387195215e-07\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "# データの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "x_batch = x_train[:3]\n",
    "t_batch = t_train[:3]\n",
    "\n",
    "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
    "grad_backprop = network.gradient(x_batch, t_batch)\n",
    "\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.average( np.abs(grad_backprop[key] - grad_numerical[key]) )\n",
    "    print(key + \":\" + str(diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ここでは誤差として、各重みパラメータにおける要素の差の絶対値を求め、その平均を算出する\n",
    "\n",
    "* この結果から、数値微分と誤差逆伝播法でそれぞれ求めた勾配の差はかならい小さいことがわかる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 誤差逆伝播法を用いた学習\n",
    "\n",
    "* 実装は、以下の通りになる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11235 0.1122\n",
      "0.9033 0.9066\n",
      "0.9241333333333334 0.9244\n",
      "0.93685 0.9347\n",
      "0.9445666666666667 0.9408\n",
      "0.94975 0.946\n",
      "0.9547 0.9516\n",
      "0.96085 0.9549\n",
      "0.96485 0.9567\n",
      "0.9666833333333333 0.9607\n",
      "0.9692166666666666 0.963\n",
      "0.9709833333333333 0.9632\n",
      "0.9727 0.9649\n",
      "0.9742333333333333 0.9666\n",
      "0.9753 0.9666\n",
      "0.9767833333333333 0.9665\n",
      "0.9783 0.9689\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "# データの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 勾配\n",
    "    #grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    # 更新\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(train_acc, test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 版   | 年/月/日   |\n",
    "| ---- | ---------- |\n",
    "| 初版 | 2019/05/11 |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
