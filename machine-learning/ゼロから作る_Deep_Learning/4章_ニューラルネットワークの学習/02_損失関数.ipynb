{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "02 損失関数\n",
    "==========\n",
    "\n",
    "* ニューラルネットワークでは、ある「ひとつの指標」によって現在の状態を表す\n",
    "\n",
    "    * そして、その指標を基準として、最適な重みパラメータの探索を行う\n",
    "    \n",
    "    * ニューラルネットワークの学習で用いられる指標は、`損失関数`と呼ばれる\n",
    "    \n",
    "* この`損失関数`は、任意の関数を用いることができるが、一般には`2乗和誤差`や`交差エントロピー誤差`などが用いられる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> `損失関数`：ニューラルネットワークの性能の\"悪さ\"を表す指標\n",
    ">\n",
    "> 現在のニューラルネットワークが教師データに対してどれだけ適合していないか、教師データに対してどれだけ一致していないかということを表す"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 2乗和誤差\n",
    "\n",
    "* `2乗和誤差`：損失関数の中でも最も有名\n",
    "\n",
    "    * 以下の数式で表される\n",
    "    \n",
    "    * $y_k$：ニューラルネットワークの出力\n",
    "    \n",
    "    * $t_k$：教師データ\n",
    "    \n",
    "    * $k$：データの次元数\n",
    "    \n",
    "\\begin{eqnarray}\n",
    "E = \\frac{1}{2}\\sum_{k} (y_k-t_k)^2 \n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 手書き数字の例では、$y_k$、$t_k$は次のような10個の要素からなるデータ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* この配列の要素は、最初のインデックスから順に、数字の「0」、「1」、「2」...に対応する\n",
    "\n",
    "* ここでニューラルネットワークの出力である`y`は、`ソフトマックス関数`の出力\n",
    "    \n",
    "    * `ソフトマックス関数`の出力は確率として解釈できるので、上の例では「0」の確率は0.1、「1」の確率は0.05、「2」の確率は0.6となる\n",
    "    \n",
    "* 一方、`t`は教師データ\n",
    "\n",
    "    * 正解となるラベルと`1`、それ以外を`0`とする\n",
    "    \n",
    "    * ここではラベルの「2」が`1`なので、正解は「2」であることを表している\n",
    "    \n",
    "* 正解ラベルを`1`として、それ以外は`0`で表す表記法を、`One-Hot表現`と呼ぶ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 2乗和誤差は、ニューラルネットワークの出力となる教師データの各要素の差の2乗を計算し、その総和を求める"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y, t):\n",
    "    return 0.5 * np.sum((y - t)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ここで、引数の`y`と`t`は、NumPyの配列とする\n",
    "\n",
    "    * 実際に計算を行なってみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 「2」を正解とする\n",
    "t =  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09750000000000003"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 例1：「2」の確率が最も高い場合(0.6)\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "mean_squared_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5975"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 例2：「7」の確率が最も高い場合(0.6)\n",
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "mean_squared_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1つ目は、正解を「2」として、ニューラルネットワークの出力が「2」で最も高い場合\n",
    "\n",
    "* 2つ目は、正解は「2」だが、ニューラルネットワークの出力は「7」で最も高くなっている\n",
    "\n",
    "* この結果では、1つ目の例の損失関数の方が小さくなっており、教師データとの誤差が小さいことがわかる\n",
    "\n",
    "    * つまり、1つ目の例の方が、出力結果が教師データより適合していることを2乗和誤差は示している"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 交差エントロピー誤差\n",
    "\n",
    "* `交差エントロピー誤差`：`2乗和誤差`とは別の損失関数\n",
    "\n",
    "    * $y_k$：ニューラルネットワークの出力\n",
    "    \n",
    "    * $t_k$：正解ラベル(正解ラベルとなるインデックスだけが`1`で、それ以外は`0`)\n",
    "    \n",
    "\\begin{eqnarray}\n",
    "E = -\\sum_{k} t_k \\log y_k\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 実質的には正解ラベルが`1`に対応する出力の自然対数を計算するだけになる\n",
    "\n",
    "    * 正解ラベルとなる出力の結果によって、その値が決まる\n",
    "    \n",
    "    * 正解ラベルに対応する出力が大きければ大きいほど`0`に近づく\n",
    "    \n",
    "    * 出力が`1`のとき交差エントロピー誤差は`0`になる\n",
    "    \n",
    "    * 正解ラベルに対応する出力が小さければ、値は大きくなる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 実際に、交差エントロピー誤差を実装する\n",
    "\n",
    "    * 引数の`y`と`t`は、NumPyの配列とする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t * np.log(y + delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "t =  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.510825457099338"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.302584092994546"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "cross_entropy_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1つ目の例では、正解となるラベルの出力0.6の場合で、交差エントロピー誤差はおよそ`0.51`\n",
    "\n",
    "* その次は、正解となるラベルの出力が0.1と低い場合の例だが、この時の交差エントロピー誤差は`2.3`\n",
    "\n",
    "    * これまでの議論と一致していることがわかる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ミニバッチ学習\n",
    "\n",
    "* 機械学習の問題は、訓練データを使って学習を行う\n",
    "\n",
    "    * 訓練データに対する`損失関数`を求め、その値をできるだけ小さくするようなパラメータを探し出す\n",
    "    \n",
    "    * そのため、`損失関数`は、全ての訓練データを対象として求める必要がある\n",
    "    \n",
    "    * つまり、訓練データが100個あれば、その100個の損失関数の和を指標とする"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 先ほどの損失関数の例は、一つのデータの損失関数を考えていた\n",
    "\n",
    "    * 訓練データの全ての損失関数の和を求めたいとする(交差エントロピー誤差の場合)\n",
    "\n",
    "\\begin{eqnarray}\n",
    "E = - \\frac{1}{N} \\sum_{n} \\sum_{k} t_{nk} \\log y_{nk}\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ここで、データが$N$個あるとして、$t_{nk}$は$n$個目のデータの$k$番目の値を意味する\n",
    "\n",
    "    * $y_{nk}$：ニューラルネットワークの出力\n",
    "    \n",
    "    * $t_{nk}$：教師データ\n",
    "    \n",
    "* 一つのデータに対する損失関数を、単に$N$個分のデータに拡張して、最後に$N$で割って正規化する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ここで、MNISTのデータセットは訓練データが60,000個あった\n",
    "\n",
    "    * そのため、全てのデータを対象にして損失関数の和を求めるには少々時間がかかる\n",
    "    \n",
    "* そこで、データの中の一部を選び出し、その一部のデータを全体の「近似」として利用する\n",
    "\n",
    "    * これを`ミニバッチ`と呼び、ミニバッチごとに学習を行う\n",
    "    \n",
    "    * 60,000枚の訓練データの中から、100枚を無作為に選び出して、その100枚を使って学習を行う(`ミニバッチ学習`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `ミニバッチ学習`のために、訓練データの中から指定された個数のデータをランダムに選び出すコードを書いてみる\n",
    "\n",
    "    * それに先立ち、MNISTデータセットを読み込むためのコードを記述する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(t_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* この訓練データの中からランダムに10枚だけ抜き出すには、NumPyの`np.random.choice()`を使って、次のように書くことができる\n",
    "\n",
    "    * `np.random.choice(60000, 10)`：0から60000未満の数字の中からランダムに10個の数字を選び出す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = x_train.shape[0]\n",
    "batch_size = 10\n",
    "batch_mask = np.random.choice(train_size, batch_size)\n",
    "x_batch = x_train[batch_mask]\n",
    "t_batch = t_train[batch_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 実際のコードで示すと、ミニバッチとして選び出すインデックスを配列として取得できる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([15155, 28764, 37764, 59840, 52865, 43911, 56091,  9382, 36223,\n",
       "       40288])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(60000, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* このランダムに選ばれたインデックスを指定して、ミニバッチを取り出し、`損失関数`を計算する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. [バッチ対応版] 交差エントロピー誤差の実装\n",
    "\n",
    "* 先ほど実装した`交差エントロピー誤差`を改良することで、バッチデータに対応したものが実装できる\n",
    "\n",
    "    * ここでは、データが一つの場合と、データがバッチとしてまとめられて入力される場合の両方のケースに対応するように実装する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t * np.log(y + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ここで、$y$はニューラルネットワークの出力、$t$は教師データとする\n",
    "\n",
    "    * $y$の次元数が1の場合(データ一つあたりの交差エントロピー誤差を求める場合)は、データの形状を整形する\n",
    "    \n",
    "    * そして、バッチの枚数で正規化し、1枚あたりの平均の交差エントロピー誤差を計算する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* また、教師データがラベルとして与えられたとき(「2」、「7」と与えられる)、交差エントロピー誤差は次のように実装することができる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 実装のポイントは、One-Hot表現で`t`が`0`の要素は、交差エントロピーも`0`であるから、その計算も無視して良いということ\n",
    "\n",
    "    * 解ラベルに対して、ニューラルネットワークの出力を得ることができれば、交差エントロピー誤差を計算することができる\n",
    "    \n",
    "    * そのため、`t`がOne-Hotラベルの時は`t * np.log(y)`で計算していたところは、`t`がラベル表現の場合は`np.log(y[np.arange(batch_size), t])`とする"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `np.arrange(batch_size)`は`0`から`batch_size - 1`までの配列を生成する\n",
    "\n",
    "    * `batch_size`が`5`としたら、`np.arange(batch_size)`は`[0, 1, 2, 3, 4]`のNumPy配列を生成する\n",
    "    \n",
    "    * `t`にはラベルが`[2, 7, 0, 9, 4]`のように格納されているので、`y[np.arange(batch_size), t]`は、各データの正解ラベルに対応するNNの出力を抽出\n",
    "    \n",
    "        * この例では、`[y[0,2], y[1,7], y[2,0], y[3,9], y[4,4]]`のNumPy配列を生成する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. なぜ損失関数を設定するのか？\n",
    "\n",
    "* ニューラルネットワークの学習における「微分」の役割に注目すると、損失関数の導入の理由がわかる\n",
    "\n",
    "    * NNの学習では、最適なパラメータ(重みとバイアス)を探索する際に、損失関数の値ができるだけ小さくなるようなパラメータを探す\n",
    "    \n",
    "    * ここで、できるだけ小さな損失関数の場所を探すために、バラメータの微分(勾配)を計算し、その微分の値を手がかりにパラメータの値を徐々に更新する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ここに仮想上のニューラルネットワークがあったとして、そのニューラルネットワークのある一つの重みパラメータに注目するとする\n",
    "\n",
    "    * この時、その一つの重みパラメータの損失関数に対する微分は、「その重みパラメータの値を少しだけ変化させた時に、損失関数がどのように変化するか」\n",
    "    \n",
    "    * もし微分の値がマイナスとなれば、その重みパラメータを正の方向へ変化させることで、損失関数を減少させることができる\n",
    "    \n",
    "    * もし微分の値がプラスとなれば、その重みパラメータを負の方向へ変化させることで、損失関数を減少させることができる\n",
    "    \n",
    "    * しかし、微分の値が`0`になると、重みパラメータをどちらに動かしても、損失関数の値が変わらないため、その重みパラメータの更新はストップする"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 認識精度を指標にしてはいけない理由は、微分がほとんどの場所で0になってしまい、パラメータの更新ができなくなってしまうため\n",
    "\n",
    "> ニューラルネットワークの学習の際に、認識精度を\"指標\"にしてはいけない。\n",
    ">\n",
    "> その理由は、認識精度を指標にすると、パラメータの微分がほとんどの場所で0になってしまうため"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 具体例として、あるNNが現在100枚ある訓練データの中で32枚を正しく認識できているとする(認識精度32%)\n",
    "\n",
    "    * 認識精度を指標とすると、重みパラメータの値を少し変えた場合でも変化が現れない\n",
    "    \n",
    "    * 損失関数を指標とした場合、パラメータの値を変化させると、それに反応して損失関数も連続的に変化する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 認識精度はパラメータの微小な変化にはほとんど変化を示さず、もし変化があってもその値は不連続にいきなり変化する\n",
    "\n",
    "    * これは活性化関数のステップ関数にも当てはまる\n",
    "    \n",
    "    * シグモイド関数ならば、微分はどの場所でも0にはならないので、NNは正しい学習が行える"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 版   | 年/月/日   |\n",
    "| ---- | ---------- |\n",
    "| 初版 | 2019/05/05 |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
