{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "05 出力層の設計\n",
    "=============\n",
    "\n",
    "* ニューラルネットワークは、`分類問題`と`回帰問題`の両方に用いることができる\n",
    "\n",
    "    * ただし、それぞれの問題に対して出力層の活性化関数を変更する必要がある\n",
    "    \n",
    "    * 一般的に、`回帰問題`では`恒等関数`を、`分類問題`では`ソフトマックス関数`を用いる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 恒等関数とソフトマックス関数\n",
    "\n",
    "* `恒等関数`：入力をそのまま出力する\n",
    "\n",
    "    * 入ってきたものに対して何も手を加えずに出力する\n",
    "    \n",
    "    * そのため、出力層で恒等関数を用いるときは、入力信号をそのまま出力するだけになる\n",
    "    \n",
    "    * 恒等関数によって変換されるプロセスは、これまでの隠れ層での活性化関数と同じで、1本の矢印で描画する\n",
    "    \n",
    "![恒等関数](./images/恒等関数.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `ソフトマックス関数`：出力層が全部で$n$個あるとして、$k$番目の出力$y_k$を求める\n",
    "\n",
    "    * 分子は入力信号$a_k$の指数関数、分母は全ての入力信号の指数関数の和から構成される\n",
    "    \n",
    "\\begin{eqnarray}\n",
    "y_k = \\frac{\\exp (a_k)}{\\sum_{i=1}^{n} \\exp (a_i)}\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `ソフトマックス関数`を図で表すと、次の図のようになる\n",
    "\n",
    "    * 全ての入力信号から矢印による結びつきがある\n",
    "    \n",
    "    * 分母からわかるように、出力の各ニューロンが、全ての信号から影響を受けることになる\n",
    "    \n",
    "![ソフトマックス関数](./images/ソフトマックス関数.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 実際に、`ソフトマックス関数`を実装してみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.34985881 18.17414537 54.59815003]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([0.3, 2.9, 4.0])\n",
    "exp_a = np.exp(a)\n",
    "print(exp_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74.1221542101633\n"
     ]
    }
   ],
   "source": [
    "sum_exp_a = np.sum(exp_a)\n",
    "print(sum_exp_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01821127 0.24519181 0.73659691]\n"
     ]
    }
   ],
   "source": [
    "y = exp_a / sum_exp_a\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* この実装は、関数として次のように定義する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(a):\n",
    "    exp_a = np.exp(a)\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    y = exp_a / sum_exp_a\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ソフトマックスの実装上の注意\n",
    "\n",
    "* `softmax`関数は式を正しく表現できているが、コンピュータでの計算を行う際には欠陥がある\n",
    "\n",
    "    * 原因として、`オーバーフロー`が挙げられる\n",
    "    \n",
    "    * 指数関数において、$e^{1000}$などの値になった場合、無限大を示す`inf`が返ってくる\n",
    "    \n",
    "    * また、大きな値同士で割り算を行うと、数値が\"不安定\"な結果になってしまう"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `ソフトマックス関数`の実装の改善案は、以下の式から導出される\n",
    "\n",
    "\\begin{eqnarray}\n",
    "y_k = \\frac{\\exp (a_k)}{\\sum_{i=1}^{n} \\exp (a_i)} = \\frac{C\\exp (a_k)}{C\\sum_{i=1}^{n} \\exp (a_i)} \\\\\n",
    "= \\frac{\\exp (a_k + \\log C)}{\\sum_{i=1}^{n} \\exp (a_i + \\log C)} \\\\\n",
    "= \\frac{\\exp (a_k + C')}{\\sum_{i=1}^{n} \\exp (a_i + C')}\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ソフトマックスの指数関数の計算を行う際には、何らかの定数を足し算(もしくは引き算)しても結果が変わらない\n",
    "\n",
    "    * ここの$C'$にはどのような値も入れることができるが、オーバーフローの対策には入力信号の中でも最大の値を用いる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  \n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([nan, nan, nan])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1010, 1000, 990])\n",
    "\n",
    "# 正しく計算されない\n",
    "np.exp(a) / np.sum(np.exp(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1010\n"
     ]
    }
   ],
   "source": [
    "c = np.max(a)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0, -10, -20])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a - c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.99954600e-01, 4.53978686e-05, 2.06106005e-09])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(a - c) / np.sum(np.exp(a - c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* この例より、普通に計算していたら`nan`が出力されるが、入力信号の最大値(`c`)を引くことで、正しく計算されることがわかる\n",
    "\n",
    "    * 以上のことを含めて`ソフトマックス関数`を実装すると、次のようになる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(a):\n",
    "    c = np.max(a)\n",
    "    # オーバーフロー対策\n",
    "    exp_a = np.exp(a - c)\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    y = exp_a / sum_exp_a\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ソフトマックス関数の特徴\n",
    "\n",
    "* `softmax()`関数を使うことで、ニューラルネットワークの出力は次のように計算することができる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01821127 0.24519181 0.73659691]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([0.3, 2.9, 4.0])\n",
    "y = softmax(a)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ここで示したように、`ソフトマックス関数`の出力は`0`から`1.0`の間の実数になる\n",
    "\n",
    "    * また、`ソフトマックス関数`の出力の総和は`1`となる\n",
    "    \n",
    "    * この性質により、`ソフトマックス関数`の出力を「確率」として解釈することができる\n",
    "    \n",
    "    * 例)`y[0]`の確率は0.018、`y[1]`の確率は0.245、`y[2]`の確率は0.737\n",
    "    \n",
    "    * この結果により、「2番目の要素が最も確率が高いため、答えは2番目のクラス」と言うことができる\n",
    "    \n",
    "    * また、「74%の確率で2番目のクラス、25%の確率で1番目のクラス、1%の確率で0番目のクラス」と言う表現もできる\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ここで注意点として、`ソフトマックス関数`を適用しても各要素の大小関係は変わらないこと\n",
    "\n",
    "    * これは、指数関数が単調増加する関数であることに起因する\n",
    "    \n",
    "    * 実際、上の例では`a`の要素の大小関係と`y`の要素の大小関係は変わっていない"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ニューラルネットワークのクラス分類では、一般的に、出力の一番大きいニューロンに相当するクラスだけを認識結果とする\n",
    "\n",
    "    * そして、`ソフトマックス関数`を適用しても、出力の一番大きいニューロンの位置は変わらない\n",
    "    \n",
    "    * そのため、ニューラルネットワークが分類を行う際には、出力層のソフトマックス関数を省略することができる\n",
    "    \n",
    "* 実際の問題では、指数関数の計算は、それなりにコンピュータの計算が必要になるので、出力層のソフトマックス関数は省略するのが一般的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 出力層のニューロンの数\n",
    "\n",
    "* 出力層のニューロンの数は、解くべき問題に応じて、適宜決める必要がある\n",
    "\n",
    "* クラス分類を行う問題では、出力層のニューロンの数は分類したいクラスの数に設定するのが一般的\n",
    "\n",
    "    * 例)ある入力画像に対して、その画像が数字の`0`から`9`のどれかを予測(10クラス分類問題)では、以下の図のように出力層のニューロンは10個に設定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![出力層のニューロンは各数字に対応する](./images/出力層のニューロンは各数字に対応する.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* この例では、出力層のニューロンは上から順に数字の`0`、`1`、...、`9`に対応する\n",
    "\n",
    "    * また、この図では$y_2$が一番濃く描画されており、$y_2$のニューロンが一番高い値を出力している\n",
    "    \n",
    "    * これは、$y_2$に該当するクラス、つまり「2」であることを、このニューラルネットワークが予測している"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 版   | 年/月/日   |\n",
    "| ---- | ---------- |\n",
    "| 初版 | 2019/05/01 |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
