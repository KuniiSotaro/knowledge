
01 ワンホットエンコーディング(ダミー変数)
==================================

* カテゴリ変数を表現する方法として、圧倒的によく用いられている手法が、`ワンホットエンコーディング`

    * `ダミー変数`とも呼ばれることがある

    * これは、カテゴリ変数を1つ以上の0と1の値を持つ新しい特徴量で置き換えるもの

    * 値0と1を使えば、線形2クラス分類の式が意味を持つので、scikit-learnのほとんどのモデルを利用できる

    * カテゴリごとに新しい特徴量を導入すれば、いくらでもカテゴリ変数を表現することができる

* 例えば、`workclass`(雇用形態)特徴量は、以下の4つの値をとるとする

    * `Government Employee`(公務員)

    * `Private Employee`(民間企業従業員)

    * `Self Employed`(自営)

    * `Self Employed Incorporated`(自営法人)

* これら4つの値をエンコードするには4つの新しい特徴量`Government Employee`、`Private Employee`、`Self Employed`、`Self Employed Incorporated`を作る

    * こちらの特徴量は、ある人物の`workclass`が対応する値だった時に1になり、それ以外の場合は0となる

    * つまり、各データポイントに対して、常に4つの新しい特徴量のうち1つだけが1になる


* この原理を以下の表に示す

    * 1つの特徴量が4つの新しい特徴量にエンコードされている

    * このデータを機械学習アルゴリズムで使うには、元の`workclass`特徴量を削除し、`0-1`特徴量だけを用いる


```python
import pandas as pd
import os
import mglearn
# The file has no headers naming the columns, so we pass header=None
# and provide the column names explicitly in "names"
adult_path = os.path.join(mglearn.datasets.DATA_PATH, "adult.data")
data = pd.read_csv(
    adult_path, header=None, index_col=False,
    names=['age', 'workclass', 'fnlwgt', 'education',  'education-num',
           'marital-status', 'occupation', 'relationship', 'race', 'gender',
           'capital-gain', 'capital-loss', 'hours-per-week', 'native-country',
           'income'])
# For illustration purposes, we only select some of the columns
data = data[['age', 'workclass', 'education', 'gender', 'hours-per-week',
             'occupation', 'income']]
data_dummies = pd.get_dummies(data)
display(data_dummies.head())
```


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>age</th>
      <th>hours-per-week</th>
      <th>workclass_ ?</th>
      <th>workclass_ Federal-gov</th>
      <th>workclass_ Local-gov</th>
      <th>workclass_ Never-worked</th>
      <th>workclass_ Private</th>
      <th>workclass_ Self-emp-inc</th>
      <th>workclass_ Self-emp-not-inc</th>
      <th>workclass_ State-gov</th>
      <th>...</th>
      <th>occupation_ Machine-op-inspct</th>
      <th>occupation_ Other-service</th>
      <th>occupation_ Priv-house-serv</th>
      <th>occupation_ Prof-specialty</th>
      <th>occupation_ Protective-serv</th>
      <th>occupation_ Sales</th>
      <th>occupation_ Tech-support</th>
      <th>occupation_ Transport-moving</th>
      <th>income_ &lt;=50K</th>
      <th>income_ &gt;50K</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>39</td>
      <td>40</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>50</td>
      <td>13</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>38</td>
      <td>40</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>53</td>
      <td>40</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>28</td>
      <td>40</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 46 columns</p>
</div>


> `ワンホットエンコーディング`は、統計で用いられる`ダミーエンコーディング`によく似ているが、全く同じではない
>
> いずれも個々のカテゴリを複数の2値特徴量で表現する
>
> 統計では、 $`k`$ 個の異なる値をとるカテゴリ特徴量を $`k-1`$ 個の特徴量で表現する(最後の1つでは全てが0として表現される)
>
> これは、解析を容易にするため(データ行列のランク不足を避けるため)だ

* カテゴリ変数を`ワンホットエンコーディング`に変換するには2つの方法がある

    * `pandas`

    * `scikit-learn`

* ここでは、`pandas`を使う方が少し簡単なので、こちらを使う

* まず、`pandas`を使ってCSVファイルからデータを読み込む


```python
import os
# このファイルにはコラム名を含んだヘッダがないので、header=Noneを指定し、コラム名を"names"で明示的に指定
adult_path = os.path.join(mglearn.datasets.DATA_PATH, "adult.data")
data = pd.read_csv(
    adult_path, header=None, index_col=False,
    names=['age', 'workclass', 'fnlwgt', 'education',  'education-num',
           'marital-status', 'occupation', 'relationship', 'race', 'gender',
           'capital-gain', 'capital-loss', 'hours-per-week', 'native-country',
           'income'])
# 解説のために、いくつかのカラムだけを選択
data = data[['age', 'workclass', 'education', 'gender', 'hours-per-week',
             'occupation', 'income']]
# IPython.displayを使うと、Jupyter notebookで綺麗な出力が得られる
display(data.head())
```


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>age</th>
      <th>workclass</th>
      <th>education</th>
      <th>gender</th>
      <th>hours-per-week</th>
      <th>occupation</th>
      <th>income</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>39</td>
      <td>State-gov</td>
      <td>Bachelors</td>
      <td>Male</td>
      <td>40</td>
      <td>Adm-clerical</td>
      <td>&lt;=50K</td>
    </tr>
    <tr>
      <th>1</th>
      <td>50</td>
      <td>Self-emp-not-inc</td>
      <td>Bachelors</td>
      <td>Male</td>
      <td>13</td>
      <td>Exec-managerial</td>
      <td>&lt;=50K</td>
    </tr>
    <tr>
      <th>2</th>
      <td>38</td>
      <td>Private</td>
      <td>HS-grad</td>
      <td>Male</td>
      <td>40</td>
      <td>Handlers-cleaners</td>
      <td>&lt;=50K</td>
    </tr>
    <tr>
      <th>3</th>
      <td>53</td>
      <td>Private</td>
      <td>11th</td>
      <td>Male</td>
      <td>40</td>
      <td>Handlers-cleaners</td>
      <td>&lt;=50K</td>
    </tr>
    <tr>
      <th>4</th>
      <td>28</td>
      <td>Private</td>
      <td>Bachelors</td>
      <td>Female</td>
      <td>40</td>
      <td>Prof-specialty</td>
      <td>&lt;=50K</td>
    </tr>
  </tbody>
</table>
</div>


## 1. 文字列で表されているカテゴリデータのチェック

* データセットを読み込んだら、各列に意味のあるカテゴリデータが含まれているかチェックする

* (Webサイトでのユーザ入力などの)人間が入力したデータを処理する場合、固定したカテゴリがない場合もあり、スペルの揺れ大文字小文字の違いを前処理する必要がある

    * 例)性別を「male」と書く人も、「man」と書く人もいるが、これらは同じカテゴリとする

    * これには、pandasの`DataFrame`中の行を表す`Series`クラスの関数`value_counts`を用いて、各行に含まれるユニークな値とその頻度を表示する


```python
print(data.gender.value_counts())
```

     Male      21790
     Female    10771
    Name: gender, dtype: int64


* `gender`(性別)には`Male`と`Female`しか含まれていないことがわかる

    * このデータは既に整理されているということなので、`ワンホットエンコーディング`できる

    * 実際のアプリケーションでは全てのカラムの値を同時にテストする必要があるが、長くなりすぎるので省略する

* pandasでは、`get_dummies`関数を使って簡単にデータを`ワンホットエンコーディング`することができる

    * `get_dummies`関数は、自動的に(文字列などの)object型やカテゴリ型(pandas特有の概念)の行を全て変換する


```python
print("Original features:\n", list(data.columns), "\n")
data_dummies = pd.get_dummies(data)
print("Features after get_dummies:\n", list(data_dummies.columns))
```

    Original features:
     ['age', 'workclass', 'education', 'gender', 'hours-per-week', 'occupation', 'income']

    Features after get_dummies:
     ['age', 'hours-per-week', 'workclass_ ?', 'workclass_ Federal-gov', 'workclass_ Local-gov', 'workclass_ Never-worked', 'workclass_ Private', 'workclass_ Self-emp-inc', 'workclass_ Self-emp-not-inc', 'workclass_ State-gov', 'workclass_ Without-pay', 'education_ 10th', 'education_ 11th', 'education_ 12th', 'education_ 1st-4th', 'education_ 5th-6th', 'education_ 7th-8th', 'education_ 9th', 'education_ Assoc-acdm', 'education_ Assoc-voc', 'education_ Bachelors', 'education_ Doctorate', 'education_ HS-grad', 'education_ Masters', 'education_ Preschool', 'education_ Prof-school', 'education_ Some-college', 'gender_ Female', 'gender_ Male', 'occupation_ ?', 'occupation_ Adm-clerical', 'occupation_ Armed-Forces', 'occupation_ Craft-repair', 'occupation_ Exec-managerial', 'occupation_ Farming-fishing', 'occupation_ Handlers-cleaners', 'occupation_ Machine-op-inspct', 'occupation_ Other-service', 'occupation_ Priv-house-serv', 'occupation_ Prof-specialty', 'occupation_ Protective-serv', 'occupation_ Sales', 'occupation_ Tech-support', 'occupation_ Transport-moving', 'income_ <=50K', 'income_ >50K']


* 連続特徴量の`age`と`hours-per-week`は変更されておらず、カテゴリ特徴量は取りうる値ごとに1つの特徴量を持つように拡張されていることがわかる


```python
display(data_dummies.head())
```


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>age</th>
      <th>hours-per-week</th>
      <th>workclass_ ?</th>
      <th>workclass_ Federal-gov</th>
      <th>workclass_ Local-gov</th>
      <th>workclass_ Never-worked</th>
      <th>workclass_ Private</th>
      <th>workclass_ Self-emp-inc</th>
      <th>workclass_ Self-emp-not-inc</th>
      <th>workclass_ State-gov</th>
      <th>...</th>
      <th>occupation_ Machine-op-inspct</th>
      <th>occupation_ Other-service</th>
      <th>occupation_ Priv-house-serv</th>
      <th>occupation_ Prof-specialty</th>
      <th>occupation_ Protective-serv</th>
      <th>occupation_ Sales</th>
      <th>occupation_ Tech-support</th>
      <th>occupation_ Transport-moving</th>
      <th>income_ &lt;=50K</th>
      <th>income_ &gt;50K</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>39</td>
      <td>40</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>50</td>
      <td>13</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>38</td>
      <td>40</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>53</td>
      <td>40</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>28</td>
      <td>40</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 46 columns</p>
</div>


* ここで`values`属性を用いれば、`data_dummies DataFrame`をNumPy配列に変換し、それを使って機械学習モデルを学習させることができる

    * モデル学習させる前に、(2つの`income`列にエンコードされている)ターゲット変数を分離する必要がある

    * 教師あり機械学習モデルを構築する際に、間違って出力変数や出力変数から導出されるような特性を特徴量に含めてしまうのは、よくあるミスなので注意する

> 注意：pandasの列のインデックスの範囲指定は最後のインデックスを含む
>
> つまり、'age':'occupation_ Transport-moving'には、`occupation_ Transport-moving`が含まれる
>
> これは、NumPy配列のスライスとは異なる
>
> スライスでは、範囲指定の最後は含まれない
>
> 例)`np.arrange(11)[0:10]`にはインデックス10は含まれない

* この場合、特徴量を含む列だけ(`age`から`occupation_ Transport-moving`までを抜き出す

    * このレンジには全ての特徴量が含まれるが、ターゲットは含まれない


```python
features = data_dummies.loc[:, 'age':'occupation_ Transport-moving']
# Extract NumPy arrays
X = features.values
y = data_dummies['income_ >50K'].values
print("X.shape: {}  y.shape: {}".format(X.shape, y.shape))
```

    X.shape: (32561, 44)  y.shape: (32561,)


* これで、データはscikit-learnが扱える形になったので、いつものように進めることができる


```python
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
logreg = LogisticRegression()
logreg.fit(X_train, y_train)
print("Training score: {:.2f}".format(logreg.score(X_train, y_train)))
print("Test score: {:.2f}".format(logreg.score(X_test, y_test)))
```

    Training score: 0.81
    Test score: 0.81


    /Users/MacUser/anaconda2/envs/tf140/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.
      FutureWarning)



| 版     | 年/月/日   |
| ------ | ---------- |
| 初版   | 2019/03/19 |
| 第二版 | 2019/05/05 |
