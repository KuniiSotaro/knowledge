{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "05 tf-idfを用いたデータのスケール変換\n",
    "==============================\n",
    "\n",
    "* `tf-idf`：特徴量がどの程度情報を持っていそうかに応じて、特徴量のスケールを変換する\n",
    "\n",
    "    * この手法は、特定の文書にだけ頻繁に現れる単語に大きな重みを与え、コーパス中の多数の文書に現れる単語にはあまり重みを与えない\n",
    "    \n",
    "    * 特定の文書にだけ頻出し、他の文書にはあまり現れない単語は、その文書をよく示しているのではないか、という発想\n",
    "    \n",
    "* scikit-learnは`tf-idf`を2つのクラスで実装している\n",
    "\n",
    "    * `TfidfTransformer`は`CountVectorizer`の生成する疎行列を入力とする\n",
    "    \n",
    "    * `TfidfVectorizer`はテキストデータを入力とし、BoW特徴量抽出とtf-idf変換を行う\n",
    "    \n",
    "    * tf-idfスケール変換には、様々な方法がある\n",
    "    \n",
    "    * `TfidTransformer`でも`TfidVectorizer`でも、文書$d$における、単語$w$のtf-idfスコアは下のように与えられる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $N$：訓練セットの文書の数\n",
    "\n",
    "* $N_w$：訓練セット中の$w$が現れる文書の数\n",
    "\n",
    "* $tf$：対象の文書$d$(変換を行う文書)中に$w$が現れる回数\n",
    "\n",
    "\\begin{eqnarray}\n",
    "tfidf(w, d) = tf(log(\\frac{N + 1}{N_w + 1}) + 1)\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 2つのクラスはいずれも、tf-idf表現を計算したあとでL2正則化を行う\n",
    "\n",
    "    * つまり、それぞれの文書の表現の長さが、ユークリッド長で1になるようにスケール変換を行う\n",
    "    \n",
    "    * このように変換すると、文書の長さ(単語数)がベクトル表現に影響を与えなくなる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* tf-idfは訓練データの統計的性質を利用するので、パイプラインを用いてグリッドサーチの結果が有効になるようにする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/MacUser/anaconda2/envs/tf140/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/MacUser/anaconda2/envs/tf140/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/MacUser/anaconda2/envs/tf140/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'C': [0.001, 0.01, 0.1, 1, 10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "\n",
    "reviews_train = load_files(\"/Users/MacUser/data/aclImdb/train/\")\n",
    "text_train, y_train = reviews_train.data, reviews_train.target\n",
    "text_train = [doc.replace(b\"<br />\", b\" \") for doc in text_train]\n",
    "reviews_test = load_files(\"/Users/MacUser/data/aclImdb/test/\")\n",
    "text_test, y_test = reviews_test.data, reviews_test.target\n",
    "text_test = [doc.replace(b\"<br />\", b\" \") for doc in text_test]\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer().fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "scores = cross_val_score(LogisticRegression(), X_train, y_train, cv=5)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10]}\n",
    "grid = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "X_test = vect.transform(text_test)\n",
    "\n",
    "vect = CountVectorizer(min_df=5).fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "grid = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/MacUser/anaconda2/envs/tf140/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.89\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "pipe = make_pipeline(TfidfVectorizer(min_df=5, norm=None),\n",
    "                     LogisticRegression())\n",
    "param_grid = {'logisticregression__C': [0.001, 0.01, 0.1, 1, 10]}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5)\n",
    "grid.fit(text_train, y_train)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* この場合には、tf-idf変換を行っても性能は向上しなかった\n",
    "\n",
    "    * tf-idfを用いると性能が少し良くなる\n",
    "    \n",
    "    * また、tf-idfがどの単語が最も重要だと判断したかをみることもできる\n",
    "    \n",
    "    * tf-idfによるスケール変換は、文書を区別するためのものだが、純粋に教師なしの手法であることに留意する\n",
    "    \n",
    "    * このため、ここでの「重要」さは、本来の興味の対象である「肯定的なレビュー」「否定的なレビュー」のラベルには必ずしも関係ない\n",
    "    \n",
    "* まず、パイプラインから`TfidfVectorizer`を取り出す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features with lowest tfidf:\n",
      "['poignant' 'disagree' 'instantly' 'importantly' 'lacked' 'occurred'\n",
      " 'currently' 'altogether' 'nearby' 'undoubtedly' 'directs' 'fond'\n",
      " 'stinker' 'avoided' 'emphasis' 'commented' 'disappoint' 'realizing'\n",
      " 'downhill' 'inane']\n",
      "Features with highest tfidf: \n",
      "['coop' 'homer' 'dillinger' 'hackenstein' 'gadget' 'taker' 'macarthur'\n",
      " 'vargas' 'jesse' 'basket' 'dominick' 'the' 'victor' 'bridget' 'victoria'\n",
      " 'khouri' 'zizek' 'rob' 'timon' 'titanic']\n"
     ]
    }
   ],
   "source": [
    "vectorizer = grid.best_estimator_.named_steps[\"tfidfvectorizer\"]\n",
    "# 訓練データセットを変換\n",
    "X_train = vectorizer.transform(text_train)\n",
    "# それぞれの特徴量のデータセット中での最大値を見つける\n",
    "max_value = X_train.max(axis=0).toarray().ravel()\n",
    "sorted_by_tfidf = max_value.argsort()\n",
    "# 特徴量名を取得\n",
    "feature_names = np.array(vectorizer.get_feature_names())\n",
    "\n",
    "print(\"Features with lowest tfidf:\\n{}\".format(\n",
    "      feature_names[sorted_by_tfidf[:20]]))\n",
    "\n",
    "print(\"Features with highest tfidf: \\n{}\".format(\n",
    "      feature_names[sorted_by_tfidf[-20:]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* tf-idfが低い特徴量は、多くの文書に共通して出現するか、あまり出現しないか、もしくは非常に長い文書にしか出現しないか、である\n",
    "\n",
    "    * tf-idfの高い特徴量の多くは、特定の映画を指している\n",
    "    \n",
    "    * これらの単語は、センチメント分析タスクにはあまり役に立たない"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 文書頻度の逆数(idf)が小さい単語を見つけることもできる\n",
    "\n",
    "    * このような単語は高い頻度で現れるため、重要でないと考えられる単語\n",
    "    \n",
    "    * 訓練セットに対する文書頻度の逆数は`idf_`属性に格納されている"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features with lowest idf:\n",
      "['the' 'and' 'of' 'to' 'this' 'is' 'it' 'in' 'that' 'but' 'for' 'with'\n",
      " 'was' 'as' 'on' 'movie' 'not' 'have' 'one' 'be' 'film' 'are' 'you' 'all'\n",
      " 'at' 'an' 'by' 'so' 'from' 'like' 'who' 'they' 'there' 'if' 'his' 'out'\n",
      " 'just' 'about' 'he' 'or' 'has' 'what' 'some' 'good' 'can' 'more' 'when'\n",
      " 'time' 'up' 'very' 'even' 'only' 'no' 'would' 'my' 'see' 'really' 'story'\n",
      " 'which' 'well' 'had' 'me' 'than' 'much' 'their' 'get' 'were' 'other'\n",
      " 'been' 'do' 'most' 'don' 'her' 'also' 'into' 'first' 'made' 'how' 'great'\n",
      " 'because' 'will' 'people' 'make' 'way' 'could' 'we' 'bad' 'after' 'any'\n",
      " 'too' 'then' 'them' 'she' 'watch' 'think' 'acting' 'movies' 'seen' 'its'\n",
      " 'him']\n"
     ]
    }
   ],
   "source": [
    "sorted_by_idf = np.argsort(vectorizer.idf_)\n",
    "print(\"Features with lowest idf:\\n{}\".format(\n",
    "       feature_names[sorted_by_idf[:100]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 予想された通り、これらのほとんどの英語のストップワードである\n",
    "\n",
    "* いくつかは明らかに、映画レビューに固有の単語である\n",
    "\n",
    "* \"good\"、\"great\"などの単語は、センチメント分析タスクには非常に重要だと思われているのにも関わらず、多くの文書に頻出する\n",
    "    \n",
    "    * tf-idfの尺度では、「最も関連性が低い」と判断されてしまっている"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 版   | 年/月/日   |\n",
    "| ---- | ---------- |\n",
    "| 初版 | 2019/04/06 |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
