{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "08 より進んだトークン分割、語幹処理、見出し語化\n",
    "=======================================\n",
    "\n",
    "* `CountVectorizer`や`TfidVectorizer`の行う特徴量抽出は比較的単純で、まだまだ向上の余地がある\n",
    "\n",
    "    * より洗練されたテキスト処理アプリケーションでよく改良されているのは、`BoW`モデル構築の最初の過程(`トークン分割`)\n",
    "    \n",
    "    * このステップでは、特徴量抽出に用いられる単語の構成を定める"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ボキャブラリの中には、複数形、動詞の変化などが含まれる\n",
    "\n",
    "    * この問題は、個々の単語を`語幹`を使って表現すれば解決する\n",
    "    \n",
    "    * これは同じ語幹を持つ全ての単語を特定する(`融合`)する必要がある\n",
    "    \n",
    "    * これを、単語の末尾につく特定の形を取り除くといったようなルールベースのヒューリスティクスで行う場合、`語幹処理`と呼ぶ\n",
    "    \n",
    "    * 知られている単語に対して辞書を用いて、単語の文章での役割を考慮して行う場合には`見出し語化`とよび、単語の標準的な形を`見出し語`と呼ぶ\n",
    "    \n",
    "    * 語幹処理、見出し語化はいずれも単語の正規形を取り出そうと試みる`正規化`と呼ぶ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 正規化を理解するために、`語幹処理`と`見出し語化`を比較してみる\n",
    "\n",
    "    * `語幹処理`には広く用いられているヒューリスティクスの集合であるPorter stemmer(nltkパッケージ)を用いる\n",
    "    \n",
    "    * `見出し語化`にはspacyパッケージのものを用いる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "\n",
    "# spacyの英語モデルをロード\n",
    "en_nlp = spacy.load('en')\n",
    "# nltkのPorter stemmerのインスタンスを作成\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "# spacyによる見出し語化とnltkによる語幹処理を比較する関数を定義\n",
    "def compare_normalization(doc):\n",
    "    # spacyで文書をトークン分割\n",
    "    doc_spacy = en_nlp(doc)\n",
    "    # spacyで見つけた見出し語を表示\n",
    "    print(\"Lemmatization:\")\n",
    "    print([token.lemma_ for token in doc_spacy])\n",
    "    # Porter stemmerで見つけたトークンを表示\n",
    "    print(\"Stemming:\")\n",
    "    print([stemmer.stem(token.norm_.lower()) for token in doc_spacy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 見出し語化と語幹処理の違いが分かるように作り込んだ文章を使って比較してみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization:\n",
      "['-PRON-', 'meeting', 'today', 'be', 'bad', 'than', 'yesterday', ',', '-PRON-', 'be', 'scared', 'of', 'meet', 'the', 'client', 'tomorrow', '.']\n",
      "Stemming:\n",
      "['our', 'meet', 'today', 'wa', 'wors', 'than', 'yesterday', ',', 'i', 'am', 'scare', 'of', 'meet', 'the', 'client', 'tomorrow', '.']\n"
     ]
    }
   ],
   "source": [
    "compare_normalization(u\"Our meeting today was worse than yesterday, \"\n",
    "                       \"I'm scared of meeting the clients tomorrow.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 語幹処理は、単語を切り縮めて語幹にすることしかないので、\"was\"は\"wa\"となってしまう\n",
    "\n",
    "* 一方、見出し語化では正しい動詞の基本形の\"be\"となっている\n",
    "\n",
    "    * 同様に、見出し語化では\"worse\"を正しく\"bad\"と正規化できているが、語幹処理では\"wors\"となっている\n",
    "    \n",
    "* もう一つ大きな違いがある\n",
    "\n",
    "    * 語幹処理では2回出現する\"meeting\"をどちらも\"meet\"にしてしまっている\n",
    "    \n",
    "    * 見出し語化では最初の\"meeting\"は名詞と判断されてそのまま残る\n",
    "    \n",
    "    * 2度目の\"meeting\"は動詞として\"meet\"とされている\n",
    "    \n",
    "* 一般に見出し語化は語幹処理よりも複雑な処理で、機械学習におけるトークンの正規化に用いるとより良い結果が得られる\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* scikit-learnはどちらの正規化手法も実装されていないが、`CountVectorizer`の`tokenizer`パラメータで、文書をトークン分割器を指定することができる\n",
    "\n",
    "    * spacyの見出し語化機能を使って、文字列から見出し語の列を作る関数を作って指定すれば良い"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# 技術的詳細：CountVectorizerが用いている正規表現ベースの\n",
    "# トークン分割器を用いて、見出し語化だけにspacyを用いるのが望ましい\n",
    "# このため、en_nlp.tokenizer(spacyのトークン分割器)を、正規表現ベースのトークン分割器に置き換えている\n",
    "import re\n",
    "# CountVectorizerで用いられているトークン分割用の正規表現\n",
    "regexp = re.compile('(?u)\\\\b\\\\w\\\\w+\\\\b')\n",
    "# spacyの言語モデルを読み込み、トークン分割器を取り出す\n",
    "en_nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "old_tokenizer = en_nlp.tokenizer\n",
    "# トークン分割器を先ほどの正規表現で置き換える\n",
    "en_nlp.tokenizer = lambda string: old_tokenizer.tokens_from_list(\n",
    "    regexp.findall(string))\n",
    "\n",
    "# spacyの文書処理パイプラインを用いてカスタムトークン分割器を作る(正規表現を用いたトークン分割器を組み込んである)\n",
    "def custom_tokenizer(document):\n",
    "    doc_spacy = en_nlp(document)\n",
    "    return [token.lemma_ for token in doc_spacy]\n",
    "\n",
    "# COuntVectorizerをカスタムトークン分割器を使って定義する\n",
    "lemma_vect = CountVectorizer(tokenizer=custom_tokenizer, min_df=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* データを変換してボキャブラリのサイズを見てみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/MacUser/anaconda2/envs/tf140/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/MacUser/anaconda2/envs/tf140/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/MacUser/anaconda2/envs/tf140/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/MacUser/anaconda2/envs/tf140/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "\n",
    "reviews_train = load_files(\"/Users/MacUser/data/aclImdb/train/\")\n",
    "text_train, y_train = reviews_train.data, reviews_train.target\n",
    "text_train = [doc.replace(b\"<br />\", b\" \") for doc in text_train]\n",
    "reviews_test = load_files(\"/Users/MacUser/data/aclImdb/test/\")\n",
    "text_test, y_test = reviews_test.data, reviews_test.target\n",
    "text_test = [doc.replace(b\"<br />\", b\" \") for doc in text_test]\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer().fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "scores = cross_val_score(LogisticRegression(), X_train, y_train, cv=5)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10]}\n",
    "grid = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "X_test = vect.transform(text_test)\n",
    "\n",
    "vect = CountVectorizer(min_df=5).fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "grid = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "pipe = make_pipeline(TfidfVectorizer(min_df=5, norm=None),\n",
    "                     LogisticRegression())\n",
    "param_grid = {'logisticregression__C': [0.001, 0.01, 0.1, 1, 10]}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5)\n",
    "grid.fit(text_train, y_train)\n",
    "\n",
    "vectorizer = grid.best_estimator_.named_steps[\"tfidfvectorizer\"]\n",
    "# 訓練データセットを変換\n",
    "X_train = vectorizer.transform(text_train)\n",
    "# それぞれの特徴量のデータセット中での最大値を見つける\n",
    "max_value = X_train.max(axis=0).toarray().ravel()\n",
    "sorted_by_tfidf = max_value.argsort()\n",
    "# 特徴量名を取得\n",
    "feature_names = np.array(vectorizer.get_feature_names())\n",
    "sorted_by_idf = np.argsort(vectorizer.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_lemma.shape: (25000, 21575)\n",
      "X_train.shape: (25000, 27271)\n"
     ]
    }
   ],
   "source": [
    "# 見出し語化を行うCountVectorizerでtext_trainを変換\n",
    "X_train_lemma = lemma_vect.fit_transform(text_train)\n",
    "print(\"X_train_lemma.shape: {}\".format(X_train_lemma.shape))\n",
    "\n",
    "# 比較のために標準のCountVectorizerでも変換\n",
    "vect = CountVectorizer(min_df=5).fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "print(\"X_train.shape: {}\".format(X_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* この結果から分かるように、見出し語化によって特徴量が27,271(標準のCountVectrizer処理の結果)から、21,596へと減少している\n",
    "\n",
    "* 見出し語化は、特定の特徴量を融合するもので、ある種の正則化と見なすことができる\n",
    "\n",
    "    * したがって、見出し語化によって最も性能が向上するのはデータセットが小さい場合であることが予測できる\n",
    "    \n",
    "* 見出し語化の有効性を確認するために、`StratifiedShuffleSplit`を用い、データの1%だけを訓練データとし、残りをテストデータとして交差検証を行う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/MacUser/anaconda2/envs/tf140/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score (standard CountVectorizer): 0.721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/MacUser/anaconda2/envs/tf140/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score (lemmatization): 0.735\n"
     ]
    }
   ],
   "source": [
    "# データの1%だけを訓練セットとして用いてグリッドサーチを行う\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10]}\n",
    "cv = StratifiedShuffleSplit(n_splits=5, test_size=0.99,\n",
    "                            train_size=0.01, random_state=0)\n",
    "grid = GridSearchCV(LogisticRegression(), param_grid, cv=cv)\n",
    "# 標準のCountVectorizerを用いてグリッドサーチを実行\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best cross-validation score \"\n",
    "      \"(standard CountVectorizer): {:.3f}\".format(grid.best_score_))\n",
    "# 見出し語化付きで、グリッドサーチを実行\n",
    "grid.fit(X_train_lemma, y_train)\n",
    "print(\"Best cross-validation score \"\n",
    "      \"(lemmatization): {:.3f}\".format(grid.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* この場合、見出し語化を行っても性能は若干向上する程度\n",
    "\n",
    "    * 他の特徴量抽出技術と同様に、結果はデータセットによって異なる\n",
    "    \n",
    "    * 見出し語化や語幹処理によって、より良いモデルを作る役に立つ場合もある\n",
    "    \n",
    "    * 特定のタスクに対して性能の最後のひとしずくまで絞り出したい際にはこの技術を使ってみることをオススメする"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 版   | 年/月/日   |\n",
    "| ---- | ---------- |\n",
    "| 初版 | 2019/04/06 |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
