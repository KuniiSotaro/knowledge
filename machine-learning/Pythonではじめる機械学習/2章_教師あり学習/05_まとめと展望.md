
05 まとめと展望
===========

* 本章では、まずモデルの複雑さについて議論し、次に**汎化**について議論した

  * `汎化`とは、新しい見たことのないデータに対してうまく機能するようにモデルを学習すること

  * この`汎化`という考え方から、訓練データに現れている変異をモデルが捉えきれていない状態を表す`適合不足`

  * 逆に訓練データに適合しすぎてしまい、新しいデータに汎化できない状態を表す`過剰適合`という概念に至った

* 次に、クラス分類と回帰を行う様々な機械学習モデルについて、それらの利点と欠点、モデルの複雑さを制御する方法を述べた

  * 多くのアルゴリズムでは、性能を得るためには正しいパラメータを選択することが重要である

  * また、アルゴリズムによって、入力データの表現、特に特徴量のスケールに敏感であることも示した

  * したがって、モデルがおいている仮定やパラメータの意味を理解せずに、適当なデータセットに適当なアルゴリズムを適用するだけでは正確なモデルを得ることができない



* 本書には様々なアルゴリズムに関する情報が大量に含まれいている

* それぞれのモデルは以下にまとめられる

|             手法             |内容|
|:------------------------------|:------|
|最近傍法                          |小さいデータに関しては良いベースラインとなる。説明が容易。|
|線形モデル                       |最初に試してみるべきアルゴリズム。非常に大きいデータセットに適する。非常に高次元のデータに適する。|
|ナイーブベイズ                  |クラス分類にしか使えない。線形モデルよりもさらに高速。非常に大きいデータセット、高次元データに適する。線形モデルよりも精度が劣ることが多い。|
|決定木                              |非常に高速。データのスケールを考慮する必要がない。可視化が可能で説明しやすい。|
|ランダムフォレスト             |ほとんどの場合単一の決定木よりも高速で、頑健で、強力。データのスケールを考慮する必要がない。高次元の疎なデータには適しない。|
|勾配ブースティング決定木|多くの場合ランダムフォレストよりも少し精度が高い。ランダムフォレストよりも訓練に時間がかかるが、予測はこちらの方が速く、メモリ使用量も小さい。ランダムフォレストよりもパラメータに敏感|
|サポートベクタマシン         |同じような特徴量からなる中規模なデータセットに対しては強力。データのスケールを調整する必要がある。パラメータに敏感。|
|ニューラルネットワーク       |非常に複雑なモデルを構築できる。特に大きなデータセットに有効。データのスケールを調整する必要がある。パラメータに敏感。大きいモデルは訓練に時間がかかる。|

* 新しいデータセットを扱う場合は、線形モデルやナイーブベイズや最近傍法などの、簡単なモデルでどのくらい精度が得られるかを試すべきである

  * データをより深く理解できたら、ランダムフォレストや勾配ブースティング、SVM、ニューラルネットワークなどの、より複雑なモデルに移行することを考える



| 版     | 年/月/日   |
| ------ | ---------- |
| 初版   | 2019/03/11 |
| 第二版 | 2019/05/05 |
