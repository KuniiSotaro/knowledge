04 メリット・デメリット・注意事項
============================

* `モデルスタッキング`：あるモデルの出力が、別のモデルの出力となる

  * `k-means`を使ってデータの空間情報を特徴量に変換すること

  * 例)決定木タイプの出力

* 線形分類器の構築と運用は簡単だが、簡単な表現にしか対応することができない

* 非線形分類器の構築と運用は大変だが、複雑な表現に対応する事ができる

* `スタッキング`は、線形分類器と非線形分類器の利点をうまく享受することができる

  * 非線形性を特徴量の中に押し込んで、最終段階ではシンプルなモデルを使用する

  * 特徴量を生成する部分は、オフラインで学習できるので、計算コストや使用メモリが大きけれど、有用な特徴量が生成できるようにする

  * 最終段階がシンプルなモデルだと、オンラインデータの分布の変化に素早く対応できる

  > ターゲティング広告のような分布の変化が早い現象に対してよく使用される

* `k-means`+`LR`(ロジスティック回帰)は、学習が速くて結果の保存が楽

  * 以下の表は、使用頻度の高い機械学習のモデルの複雑さを「計算量」と「結果を保存するために使用する容量」の観点から一覧にしたもの

  * $`n`$ ：データ点の数

  * $`d`$ ：元の特徴量の数

| モデル                           | 計算量             | 容量           |
| -------------------------------- | ------------------ | -------------- |
| `k-means`の学習                  | $`O(nkd)`$         | $`O(kd)`$      |
| `k-means`の予測                  | $`O(kd)`$          | $`O(kd)`$      |
| クラスタ特徴量を使った`LR`の学習 | $`O(n(d+k))`$      | $`O(d+k)`$     |
| クラスタ特徴量を使った`LR`の予測 | $`O(d+k)`$         | $`O(d+k)`$     |
| `RBF SVM`の学習                  | $`O(n^2d)`$        | $`O(n^2)`$     |
| `RBF SVM`の予測                  | $`O(sd)`$          | $`O(sd)`$      |
| `GBT`の学習                      | $`O(nd2^mt)`$      | $`O(nd+2^mt)`$ |
| `GBT`の予測                      | $`O(2^mt)`$        | $`O(2^m)t`$    |
| `kNN`の学習                      | $`O(1)`$           | $`O(nd)`$      |
| `kNN`の予測                      | $`O(nd+k \log n)`$ | $`O(nd)`$      |



### k-meansの場合

* 学習の計算量は、 $`O(nkd)`$

  * 計算の各イテレーションにおいて、 $`n`$ 個のデータ点と $`k`$ 個のクラスタの中心の全ての組み合わせについて、 $`d`$ 次元空間の距離を計算するため

  > イテレーションの数は $`n`$ の関数でないと仮定していないが、その仮定が常に成り立つとは限らない

* 予測の計算量は、新しいデータ点と $`k`$ 個のクラスタの中心との距離を計算するので、 $`O(kd)`$ となる

* 保存のための容量は、 $`k`$ 個のクラスタの中心の座標を保存するため、 $`O(kd)`$ になる



### その他のモデルの場合

* `LR`の学習と予測の計算量は、データ点の数及び特徴量の数のどちらに対しても線形

* `RBF SVM`の学習は、入力データの全ての組み合わせに対するカーネル行列を計算するので、処理が重い

  * 予測は、サポートベクタの数 $`s`$ および特徴量の次元 $`d`$ について線形になるので、学習よりは処理が軽い

* `GBT`の学習と予測は、データ点の数とモデルのサイズ( $`t`$ 本の木、それぞれの木に対して最大 $`2^m`$ の葉がある)について線形

  > $`m`$ ：木の最大の深さ

* 単純な`kNN`については、学習のための計算量は必要がない(学習データそのものがモデル)

  * コストは予測にかかる

  > 学習データの各データ点と新しいデータ点との距離を算出し、最も近い $`k`$ 個のデータ点を得るために部分的にソートする必要があるため



### 統括

* `k-means`+`LR`は学習と予測の両方において、学習データの大きさ $`O(nd)`$ と、モデルの大きさ $`O(kd)`$ について線形な唯一の手法

  * その複雑さは、`GBT`と最も似ている

  * `GBT`は、データ点の数、特徴量の次元、モデルの大きさ( $`O(2^mt)`$ )について線形

  * どちらが結果の容量が小さくなるかは、データの空間的な特性に依存する

* 特徴量が有界の実数値を持ち、データ点が複数のかたまりから成る分布を持つ場合には、`k-means`による特徴量生成は有用

  * クラスタの個数を増やすだけでその塊を近似できるので、どんな形でも問題はない

  * ただ、データ点を覆う必要がある(個数の「正解」を発見することにはこだわりがない)



## 適用できない例

* ユークリッド距離が意味をなさない特徴空間の場合、`k-means`を適用できない

  * すなわち、奇妙な分布を持つ数値変数やカテゴリ変数の場合

  > **特徴量のセットの中にこのような変数を含んでいる場合の対処法**
  >
  > 1. 有界の実数値のみに`k-means`を適用する
  >
  > 1. 独自のメトリックを定義して、`k-medoids`を使う(任意のメトリックを使える`k-means`)
  >
  > 1. カテゴリ変数を集計値に変換し、それらの対して`k-means`を使って特徴量を生成する

* カテゴリ変数と時系列を扱うテクニックと組み合わせれば、`k-means`による特徴量生成は、マーケティングや売上の分析で見られる多様なデータに適用できる

* 結果のクラスタはユーザーのセグメントと考えることができ、次のモデリングの段階で非常に有用な特徴量となる



| 版   | 年/月/日   |
| ---- | ---------- |
| 初版 | 2019/05/26 |
