はじめに
=======

* 機械学習プロジェクトに置いて、生データとして数値が得られる状況はいくつも考えられる

  * 例)人や場所の位置情報、商品の価格、センサーの測定値、交通量

* 数値データはそのままでも機械学習モデルに入力として渡すことができるが、特徴量エンジニアリングが不要という訳ではない

  * 良い特徴量は、データの重要な側面を表現するだけでなく、機械学習モデルが数理的に仮定する条件を満たす必要がある

  * そのため、数値データにも何らかの変換が必要になる

  * 特徴量エンジニアリングの中で数値データの変換は最も基本的なテクニック

  * このテクニックは、テキストや画像などの数値でない生データから抽出された値を特徴量として使うときにも有効

* 数値データを使う際にまず確認すべきことは、その値に意味があるかどうか

  * 解くべき課題によっては、数値がプラスかマイナスかだけが重要な場合がある

  * また、もっと粗い粒度で、大体の大きさがわかれば十分ということもある

    * このような場合、二値化や離散化によって数値データを変換する

    * 例)webサイトの1日の訪問回数、レストランのレビュー件数など、自動的にカウントされて増えるタイプの数値データで特に重要

* 次に、数値データが取る値の範囲に付いて考える必要がある

  * 扱う数値データの最小値と最大値はいくつか？

  * また、数値は10、100、1000など複数の桁にわたるか？

  * `スケール`：数値データの取る範囲とその規模のこと

    * モデルが特徴量の滑らかな関数として表現される場合、その出力は特徴量のスケールに影響を受ける

    * 例)以下の関数は入力`x`の単純な線型関数であり、その出力は入力のスケールいよって決まる

    ![f(x)](https://latex.codecogs.com/gif.latex?f%28x%29%3D%203x%20&plus;%201)

  * その他にも結果が入力特徴量のスケールに影響を受ける手法として、以下の手法がある

    * k-meansクラスタリング

    * kNN

    * RBFカーネル

  * また、ユークリッド距離を使用する全てのモデルにおいて、出力のスケールは入力のスケールに影響を受ける

  * このような手法やモデルを使用する場合、出力が期待される範囲に収まるように、特徴量を`正規化`する必要がある

* 一方、論理関数は入力のスケールに影響を受けない

  * 論理関数の出力はその入力が何であれ、0または1だから

  * 例)AND関数は入力として2つの二値変数を取る論理関数であり、その出力は両方の入力が真の場合にのみ1を返す

  * 重要な論理関数の一つに`ステップ関数`がある

    * これは、入力がある値より大きければ1を返し、そうでなければ0を返す関数

    * 例)入力が5より大きい時は1を返し、5以下の時は0を返す

* 機械学習モデルの1つである決定木は、特徴量を入力とする複数のステップ関数の組み合わせによって構成される

  * 従って、決定木に基づくモデル(決定木、勾配ブースティング回帰木、ランダムフォレストなど)は入力のスケールに影響を受けない

  * ただし、入力のスケールが時間とともに増大するような場合には注意が必要

    * 例)蓄積されていくカウントデータを特徴量として使うと、時間が経つに連れてモデルを学習した時のデータ範囲から外れる

    * この場合、定期的に入力データをスケーリングする必要があるかもしれない

* 数値データの分布について考えることも重要

  * 分布はデータがどのような値を取るかを要約してくれる

  * モデルによっては入力特徴量の分布が問題を起こす場合もある

  * 例)線形回帰モデルは、誤差が正規分布に従うことを仮定する

* この仮定が問題になることもある

  * 予測したいターゲット変数の取る値が10、100、1000など複数の桁にわたって分布するような場合には、誤差が正規分布に従うという仮定は成り立たない

  * これに対処する1つの方法は、ターゲット変数を変換して桁数を調整すること

  * `対数変換`は、このようなデータの分布を正規分布に近づけることができる

  * また、対数変換を一般化した`べき変換`も使われる

* 複数の特徴量を組み合わせて「複合的な特徴量」を作ることもできる

  * 複合的な特徴量を作る目的は、生データの重要な情報を簡潔に表現できること

  * 結果的に、モデルがシンプルになり、学習と評価が容易になり、精度の良い予測が可能になる

  * 極端な場合、複合的な特徴量として統計モデルの出力値を使うこともある

    * これは、モデルの`スタッキング`として知られるテクニック

    * また、複合的な特徴量の最も簡単な例として`交互作用特徴量`を扱う

    * これはモデルに組み込むのは簡単だが、特徴量の数が増えるためモデルの計算コストが増大する

    * 計算コストを抑えるために、多数の特徴量の中から有用でないものを取り除く手法として`特徴選択`が利用される



| 版   | 年/月/日   |
| ---- | ---------- |
| 初版 | 2019/04/14 |
