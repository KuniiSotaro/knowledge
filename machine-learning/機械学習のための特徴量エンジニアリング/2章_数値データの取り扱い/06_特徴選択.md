06 特徴選択
==========

* `特徴選択`：有用でない特徴量を取り除くことで、モデルの複雑さを軽減する手法

  * 予測精度をなるべく悪化せずに計算速度の速い簡潔なモデルを手に入れる為に使われる

  * その為に、特徴選択の手法の中には複数の候補モデルを学習するものもある

  * つまり、特徴量選択は学習時間を削減するとは限らない

    * 実際、いくつかの手法は全体の学習時間を増加させる

  * ただし、R^2スコアなどのテストスコアの算出時間は削減される

* 大まかには、特徴選択には以下の3つのタイプに分類される



## フィルタ法

* `フィルタ法`：閾値と使って有用でないと思われる特徴量を除去する手法

  * 例)特徴量とターゲット変数の相関や相互情報量を計算し、閾値より小さければ削除するという方法がある

* この方法は、`ラッパー法`よりも計算コストははるかに安価

* しかし、使用するモデルについては考慮しない為、モデルにとって良い特徴量を選んでいるかどうかはわからない

* `フィルタ法`を使う場合は、モデルの学習ステップに入る前に有用な特徴量を誤って削除してしまわないようにする必要がある



## ラッパー法

* `ラッパー法`：特徴量の一部を使って実際にモデルを学習し、精度を調べることでそれが有用かどうかを判断する手法

  * モデルを特徴量の品質スコアを算出するためのブラックボックスとして使う

  * これを何度も繰り返してスコアを改善すれば、品質の高い特徴量を選択できる

* `ラッパー法`は計算コストの大きい手法だが、実際に学習してみることで、

  * 単独では有用でなくても他と組み合わせることで有用になるような特徴量を削除してしまうことを防ぐことができる



## 組み込み法

* `組み込み法`：モデルの学習プロセスに特徴選択が組み込まれていることを指す

  * 例)決定木の学習プロセスには特徴選択が組み込まれている

  * これは、決定木の各学習ステップに置いて木を分割するための特徴量が1つずつ選択されているため

* 他にも、l^2正則化項は線形モデルの目的関数に加えることで多数の特徴量から一部だけを使うように学習する(`スパース制約`)

* `組み込み法`は、`ラッパー法`よりも結果の品質は劣るが、計算コストを抑えることができる

* また、`フィルタ法`と比べて、`組み込み法`はモデルに適した特徴量を選択する

  * この意味で、`組み込み法`は計算コストと結果の品質のバランスの良い手法



| 版   | 年/月/日   |
| ---- | ---------- |
| 初版 | 2019/04/29 |
