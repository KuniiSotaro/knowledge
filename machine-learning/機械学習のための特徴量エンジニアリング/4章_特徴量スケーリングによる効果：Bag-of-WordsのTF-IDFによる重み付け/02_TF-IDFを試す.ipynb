{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "02　TF-IDFを試す\n",
    "==============\n",
    "\n",
    "* `TF-IDF`は、単語の出現頻度に定数を掛け算して得られる特徴量\n",
    "\n",
    "    * したがって、これは`特徴量スケーリング`と言い換えることもできる\n",
    "    \n",
    "    * 実際にどの程度昨日しているのか、単純なテキスト分類のタスクでスケーリングされた特徴量と、スケーリングされていない特徴量の性能を比較する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ここでは、Yelpでのレビューをデータセットとして利用する\n",
    "\n",
    "    * Yelpチャレンジ第6ラウンドのデータセットには、米国6都市でのレビューが60万件数含まれている"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-72f03eaa0679>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Yelpのビジネスデータを読み込み\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/kunii.sotaro/Downloads/business.json'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbiz_f\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mbiz_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbiz_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Yelpのレビューデータを読み込み\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-72f03eaa0679>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Yelpのビジネスデータを読み込み\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/kunii.sotaro/Downloads/business.json'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbiz_f\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mbiz_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbiz_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Yelpのレビューデータを読み込み\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.6.5/Frameworks/Python.framework/Versions/3.6/lib/python3.6/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 354\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.6.5/Frameworks/Python.framework/Versions/3.6/lib/python3.6/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \"\"\"\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.6.5/Frameworks/Python.framework/Versions/3.6/lib/python3.6/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m         \"\"\"\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# エラーが発生しているが、専攻分野で必要でない部分なので省略する\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Yelpのビジネスデータを読み込み\n",
    "with open('/Users/kunii.sotaro/Downloads/business.json') as biz_f:\n",
    "    biz_df = pd.DataFrame([json.loads(x) for x in biz_f.readlines()])\n",
    "\n",
    "# Yelpのレビューデータを読み込み\n",
    "with open('/Users/kunii.sotaro/Downloads/review.json') as review_file:\n",
    "    review_df = pd.DataFrame([json.loads(x) for x in review_file.readlines()])\n",
    "\n",
    "# YelpのビジネスデータからcategoriesがNightlife（ナイトライフ@<fn>{nightlife}）またはRestaurants(レストラン)のデータを取り出し\n",
    "filter_func = lambda x:  len(set(x) &  set(['Nightlife', 'Restaurants'])) > 0\n",
    "twobiz = biz_df[biz_df['categories'].apply(filter_func)]\n",
    "\n",
    "# 取り出した2つのカテゴリのYelpのビジネスデータとYelpのレビューデータを結合する\n",
    "twobiz_reviews = twobiz.merge(review_df, on='business_id', how='inner')\n",
    "\n",
    "# 必要ない特徴量を排除\n",
    "twobiz_reviews = twobiz_reviews[['business_id', 'name', 'stars_y', 'text', 'categories']]\n",
    "\n",
    "# target列を作成。categoriesがNightlifeの時はTrue、それ以外の場合はFalse\n",
    "twobiz_reviews['target'] =  twobiz_reviews['categories'].apply(set(['Nightlife']).issubset)\n",
    "\n",
    "twobiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. クラス分類用のデータセット作成\n",
    "\n",
    "* レビューを使って、レストランかナイトライフかどうかを分類する\n",
    "\n",
    "* 学習時間を短縮するために、一部のデータを使う\n",
    "\n",
    "    * ここでは、2つのカテゴリでレビュー数が大きく異なっている\n",
    "    \n",
    "    * このようなデータえっとを、`クラス不均衡データ`と言う\n",
    "    \n",
    "    * このデータセットをそのままモデリングすると問題が起き、より大きなクラスに当てはまるように作られてしまう\n",
    "    \n",
    "* 今回は、両方のクラスに多くのデータがあるので、`ダウンサンプリング`でこの問題を解決する\n",
    "\n",
    "    * これは、レコード数が大きいクラスを、レコード数が少ないクラスとほぼ同じサイズになるようにサンプリングして、データを減らす方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ナイトライフに対するレビューの10%とレストランに対するレビューの2.1%をランダムサンプリングする\n",
    "\n",
    "1. このデータセットの70%を学習データに、30%をテストデータになるようにデータを分離する\n",
    "\n",
    "1. 学習データには46,924個のユニークな単語が含まれており、これがBag-of-Wordsにおける特徴量の数になる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# サンプリングしてクラス均衡を是正したデータセットを作成\n",
    "nightlife = twobiz_reviews[twobiz_reviews['categories'].apply(set(['Nightlife']).issubset)]\n",
    "restaurants = twobiz_reviews[twobiz_reviews['categories'].apply(set(['Restaurants']).issubset)]\n",
    "\n",
    "nightlife_subset = nightlife.sample(frac=0.1, random_state=123)\n",
    "restaurant_subset = restaurants.sample(frac=0.021, random_state=123)\n",
    "combined = pd.concat([nightlife_subset, restaurant_subset])\n",
    "\n",
    "# 学習データとテストデータに分割\n",
    "training_data, test_data = train_test_split(combined, test_size=0.3, random_state=123)\n",
    "training_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. TF-IDF変換を用いたBag-of-Wordsのスケーリング\n",
    "\n",
    "* 線形クラス分類における`Bag-of-Words`、`TF-IDF`、$l^2$正規化の効果を実験により比較する\n",
    "\n",
    "    * `TF-IDF`に$l^2$正規化を行なった際の結果は、$l^2$正規化を単体で使う場合と同じ\n",
    "    \n",
    "    * 従って、`Bag-of-Words`、`TF-IDF`、Bag-of-Wordsの$l^2$正規化の3つについての実験が必要になる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 以下の例では、scikit-learnの`CountVectorizer`を使用してレビューテキストを`Bag-of-Words`に変換する\n",
    "\n",
    "    * 全てのテキストを特徴量化する手法は、テキストをトークン(単語)のリストに変換する機能であるトークナイザに依存する\n",
    "    \n",
    "    * 全てのテキストを特徴量化する手法は、テキストをトークン(単語)のリストに変換する機能であるトークナイザに依存する\n",
    "    \n",
    "* この例では、scikit-learnのデフォルトのトークン化パターンは、2つ以上の英数字の連続を探す\n",
    "\n",
    "    * 句読点は、トークンを分割する区切り文字となる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "\n",
    "# レビューをBag-of-Wordsで表す\n",
    "bow_transform = text.CountVectorizer()\n",
    "X_tr_bow = bow_transform.fit_transform(training_data['text'])\n",
    "X_te_bow = bow_transform.transform(test_data['text'])\n",
    "len(bow_transform.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "y_tr = training_data['target']\n",
    "y_te = test_data['target']\n",
    "\n",
    "# Bag-of-Words行列からTF-IDFを作成\n",
    "tfidf_trfm = text.TfidfTransformer(norm=None)\n",
    "X_tr_tfidf = tfidf_trfm.fit_transform(X_tr_bow)\n",
    "X_te_tfidf = tfidf_trfm.transform(X_te_bow)\n",
    "\n",
    "#  Bag-of-WordsのL2正規化\n",
    "X_tr_l2 = normalize(X_tr_bow, norm='l2', axis=0)\n",
    "X_te_l2 = normalize(X_te_bow, norm='l2', axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **テストデータにおける特徴量スケーリング**\n",
    ">\n",
    "> 特徴量をスケーリングする際のポイントとして、`テストデータの平均`、`分散`、`文書頻度`、$l^2$ノルムなど、実際にはわからない可能性が高い特徴量の統計を使う必要がある\n",
    ">\n",
    "> `TF-IDF`を計算するには、**学習データ**で逆文書頻度を計算して、学習データおよびテストデータの両方をスケーリングする\n",
    ">\n",
    "> scikit-learnでは、学習データで特徴量変換をすると、関連する統計量が保存される\n",
    "> \n",
    "> テストデータの特徴量に対しても、学習データと同じ統計量で変換することができる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 学習データでテストデータのスケーリングをすると、不自然な結果となる可能性がある\n",
    "\n",
    "    * テストデータに対してMin-Maxスケーリングをすると、最大1で最小10に変換されない\n",
    "    \n",
    "    * これは、$l^2$ノルム、`平均`、`分散`が学習データとテストデータで異なるため\n",
    "    \n",
    "* 一般的な解決方法は、新しい単語をテストデータから取り除く"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ロジスティック回帰によるクラス分類\n",
    "\n",
    "* `ロジスティック回帰`：シンプルな線形分類器であり、そのシンプルさゆえに最初に試す分類器として適している\n",
    "\n",
    "    * 重みのついた特徴量を足し合わせた結果を、**シグモイド関数**に渡す\n",
    "    \n",
    "    * この関数は、実数$x$を0から1の連続値に変換する\n",
    "    \n",
    "        * $w$：傾き\n",
    "    \n",
    "        * $b$：切片項、関数出力がどこで中間点0.5になるかを表す\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `ロジスティック回帰`では、シグモイド関数の出力が0.5より大きい場合は正のクラスを、そうでない場合は負のクラスとちえ予測する\n",
    "\n",
    "    * クラスの境界地点がどこになるか、入力値の変化の影響度合いをパラメータ$w$と$b$で決定する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![sigmoid関数](./images/sigmoid関数.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 次に、3パターンのデータセットでロジスティック回帰を作り挙動を確認する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def simple_logistic_classify(X_tr, y_tr, X_test, y_test, description, _C=1.0):\n",
    "### ロジスティック回帰で学習し、テストデータでの予測結果を得る関数\n",
    "    m = LogisticRegression(solver='liblinear', C=_C).fit(X_tr, y_tr)\n",
    "    s = m.score(X_test, y_test)\n",
    "    print ('Test score with', description, 'features:', s)\n",
    "    return m\n",
    "\n",
    "m1 = simple_logistic_classify(X_tr_bow, y_tr, X_te_bow, y_te, 'bow')\n",
    "m2 = simple_logistic_classify(X_tr_l2, y_tr, X_te_l2, y_te, 'l2-normalized')\n",
    "m3 = simple_logistic_classify(X_tr_tfidf, y_tr, X_te_tfidf, y_te, 'tf-idf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Bag-of-Wordsを用いるモデルが比較対象の中で最も精度が高くなっている\n",
    "\n",
    "    * この結果は、モデルのチューニングが不十分のため起きた現象で、モデルを比較する時に犯す典型的なミスの1つ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 正則化によるロジスティック回帰のチューニング\n",
    "\n",
    "![4-2-1](./images/4-2-1.jpg)\n",
    "\n",
    "![4-2-2](./images/4-2-2.jpg)\n",
    "\n",
    "![4-2-3](./images/4-2-3.jpg)\n",
    "\n",
    "![4-2-4](./images/4-2-4.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 探索範囲を指定して、5分割でグリッドサーチを実行します\n",
    "param_grid_ = {'C': [1e-5, 1e-3, 1e-1, 1e0, 1e1, 1e2]}\n",
    "\n",
    "# Bag-of-Wordsでの分類器をチューニング\n",
    "bow_search = GridSearchCV(LogisticRegression(solver='liblinear'), cv=5, param_grid=param_grid_, return_train_score=True)\n",
    "bow_search.fit(X_tr_bow, y_tr)\n",
    "\n",
    "# L2正規化単語ベクトルでの分類器をチューニング\n",
    "l2_search = GridSearchCV(LogisticRegression(solver='liblinear'), cv=5, param_grid=param_grid_, return_train_score=True)\n",
    "l2_search.fit(X_tr_l2, y_tr)\n",
    "\n",
    "# TF-IDFでの分類器をチューニング\n",
    "tfidf_search = GridSearchCV(LogisticRegression(solver='liblinear'), cv=5, param_grid=param_grid_, return_train_score=True)\n",
    "tfidf_search.fit(X_tr_tfidf, y_tr)\n",
    "\n",
    "# グリッドサーチにおける出力を見て、挙動を確認します\n",
    "bow_search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# クロスバリデーションの結果を箱ひげ図でプロットする\n",
    "# 分類器のパフォーマンスを可視化して比較する\n",
    "search_results = pd.DataFrame.from_dict({\n",
    "    'bow': bow_search.cv_results_['mean_test_score'],\n",
    "    'tfidf': tfidf_search.cv_results_['mean_test_score'],\n",
    "    'l2': l2_search.cv_results_['mean_test_score']\n",
    "})\n",
    "\n",
    "# matplotlibでグラフを描く。ここでSeabornはグラフの見た目を整える為に用いている。\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "ax = sns.boxplot(data=search_results, width=0.4)\n",
    "ax.set_ylabel('Accuracy', size=14)\n",
    "ax.tick_params(labelsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# クロスバリデーションで得られた最適なハイパーパラメータと学習用データ全てを用いて最終的なモデルを学習し、そのモデルを用いて検証用データにおける精度を算出する\n",
    "m1 = simple_logistic_classify(X_tr_bow, y_tr, X_te_bow, y_te, 'bow',  _C=bow_search.best_params_['C'])\n",
    "m2 = simple_logistic_classify(X_tr_l2, y_tr, X_te_l2, y_te, 'l2-normalized',  _C=l2_search.best_params_['C'])\n",
    "m3 = simple_logistic_classify(X_tr_tfidf, y_tr, X_te_tfidf, y_te, 'tf-idf',  _C=tfidf_search.best_params_['C'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
