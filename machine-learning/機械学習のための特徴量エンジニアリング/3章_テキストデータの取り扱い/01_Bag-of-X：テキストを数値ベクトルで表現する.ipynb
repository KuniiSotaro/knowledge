{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "01 Bag-of-X：テキストを数値ベクトルで表現する\n",
    "=======================================\n",
    "\n",
    "* 機械学習モデルと特徴量はシンプルで解釈しやすいものが望ましい\n",
    "\n",
    "  * シンプルであれば試行錯誤しやすく、解釈しやすければ問題が起こった時に原因を特定しやすい\n",
    "\n",
    "  * シンプルで解釈しやすい特徴量は必ずしも精度の良い結果に繋がる訳ではない\n",
    "\n",
    "  * しかし、まずシンプルに始めてみて、必要になったら複雑にしていく\n",
    "\n",
    "* `Bag-of-Words`：テキストデータを周知の特徴量として表現する方法\n",
    "\n",
    "  * テキスト中にどの単語が何回含まれるかという頻度情報のベクトル\n",
    "\n",
    "  * 単にテキスト中の単語をカウントするだけで、その単語がメインキャラクターの名前かどうかを全く気にしない\n",
    "\n",
    "    * しかし、重要な単語はテキスト中で繰り返し用いられるため、一度しか現れない単語よりも重要である\n",
    "\n",
    "  * 単語の出現回数は、テキスト分類のようなシンプルなタスクにおいて十分な効力を発揮する\n",
    "\n",
    "    * これらは、情報検索でも有効\n",
    "\n",
    "    * 情報検索は、テキストクエリが入力として与えられた時に、関連のある文書セットを見つけ出すというタスク\n",
    "\n",
    "    * どちらのタスクでも単語の出現回数という特徴量がうまく機能する\n",
    "\n",
    "      * これは、テキストに特定の単語が含まれるかどうかが、テキストの内容と強く関連するため\n",
    "\n",
    "\n",
    "\n",
    "## 1. Bag-of-Words\n",
    "\n",
    "* `Bag-of-Words`：テキスト文書を単語の出現回数のベクトルで表現したもの\n",
    "\n",
    "  * `語彙`：テキストデータ中に現れる全ての単語を集めたのもの\n",
    "\n",
    "* `BoW`ベクトルは、語彙中の全ての単語に対して、テキスト中にその単語が出現した回数を並べたもの\n",
    "\n",
    "  * 従って、語彙がn個の単語からなる場合、`BoW`ベクトルはn次元の特徴ベクトルになる\n",
    "\n",
    "  * 例)テキストに単語\"aardvark\"が3回現れたら、特徴ベクトルのその単語に対応する要素の値は3となる\n",
    "\n",
    "  * 例)単語がテキスト中に現れない場合、対応する要素の値は0となる\n",
    "\n",
    "* 例)テキスト\"it is a puppy and it is extremely cute\"を`BoW`で表現する\n",
    "\n",
    "![Bag-of-Words変換](./images/Bag-of-Words変換.png)\n",
    "\n",
    "* `Bag-of-Words`はテキスト文書を平坦なベクトルに変換する\n",
    "\n",
    "  * 「平坦な」：元のテキストの構造を保持しないため\n",
    "\n",
    "  * 文章は単語を一列に並べたものであり、単語がどの順番で並んでいるかは重要\n",
    "\n",
    "  * しかし、`BoW`では単語の順序は保持されず、テキスト中に単語が何回現れたかだけを記憶する\n",
    "\n",
    "* しかし、単語の並び順はデータセット中の全ての文書に対して一貫して同じである必要がある\n",
    "\n",
    "  * また、`BoW`は単語の階層の概念を表現しない\n",
    "\n",
    "  * 例)「動物」という概念は「犬」、「猫」などを含む\n",
    "\n",
    "  * しかし、`BoW`表現では、これらの単語はベクトルの要素として同じ階層に属する\n",
    "\n",
    "\n",
    "\n",
    "### n次元の特徴空間\n",
    "\n",
    "* 特徴空間において、データがどのように表されるかをみてみる\n",
    "\n",
    "* `BoW`ベクトルでは、語彙中の単語はベクトルの次元に対応する\n",
    "\n",
    "  * 語彙にn個数の単語がある場合、1つの文書はn次元空間の1点となる\n",
    "\n",
    "* 例)\"puppy\"と\"cute\"の2つの単語からなる2次元の特徴空間\n",
    "\n",
    "![2次元の特徴空間におけるテキスト](./images/2次元の特徴空間におけるテキスト.png)\n",
    "\n",
    "* 例)単語\"puppy\"、\"extremely\"、\"cute\"を軸とする3次元空間上\n",
    "\n",
    "![3次元特徴量空間上の3つの文章](./images/3次元特徴量空間上の3つの文章.png)\n",
    "\n",
    "* これらの図は、特徴空間におけるデータ点を表したもの\n",
    "\n",
    "  * 各軸は`BoW`表現における特徴量(単語)を表し、特徴空間の点はデータ点(文書)を表す\n",
    "\n",
    "\n",
    "\n",
    "### データ空間におけるデータベクトル\n",
    "\n",
    "* `データ空間`における`データベクトル`について考える\n",
    "\n",
    "  * `データベクトル`：特定の特徴量に注目した場合の各データの点の値からなるベクトル\n",
    "\n",
    "  * データ空間の`軸`は`データ点`(文書)を表し、`点`は`データベクトル`(単語)を表す\n",
    "\n",
    "* 例)データ空間におけるデータベクトル\n",
    "\n",
    "  * これは、単語に対する\"Bag-of-Documents\"表現とも言える\n",
    "\n",
    "  * Bag-of-Documents表現は、Bag-of-Words表現の転置行列となる\n",
    "\n",
    "![データ空間におけるデータベクトル](./images/データ空間におけるデータベクトル.png)\n",
    "\n",
    "* Bag-of-Words表現は、テキストの内容を完全に表すものではない\n",
    "\n",
    "  * 文章を単語に分解することで失われる情報がある\n",
    "\n",
    "  * 例)\"not bad\"は\"decent\"や\"good\"と同じ意味だが、\"not\"と\"bad\"に分解されることで意味を失う\n",
    "\n",
    "* Bag-of-Wordsはシンプルで強力な表現方法だが、テキストの意味を正しく理解したい場合にはあまり役に立たない\n",
    "\n",
    "\n",
    "\n",
    "| 版   | 年/月/日   |\n",
    "| ---- | ---------- |\n",
    "| 初版 | 2019/04/21 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02_Bag-or-n-Grams\n",
    "\n",
    "* `Bag-or-n-Grams`：`Bag-of-Words`の自然な拡張\n",
    "\n",
    "    * nグラム(n-gram)は、n個の連続したトークンからなる配列\n",
    "    \n",
    "    * 単語はnグラムの$n=1$の場合と考えることができ、`ユニグラム`とも呼ばれる\n",
    "    \n",
    "    * `トークン化`のあと、個々のトークンの数をカウントすると単語数となり、n個の連続したトークンの数をカウントすると、nグラム数となる\n",
    "    \n",
    "    * 例)\"Emma knocked on the door\"は、\"Emma knocked\"、\"knocked on\"、\"on the\"、\"the door\"の4つのnグラム($n=2$)を生成する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `nグラム`は、テキストの順序構造を部分的に保持する\n",
    "\n",
    "    * 従って、`Bag-of-n-Grams`は`Bag-of-Words`より情報量の多い表現\n",
    "    \n",
    "    * 一方、その取り扱いはコストが大きくなる\n",
    "    \n",
    "    * 理論的には、k個の単語に対して最大で$k^2$個の`バイグラム`(bigram)が生成できる\n",
    "    \n",
    "    * 実際には、全ての単語の組み合わせが文章中に現れることはないが、単語数に比べてnグラム($n > 1$)の数は非常に大きくなる\n",
    "    \n",
    "        * これは、`Bag-of-n-Grams`の作る特徴量空間が非常に大きくスパースになることを意味する\n",
    "        \n",
    "    * `Bag-of-n-Grams`はnが大きいほど含まれる情報は豊富になるが、特徴量の保存やモデリングにかかるコストが増大することに注意が必要"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* nが増えるとnグラムの数がどのように増加するかを以下の図に示す\n",
    "\n",
    "    * Yelpデータセットを使って、nグラムを作成するコードを示す\n",
    "    \n",
    "    * 最初の10,000件のレビューのnグラムを、scikit-learnの`CountVectorizer`によって作成している"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# 最初の 10,000 件のレビューを読み込む\n",
    "with open('data/yelp/yelp_academic_dataset_review.json') as f:\n",
    "    js = []\n",
    "    for i in range(10000):\n",
    "        js.append(json.loads(f.readline()))\n",
    "review_df = pd.DataFrame(js)\n",
    "\n",
    "# scikit-learn の CountVectorizer を使ってユニグラム（BoW）、\n",
    "# バイグラム、トライグラムの特徴量変換器を作成する。\n",
    "# CountVectorizer はデフォルトでは1文字の単語を無視するが、\n",
    "# これは意味のない単語を除外するため実用的である。\n",
    "# ただしここでは全ての単語を含むように設定している。\n",
    "bow_converter = CountVectorizer(token_pattern='(?u)\\\\b\\\\w+\\\\b')\n",
    "bigram_converter = CountVectorizer(ngram_range=(2,2), token_pattern='(?u)\\\\b\\\\w+\\\\b')\n",
    "trigram_converter = CountVectorizer(ngram_range=(3,3), token_pattern='(?u)\\\\b\\\\w+\\\\b')\n",
    "\n",
    "# 変換器を適用し、語彙数を確認する\n",
    "bow_converter.fit(review_df['text'])\n",
    "words = bow_converter.get_feature_names()\n",
    "\n",
    "bigram_converter.fit(review_df['text'])\n",
    "bigrams = bigram_converter.get_feature_names()\n",
    "\n",
    "trigram_converter.fit(review_df['text'])\n",
    "trigrams = trigram_converter.get_feature_names()\n",
    "\n",
    "print (len(words), len(bigrams), len(trigrams))\n",
    "\n",
    "# n-グラムを確認する\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 版   | 年/月/日   |\n",
    "| ---- | ---------- |\n",
    "| 初版 | 2019/04/21 |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
